{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENV, keep, scalar, deprecationWarn, tidy, util, onesLike, tensor1d, where, zerosLike, train, cast, dispose, memory, serialization, nextFrame, add, mul, div, Tensor, Optimizer, concat, mean, io, transpose, expandDims, reverse, unstack, stack, zeros, sum, split, neg, variable, ones, eye, randomUniform, truncatedNormal, randomNormal, sub, sqrt, mulStrict, clipByValue, relu, linalg, leakyRelu, prelu, elu, conv1d, conv2d, conv2dTranspose, separableConv2d, depthwiseConv2d, notEqual, logicalAnd, maximum, minimum, all, batchNorm2d, batchNorm3d, batchNorm4d, moments, pad, maxPool, avgPool, squeeze, max, abs, log, softplus, softmax, floor, oneHot, greater, equal, argMax, slice1d, slice2d, slice3d, slice4d, concat1d, concat2d, concat3d, concat4d, tile, matMul, gather, step, min, selu, sigmoid, tanh } from \"@tensorflow/tfjs-core\";\n\nvar _extendStatics = function extendStatics(e, t) {\n  return (_extendStatics = Object.setPrototypeOf || {\n    __proto__: []\n  } instanceof Array && function (e, t) {\n    e.__proto__ = t;\n  } || function (e, t) {\n    for (var n in t) {\n      t.hasOwnProperty(n) && (e[n] = t[n]);\n    }\n  })(e, t);\n};\n\nfunction __extends(e, t) {\n  function n() {\n    this.constructor = e;\n  }\n\n  _extendStatics(e, t), e.prototype = null === t ? Object.create(t) : (n.prototype = t.prototype, new n());\n}\n\nvar _epsilon,\n    _assign = function __assign() {\n  return (_assign = Object.assign || function (e) {\n    for (var t, n = 1, r = arguments.length; n < r; n++) {\n      for (var i in t = arguments[n]) {\n        Object.prototype.hasOwnProperty.call(t, i) && (e[i] = t[i]);\n      }\n    }\n\n    return e;\n  }).apply(this, arguments);\n};\n\nfunction __awaiter(e, t, n, r) {\n  return new (n || (n = Promise))(function (i, a) {\n    function o(e) {\n      try {\n        l(r.next(e));\n      } catch (e) {\n        a(e);\n      }\n    }\n\n    function s(e) {\n      try {\n        l(r.throw(e));\n      } catch (e) {\n        a(e);\n      }\n    }\n\n    function l(e) {\n      e.done ? i(e.value) : new n(function (t) {\n        t(e.value);\n      }).then(o, s);\n    }\n\n    l((r = r.apply(e, t || [])).next());\n  });\n}\n\nfunction __generator(e, t) {\n  var n,\n      r,\n      i,\n      a,\n      o = {\n    label: 0,\n    sent: function sent() {\n      if (1 & i[0]) throw i[1];\n      return i[1];\n    },\n    trys: [],\n    ops: []\n  };\n  return a = {\n    next: s(0),\n    throw: s(1),\n    return: s(2)\n  }, \"function\" == typeof Symbol && (a[Symbol.iterator] = function () {\n    return this;\n  }), a;\n\n  function s(a) {\n    return function (s) {\n      return function (a) {\n        if (n) throw new TypeError(\"Generator is already executing.\");\n\n        for (; o;) {\n          try {\n            if (n = 1, r && (i = 2 & a[0] ? r.return : a[0] ? r.throw || ((i = r.return) && i.call(r), 0) : r.next) && !(i = i.call(r, a[1])).done) return i;\n\n            switch (r = 0, i && (a = [2 & a[0], i.value]), a[0]) {\n              case 0:\n              case 1:\n                i = a;\n                break;\n\n              case 4:\n                return o.label++, {\n                  value: a[1],\n                  done: !1\n                };\n\n              case 5:\n                o.label++, r = a[1], a = [0];\n                continue;\n\n              case 7:\n                a = o.ops.pop(), o.trys.pop();\n                continue;\n\n              default:\n                if (!(i = (i = o.trys).length > 0 && i[i.length - 1]) && (6 === a[0] || 2 === a[0])) {\n                  o = 0;\n                  continue;\n                }\n\n                if (3 === a[0] && (!i || a[1] > i[0] && a[1] < i[3])) {\n                  o.label = a[1];\n                  break;\n                }\n\n                if (6 === a[0] && o.label < i[1]) {\n                  o.label = i[1], i = a;\n                  break;\n                }\n\n                if (i && o.label < i[2]) {\n                  o.label = i[2], o.ops.push(a);\n                  break;\n                }\n\n                i[2] && o.ops.pop(), o.trys.pop();\n                continue;\n            }\n\n            a = t.call(e, o);\n          } catch (e) {\n            a = [6, e], r = 0;\n          } finally {\n            n = i = 0;\n          }\n        }\n\n        if (5 & a[0]) throw a[1];\n        return {\n          value: a[0] ? a[1] : void 0,\n          done: !0\n        };\n      }([a, s]);\n    };\n  }\n}\n\nfunction epsilon() {\n  return null == _epsilon && (_epsilon = ENV.get(\"EPSILON\")), _epsilon;\n}\n\nfunction imageDataFormat() {\n  return \"channelsLast\";\n}\n\nvar _nextUniqueTensorId = 0;\n\nfunction getNextUniqueTensorId() {\n  return _nextUniqueTensorId++;\n}\n\nvar _uidPrefixes = {};\n\nfunction getUid(e) {\n  return void 0 === e && (e = \"\"), e in _uidPrefixes || (_uidPrefixes[e] = 0), _uidPrefixes[e] += 1, e + _uidPrefixes[e].toString();\n}\n\nvar scalarCache = {\n  float32: {},\n  int32: {}\n},\n    DEFAULT_DTYPE = \"float32\";\n\nfunction getScalar(e, t) {\n  return void 0 === t && (t = DEFAULT_DTYPE), null == scalarCache[t][e] && (scalarCache[t][e] = scalar(e, t), keep(scalarCache[t][e])), scalarCache[t][e];\n}\n\nvar AttributeError = function (e) {\n  function t(n) {\n    var r = e.call(this, n) || this;\n    return Object.setPrototypeOf(r, t.prototype), r;\n  }\n\n  return __extends(t, e), t;\n}(Error),\n    RuntimeError = function (e) {\n  function t(n) {\n    var r = e.call(this, n) || this;\n    return Object.setPrototypeOf(r, t.prototype), r;\n  }\n\n  return __extends(t, e), t;\n}(Error),\n    ValueError = function (e) {\n  function t(n) {\n    var r = e.call(this, n) || this;\n    return Object.setPrototypeOf(r, t.prototype), r;\n  }\n\n  return __extends(t, e), t;\n}(Error),\n    NotImplementedError = function (e) {\n  function t(n) {\n    var r = e.call(this, n) || this;\n    return Object.setPrototypeOf(r, t.prototype), r;\n  }\n\n  return __extends(t, e), t;\n}(Error),\n    AssertionError = function (e) {\n  function t(n) {\n    var r = e.call(this, n) || this;\n    return Object.setPrototypeOf(r, t.prototype), r;\n  }\n\n  return __extends(t, e), t;\n}(Error),\n    IndexError = function (e) {\n  function t(n) {\n    var r = e.call(this, n) || this;\n    return Object.setPrototypeOf(r, t.prototype), r;\n  }\n\n  return __extends(t, e), t;\n}(Error);\n\nfunction pyListRepeat(e, t) {\n  if (Array.isArray(e)) {\n    for (var n = [], r = 0; r < t; r++) {\n      n = n.concat(e);\n    }\n\n    return n;\n  }\n\n  return (n = new Array(t)).fill(e), n;\n}\n\nfunction assert(e, t) {\n  if (!e) throw new AssertionError(t);\n}\n\nfunction count(e, t) {\n  for (var n = 0, r = 0, i = e; r < i.length; r++) {\n    i[r] === t && n++;\n  }\n\n  return n;\n}\n\nfunction singletonOrArray(e) {\n  return 1 === e.length ? e[0] : e;\n}\n\nfunction toList(e) {\n  return Array.isArray(e) ? e : [e];\n}\n\nfunction toSnakeCase(e) {\n  var t = e.replace(/(.)([A-Z][a-z0-9]+)/g, \"$1_$2\").replace(/([a-z])([A-Z])/g, \"$1_$2\").toLowerCase();\n  return \"_\" !== t[0] ? t : \"private\" + t;\n}\n\nfunction toCamelCase(e) {\n  return e.length <= 1 ? e : -1 === e.indexOf(\"_\") ? e : e.replace(/[_]+(\\w|$)/g, function (e, t) {\n    return t.toUpperCase();\n  });\n}\n\nvar _GLOBAL_CUSTOM_OBJECTS = {};\n\nfunction serializeKerasObject(e) {\n  return null === e || void 0 === e ? null : {\n    className: e.getClassName(),\n    config: e.getConfig()\n  };\n}\n\nfunction convertNDArrayScalarsInConfig(e) {\n  if (null != e && \"object\" == typeof e) if (Array.isArray(e)) e.forEach(function (e) {\n    return convertNDArrayScalarsInConfig(e);\n  });else for (var t = 0, n = Object.keys(e); t < n.length; t++) {\n    var r = n[t],\n        i = e[r];\n    null != i && \"object\" == typeof i && (Array.isArray(i) || \"ndarray\" !== i.type || \"number\" != typeof i.value ? convertNDArrayScalarsInConfig(i) : e[r] = i.value);\n  }\n}\n\nfunction deserializeKerasObject(e, t, n, r, i) {\n  if (void 0 === t && (t = {}), void 0 === n && (n = {}), void 0 === r && (r = \"object\"), void 0 === i && (i = !1), \"string\" == typeof e) {\n    var a = e,\n        o = void 0;\n    if (a in n) o = n[a];else if (a in _GLOBAL_CUSTOM_OBJECTS) o = _GLOBAL_CUSTOM_OBJECTS[a];else if (null == (o = t[a])) throw new ValueError(\"Unknown \" + r + \": \" + e + \". This may be due to one of the following reasons:\\n1. The \" + r + \" is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\\n2. The custom \" + r + \" is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().\");\n    return o;\n  }\n\n  var s = e;\n  if (null == s.className || null == s.config) throw new ValueError(r + \": Improper config format: \" + JSON.stringify(s) + \".\\n'className' and 'config' must set.\");\n  var l = s.className,\n      u = void 0,\n      c = void 0;\n  if (l in n ? (u = (I = n.get(l))[0], c = I[1]) : l in _GLOBAL_CUSTOM_OBJECTS ? (u = (A = _GLOBAL_CUSTOM_OBJECTS.className)[0], c = A[1]) : l in t && (u = (C = t[l])[0], c = C[1]), null == u) throw new ValueError(\"Unknown \" + r + \": \" + l + \". This may be due to one of the following reasons:\\n1. The \" + r + \" is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\\n2. The custom \" + r + \" is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().\");\n\n  if (null != c) {\n    for (var p = {}, h = 0, d = Object.keys(_GLOBAL_CUSTOM_OBJECTS); h < d.length; h++) {\n      p[b = d[h]] = _GLOBAL_CUSTOM_OBJECTS[b];\n    }\n\n    for (var f = 0, g = Object.keys(n); f < g.length; f++) {\n      p[b = g[f]] = n[b];\n    }\n\n    s.config.customObjects = p;\n\n    for (var m = _assign({}, _GLOBAL_CUSTOM_OBJECTS), y = 0, v = Object.keys(n); y < v.length; y++) {\n      var b = v[y];\n      _GLOBAL_CUSTOM_OBJECTS[b] = n[b];\n    }\n\n    convertNDArrayScalarsInConfig(s.config);\n    var w = c(u, s.config, n, i);\n    return _GLOBAL_CUSTOM_OBJECTS = _assign({}, m), w;\n  }\n\n  m = _assign({}, _GLOBAL_CUSTOM_OBJECTS);\n\n  for (var z = 0, S = Object.keys(n); z < S.length; z++) {\n    b = S[z];\n    _GLOBAL_CUSTOM_OBJECTS[b] = n[b];\n  }\n\n  var I, A, C;\n  w = new u(s.config);\n  return _GLOBAL_CUSTOM_OBJECTS = _assign({}, m), w;\n}\n\nfunction numberCompare(e, t) {\n  return e < t ? -1 : e > t ? 1 : 0;\n}\n\nfunction reverseNumberCompare(e, t) {\n  return -1 * numberCompare(e, t);\n}\n\nfunction stringToDType(e) {\n  switch (e) {\n    case \"float32\":\n      return \"float32\";\n\n    default:\n      throw new ValueError(\"Invalid dtype: \" + e);\n  }\n}\n\nfunction unique(e) {\n  if (null == e) return e;\n\n  for (var t = [], n = 0, r = e; n < r.length; n++) {\n    var i = r[n];\n    -1 === t.indexOf(i) && t.push(i);\n  }\n\n  return t;\n}\n\nfunction isObjectEmpty(e) {\n  if (null == e) throw new ValueError(\"Invalid value in obj: \" + JSON.stringify(e));\n\n  for (var t in e) {\n    if (e.hasOwnProperty(t)) return !1;\n  }\n\n  return !0;\n}\n\nfunction checkStringTypeUnionValue(e, t, n) {\n  if (null != n && e.indexOf(n) < 0) throw new ValueError(n + \" is not a valid \" + t + \".  Valid values are \" + e + \" or null/undefined.\");\n}\n\nfunction checkArrayTypeAndLength(e, t, n, r) {\n  return void 0 === n && (n = 0), void 0 === r && (r = 1 / 0), assert(n >= 0), assert(r >= n), Array.isArray(e) && e.length >= n && e.length <= r && e.every(function (e) {\n    return typeof e === t;\n  });\n}\n\nfunction calcL2Norms(e, t) {\n  return tidy(function () {\n    return sqrt(sum(mulStrict(e, e), t, !0));\n  });\n}\n\nvar Constraint = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.getConfig = function () {\n    return {};\n  }, t;\n}(serialization.Serializable),\n    MaxNorm = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    return n.defaultMaxValue = 2, n.defaultAxis = 0, n.maxValue = null != t.maxValue ? t.maxValue : n.defaultMaxValue, n.axis = null != t.axis ? t.axis : n.defaultAxis, n;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    var t = this;\n    return tidy(function () {\n      var n = calcL2Norms(e, t.axis),\n          r = clipByValue(n, 0, t.maxValue);\n      return mul(e, div(r, add(getScalar(epsilon()), n)));\n    });\n  }, t.prototype.getConfig = function () {\n    return {\n      maxValue: this.maxValue,\n      axis: this.axis\n    };\n  }, t.className = \"MaxNorm\", t;\n}(Constraint);\n\nserialization.registerClass(MaxNorm);\n\nvar UnitNorm = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    return n.defaultAxis = 0, n.axis = null != t.axis ? t.axis : n.defaultAxis, n;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    var t = this;\n    return tidy(function () {\n      return div(e, add(getScalar(epsilon()), calcL2Norms(e, t.axis)));\n    });\n  }, t.prototype.getConfig = function () {\n    return {\n      axis: this.axis\n    };\n  }, t.className = \"UnitNorm\", t;\n}(Constraint);\n\nserialization.registerClass(UnitNorm);\n\nvar NonNeg = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    return relu(e);\n  }, t.className = \"NonNeg\", t;\n}(Constraint);\n\nserialization.registerClass(NonNeg);\n\nvar MinMaxNorm = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    return n.defaultMinValue = 0, n.defaultMaxValue = 1, n.defaultRate = 1, n.defaultAxis = 0, n.minValue = null != t.minValue ? t.minValue : n.defaultMinValue, n.maxValue = null != t.maxValue ? t.maxValue : n.defaultMaxValue, n.rate = null != t.rate ? t.rate : n.defaultRate, n.axis = null != t.axis ? t.axis : n.defaultAxis, n;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    var t = this;\n    return tidy(function () {\n      var n = calcL2Norms(e, t.axis),\n          r = add(mul(getScalar(t.rate), clipByValue(n, t.minValue, t.maxValue)), mul(getScalar(1 - t.rate), n));\n      return mul(e, div(r, add(getScalar(epsilon()), n)));\n    });\n  }, t.prototype.getConfig = function () {\n    return {\n      minValue: this.minValue,\n      maxValue: this.maxValue,\n      rate: this.rate,\n      axis: this.axis\n    };\n  }, t.className = \"MinMaxNorm\", t;\n}(Constraint);\n\nserialization.registerClass(MinMaxNorm);\nvar CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP = {\n  maxNorm: \"MaxNorm\",\n  minMaxNorm: \"MinMaxNorm\",\n  nonNeg: \"NonNeg\",\n  unitNorm: \"UnitNorm\"\n};\n\nfunction serializeConstraint(e) {\n  return serializeKerasObject(e);\n}\n\nfunction deserializeConstraint(e, t) {\n  return void 0 === t && (t = {}), deserializeKerasObject(e, serialization.SerializationMap.getMap().classNameMap, t, \"constraint\");\n}\n\nfunction getConstraint(e) {\n  return null == e ? null : \"string\" == typeof e ? deserializeConstraint({\n    className: e in CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP ? CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP[e] : e,\n    config: {}\n  }) : e instanceof Constraint ? e : deserializeConstraint(e);\n}\n\nfunction maxNorm(e) {\n  return new MaxNorm(e);\n}\n\nfunction unitNorm(e) {\n  return new UnitNorm(e);\n}\n\nfunction nonNeg() {\n  return new NonNeg();\n}\n\nfunction minMaxNorm(e) {\n  return new MinMaxNorm(e);\n}\n\nvar exports_constraints = Object.freeze({\n  maxNorm: maxNorm,\n  unitNorm: unitNorm,\n  nonNeg: nonNeg,\n  minMaxNorm: minMaxNorm\n}),\n    VALID_DATA_FORMAT_VALUES = [\"channelsFirst\", \"channelsLast\"],\n    VALID_PADDING_MODE_VALUES = [\"valid\", \"same\", \"causal\"],\n    VALID_POOL_MODE_VALUES = [\"max\", \"avg\"],\n    VALID_BIDIRECTIONAL_MERGE_MODES = [\"sum\", \"mul\", \"concat\", \"ave\"],\n    nameMap = new Map();\n\nfunction checkDataFormat(e) {\n  checkStringTypeUnionValue(VALID_DATA_FORMAT_VALUES, \"DataFormat\", e);\n}\n\nfunction checkPaddingMode(e) {\n  checkStringTypeUnionValue(VALID_PADDING_MODE_VALUES, \"PaddingMode\", e);\n}\n\nfunction checkPoolMode(e) {\n  checkStringTypeUnionValue(VALID_POOL_MODE_VALUES, \"PoolMode\", e);\n}\n\nvar _nameScopeStack = [],\n    _nameScopeDivider = \"/\";\n\nfunction nameScope(e, t) {\n  _nameScopeStack.push(e);\n\n  try {\n    var n = t();\n    return _nameScopeStack.pop(), n;\n  } catch (e) {\n    throw _nameScopeStack.pop(), e;\n  }\n}\n\nfunction currentNameScopePrefix() {\n  return 0 === _nameScopeStack.length ? \"\" : _nameScopeStack.join(_nameScopeDivider) + _nameScopeDivider;\n}\n\nfunction getScopedTensorName(e) {\n  if (!isValidTensorName(e)) throw new Error(\"Not a valid tensor name: '\" + e + \"'\");\n  return currentNameScopePrefix() + e;\n}\n\nfunction getUniqueTensorName(e) {\n  if (!isValidTensorName(e)) throw new Error(\"Not a valid tensor name: '\" + e + \"'\");\n  nameMap.has(e) || nameMap.set(e, 0);\n  var t = nameMap.get(e);\n\n  if (nameMap.set(e, nameMap.get(e) + 1), t > 0) {\n    var n = e + \"_\" + t;\n    return nameMap.set(n, 1), n;\n  }\n\n  return e;\n}\n\nvar tensorNameRegex = new RegExp(/^[A-Za-z][-A-Za-z0-9\\._\\/]*$/);\n\nfunction isValidTensorName(e) {\n  return !!e.match(tensorNameRegex);\n}\n\nfunction isInteger(e) {\n  return e === parseInt(e.toString(), 10);\n}\n\nfunction arrayProd(e, t, n) {\n  null == t && (t = 0), null == n && (n = e.length);\n\n  for (var r = 1, i = t; i < n; ++i) {\n    r *= e[i];\n  }\n\n  return r;\n}\n\nfunction toArray1D(e) {\n  return e = Array.isArray(e) ? new Float32Array(e) : e, tensor1d(e);\n}\n\nfunction min$1(e) {\n  return min(toArray1D(e)).dataSync()[0];\n}\n\nfunction max$1(e) {\n  return max(toArray1D(e)).dataSync()[0];\n}\n\nfunction range(e, t) {\n  if (t < e) throw new ValueError(\"end (\" + t + \") < begin (\" + e + \") is forbidden.\");\n\n  for (var n = [], r = e; r < t; ++r) {\n    n.push(r);\n  }\n\n  return n;\n}\n\nfunction cast$1(e, t) {\n  return e.asType(t);\n}\n\nfunction expandDims$1(e, t) {\n  void 0 === t && (t = -1);\n  var n = e.shape.slice();\n  return t < 0 && (t = n.length + t + 1), n.splice(t, 0, 1), e.reshape(n);\n}\n\nfunction repeat(e, t) {\n  return tidy(function () {\n    if (2 !== e.shape.length) throw new ValueError(\"repeat() expects a rank-2 tensor, but received a rank-\" + e.shape.length + \" tensor.\");\n    return tile$1(expandDims$1(e, 1), [1, t, 1]);\n  });\n}\n\nfunction flatten(e) {\n  var t = [arrayProd(e.shape)];\n  return e.reshape(t);\n}\n\nfunction batchFlatten(e) {\n  if (e.rank <= 1) throw new ValueError(\"batchFlatten requires a minimum rank of 2. Got rank: \" + e.rank + \".\");\n  var t = [e.shape[0], arrayProd(e.shape, 1)];\n  return e.reshape(t);\n}\n\nfunction sliceAlongFirstAxis(e, t, n) {\n  return tidy(function () {\n    switch (e.rank) {\n      case 1:\n        return slice1d(e, t, n);\n\n      case 2:\n        return slice2d(e, [t, 0], [n, e.shape[1]]);\n\n      case 3:\n        return slice3d(e, [t, 0, 0], [n, e.shape[1], e.shape[2]]);\n\n      case 4:\n        return slice4d(e, [t, 0, 0, 0], [n, e.shape[1], e.shape[2], e.shape[3]]);\n\n      default:\n        throw new ValueError(\"sliceAlongFirstAxis() received an unsupported tensor rank: \" + e.rank);\n    }\n  });\n}\n\nfunction sliceAlongLastAxis(e, t, n) {\n  return tidy(function () {\n    switch (e.rank) {\n      case 1:\n        return slice1d(e, t, n);\n\n      case 2:\n        return slice2d(e, [0, t], [e.shape[0], n]);\n\n      case 3:\n        return slice3d(e, [0, 0, t], [e.shape[0], e.shape[1], n]);\n\n      case 4:\n        return slice4d(e, [0, 0, 0, t], [e.shape[0], e.shape[1], e.shape[2], n]);\n\n      default:\n        throw new ValueError(\"sliceAlongLastAxis() received an unsupported tensor rank: \" + e.rank);\n    }\n  });\n}\n\nfunction sliceAlongAxis(e, t, n, r) {\n  return tidy(function () {\n    switch (e.rank) {\n      case 1:\n        return slice1d(e, t, n);\n\n      case 2:\n        switch (r) {\n          case 1:\n            return sliceAlongFirstAxis(e, t, n);\n\n          case 2:\n            return sliceAlongLastAxis(e, t, n);\n\n          default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \" + r);\n        }\n\n      case 3:\n        switch (r) {\n          case 1:\n            return sliceAlongFirstAxis(e, t, n);\n\n          case 2:\n            return slice3d(e, [0, t, 0], [e.shape[0], n, e.shape[2]]);\n\n          case 3:\n            return sliceAlongLastAxis(e, t, n);\n\n          default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \" + r);\n        }\n\n      case 4:\n        switch (r) {\n          case 1:\n            return sliceAlongFirstAxis(e, t, n);\n\n          case 2:\n            return slice4d(e, [0, t, 0, 0], [e.shape[0], n, e.shape[2], e.shape[3]]);\n\n          case 3:\n            return slice4d(e, [0, 0, t, 0], [e.shape[0], e.shape[1], n, e.shape[3]]);\n\n          case 4:\n            return sliceAlongLastAxis(e, t, n);\n\n          default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \" + r);\n        }\n\n      default:\n        throw new ValueError(\"sliceAlongLastAxis() received an unsupported tensor rank: \" + e.rank);\n    }\n  });\n}\n\nfunction concatenate(e, t) {\n  var n;\n  return void 0 === t && (t = -1), t < 0 && (t = 0 !== (n = e[0].rank) ? n : 0), t === e[0].rank && (t = -1), concat(e, t);\n}\n\nfunction concatAlongFirstAxis(e, t) {\n  switch (e.rank) {\n    case 1:\n      return concat1d([e, t]);\n\n    case 2:\n      return concat2d([e, t], 0);\n\n    case 3:\n      return concat3d([e, t], 0);\n\n    case 4:\n      return concat4d([e, t], 0);\n\n    default:\n      throw new ValueError(\"concatAlongFirstAxis() received an unsupported tensor rank: \" + e.rank);\n  }\n}\n\nfunction tile$1(e, t) {\n  if (Array.isArray(t) || (t = [t]), e.rank !== t.length) throw new ValueError(\"The length of input n (\" + t.length + \") does not match the number of dimensions in input x (\" + e.rank + \")\");\n  return tile(e, t);\n}\n\nfunction randomNormal$1(e, t, n, r, i) {\n  return void 0 === t && (t = 0), void 0 === n && (n = 1), randomNormal(e, t, n, r, i);\n}\n\nfunction dot(e, t) {\n  if (e.rank < 2 || t.rank < 2) throw new NotImplementedError(\"dot requires both inputs to be rank >= 2 but got x shape = \" + e.shape + \" and y shape = \" + t.shape);\n  if (t.rank >= 3 && (r = e.shape.slice(-1)[0]) !== (o = t.shape.slice(-2)[0])) throw new NotImplementedError(\"If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = \" + e.shape + \" and  y shape = \" + t.shape);\n  if (2 === e.rank && 2 === t.rank) return matMul(e, t);\n  var n = e.shape.slice(),\n      r = n.pop();\n  e = e.reshape([-1, r]);\n  var i = t.shape.slice(),\n      a = i.pop(),\n      o = i.pop(),\n      s = i.concat([a]),\n      l = Array.from({\n    length: t.rank\n  }, function (e, n) {\n    return 0 === n ? t.rank - 2 : n <= t.rank - 2 ? n - 1 : n;\n  });\n  t = t.transpose(l).reshape([o, -1]);\n  var u = n.concat(s);\n  return matMul(e, t).reshape(u);\n}\n\nfunction gather$1(e, t, n) {\n  return tidy(function () {\n    return t = Array.isArray(t) ? tensor1d(t, \"int32\") : t.toInt(), gather(e, t, n);\n  });\n}\n\nfunction square(e) {\n  return mulStrict(e, e);\n}\n\nfunction biasAdd(e, t, n) {\n  return tidy(function () {\n    if (null == n && (n = imageDataFormat()), checkDataFormat(n), 1 !== t.rank && t.rank !== e.rank) throw new ValueError(\"Unexpected bias dimensions: \" + t.rank + \"; expected it to be 1 or \" + e.rank);\n    var r,\n        i = t.shape;\n    if (5 === e.rank) \"channelsFirst\" === n ? r = 1 === i.length ? e.add(t.reshape([1, i[0], 1, 1, 1])) : e.add(t.reshape([1, i[3], i[0], i[1], i[2]])) : \"channelsLast\" === n && (r = 1 === i.length ? e.add(t.reshape([1, 1, 1, 1, i[0]])) : e.add(t.reshape([1].concat(i))));else if (4 === e.rank) \"channelsFirst\" === n ? r = 1 === i.length ? e.add(t.reshape([1, i[0], 1, 1])) : e.add(t.reshape([1, i[2], i[0], i[1]])) : \"channelsLast\" === n && (r = 1 === i.length ? e.add(t.reshape([1, 1, 1, i[0]])) : e.add(t.reshape([1].concat(i))));else if (3 === e.rank) \"channelsFirst\" === n ? r = 1 === i.length ? e.add(t.reshape([1, i[0], 1])) : e.add(t.reshape([1, i[1], i[0]])) : \"channelsLast\" === n && (r = 1 === i.length ? e.add(t.reshape([1, 1, i[0]])) : e.add(t.reshape([1].concat(i))));else {\n      if (!(e.rank < 3)) throw new ValueError(\"Unsupported input rank by biasAdd: \" + e.rank);\n      r = e.add(t);\n    }\n    return r;\n  });\n}\n\nfunction elu$1(e, t) {\n  if (void 0 === t && (t = 1), 1 !== t) throw new NotImplementedError(\"Support for alpha values other than 1 (\" + t + \") is not implemented yet.\");\n  return elu(e);\n}\n\nfunction softsign(e) {\n  return tidy(function () {\n    return div(e, add(getScalar(1), abs(e)));\n  });\n}\n\nfunction dropout(e, t, n, r) {\n  return tidy(function () {\n    if (null != n && !util.arraysEqual(e.shape, n)) throw new NotImplementedError(\"Non-default noise shape is not implemented yet: \" + JSON.stringify(n));\n    if (null != r) throw new NotImplementedError(\"seed is not implemented for dropout yet.\");\n    var i = step(add(neg(t), randomUniform(e.shape, 0, 1, \"float32\")));\n    return i = mul(div(getScalar(1), sub(getScalar(1), t)), i), mul(e, i);\n  });\n}\n\nfunction hardSigmoid(e) {\n  return tidy(function () {\n    var t = add(getScalar(.5), mul(getScalar(.2), e));\n    return clipByValue(t, 0, 1);\n  });\n}\n\nfunction inTrainPhase(e, t, n) {\n  return void 0 === n && (n = !1), n ? e() : t();\n}\n\nvar VALID_FAN_MODE_VALUES = [\"fanIn\", \"fanOut\", \"fanAvg\"],\n    VALID_DISTRIBUTION_VALUES = [\"normal\", \"uniform\"];\n\nfunction checkFanMode(e) {\n  checkStringTypeUnionValue(VALID_FAN_MODE_VALUES, \"FanMode\", e);\n}\n\nfunction checkDistribution(e) {\n  checkStringTypeUnionValue(VALID_DISTRIBUTION_VALUES, \"Distribution\", e);\n}\n\nvar Initializer = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.fromConfigUsesCustomObjects = function () {\n    return !1;\n  }, t.prototype.getConfig = function () {\n    return {};\n  }, t;\n}(serialization.Serializable),\n    Zeros = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e, t) {\n    return zeros(e, t);\n  }, t.className = \"Zeros\", t;\n}(Initializer);\n\nserialization.registerClass(Zeros);\n\nvar Ones = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e, t) {\n    return ones(e, t);\n  }, t.className = \"Ones\", t;\n}(Initializer);\n\nserialization.registerClass(Ones);\n\nvar Constant = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    if (\"object\" != typeof t) throw new ValueError(\"Expected argument of type ConstantConfig but got \" + t);\n    if (void 0 === t.value) throw new ValueError(\"config must have value set but got \" + t);\n    return n.value = t.value, n;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      return mul(scalar(n.value), ones(e, t));\n    });\n  }, t.prototype.getConfig = function () {\n    return {\n      value: this.value\n    };\n  }, t.className = \"Constant\", t;\n}(Initializer);\n\nserialization.registerClass(Constant);\n\nvar RandomUniform = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    return n.DEFAULT_MINVAL = -.05, n.DEFAULT_MAXVAL = .05, n.minval = t.minval || n.DEFAULT_MINVAL, n.maxval = t.maxval || n.DEFAULT_MAXVAL, n.seed = t.seed, n;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e, t) {\n    return randomUniform(e, this.minval, this.maxval, t);\n  }, t.prototype.getConfig = function () {\n    return {\n      minval: this.minval,\n      maxval: this.maxval,\n      seed: this.seed\n    };\n  }, t.className = \"RandomUniform\", t;\n}(Initializer);\n\nserialization.registerClass(RandomUniform);\n\nvar RandomNormal = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    return n.DEFAULT_MEAN = 0, n.DEFAULT_STDDEV = .05, n.mean = t.mean || n.DEFAULT_MEAN, n.stddev = t.stddev || n.DEFAULT_STDDEV, n.seed = t.seed, n;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e, t) {\n    if (\"float32\" !== (t = t || \"float32\") && \"int32\" !== t) throw new NotImplementedError(\"randomNormal does not support dType \" + t + \".\");\n    return randomNormal$1(e, this.mean, this.stddev, t, this.seed);\n  }, t.prototype.getConfig = function () {\n    return {\n      mean: this.mean,\n      stddev: this.stddev,\n      seed: this.seed\n    };\n  }, t.className = \"RandomNormal\", t;\n}(Initializer);\n\nserialization.registerClass(RandomNormal);\n\nvar TruncatedNormal = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    return n.DEFAULT_MEAN = 0, n.DEFAULT_STDDEV = .05, n.mean = t.mean || n.DEFAULT_MEAN, n.stddev = t.stddev || n.DEFAULT_STDDEV, n.seed = t.seed, n;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e, t) {\n    if (\"float32\" !== (t = t || \"float32\") && \"int32\" !== t) throw new NotImplementedError(\"truncatedNormal does not support dType \" + t + \".\");\n    return truncatedNormal(e, this.mean, this.stddev, t, this.seed);\n  }, t.prototype.getConfig = function () {\n    return {\n      mean: this.mean,\n      stddev: this.stddev,\n      seed: this.seed\n    };\n  }, t.className = \"TruncatedNormal\", t;\n}(Initializer);\n\nserialization.registerClass(TruncatedNormal);\n\nvar Identity = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    return n.gain = null != t.gain ? scalar(t.gain) : getScalar(1), n;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      if (2 !== e.length || e[0] !== e[1]) throw new ValueError(\"Identity matrix initializer can only be used for 2D square matrices.\");\n      return mul(n.gain, eye(e[0]));\n    });\n  }, t.prototype.getConfig = function () {\n    return {\n      gain: this.gain.get()\n    };\n  }, t.className = \"Identity\", t;\n}(Initializer);\n\nfunction computeFans(e, t) {\n  var n, r;\n  if (void 0 === t && (t = \"channelsLast\"), checkDataFormat(t), 2 === e.length) n = e[0], r = e[1];else if (-1 !== [3, 4, 5].indexOf(e.length)) {\n    if (\"channelsFirst\" === t) {\n      var i = arrayProd(e, 2);\n      n = e[1] * i, r = e[0] * i;\n    } else if (\"channelsLast\" === t) {\n      i = arrayProd(e, 0, e.length - 2);\n      n = e[e.length - 2] * i, r = e[e.length - 1] * i;\n    }\n  } else {\n    var a = arrayProd(e);\n    n = Math.sqrt(a), r = Math.sqrt(a);\n  }\n  return [n, r];\n}\n\nserialization.registerClass(Identity);\n\nvar VarianceScaling = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    if (t.scale < 0) throw new ValueError(\"scale must be a positive float. Got: \" + t.scale);\n    return n.scale = null == t.scale ? 1 : t.scale, n.mode = t.mode, checkFanMode(n.mode), n.distribution = t.distribution, checkDistribution(n.distribution), n.seed = t.seed, n;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e, t) {\n    var n = computeFans(e),\n        r = n[0],\n        i = n[1],\n        a = this.scale;\n\n    if (\"fanIn\" === this.mode ? a /= Math.max(1, r) : \"fanOut\" === this.mode ? a /= Math.max(1, i) : a /= Math.max(1, (r + i) / 2), \"normal\" === this.distribution) {\n      var o = Math.sqrt(a);\n      if (\"float32\" !== (t = t || \"float32\") && \"int32\" !== t) throw new NotImplementedError(this.getClassName() + \" does not support dType \" + t + \".\");\n      return truncatedNormal(e, 0, o, t, this.seed);\n    }\n\n    var s = Math.sqrt(3 * a);\n    return randomUniform(e, -s, s, t);\n  }, t.prototype.getConfig = function () {\n    return {\n      scale: this.scale,\n      mode: this.mode,\n      distribution: this.distribution,\n      seed: this.seed\n    };\n  }, t.className = \"VarianceScaling\", t;\n}(Initializer);\n\nserialization.registerClass(VarianceScaling);\n\nvar GlorotUniform = function (e) {\n  function t(t) {\n    return e.call(this, {\n      scale: 1,\n      mode: \"fanAvg\",\n      distribution: \"uniform\",\n      seed: null == t ? null : t.seed\n    }) || this;\n  }\n\n  return __extends(t, e), t.prototype.getClassName = function () {\n    return VarianceScaling.className;\n  }, t.className = \"GlorotUniform\", t;\n}(VarianceScaling);\n\nserialization.registerClass(GlorotUniform);\n\nvar GlorotNormal = function (e) {\n  function t(t) {\n    return e.call(this, {\n      scale: 1,\n      mode: \"fanAvg\",\n      distribution: \"normal\",\n      seed: null == t ? null : t.seed\n    }) || this;\n  }\n\n  return __extends(t, e), t.prototype.getClassName = function () {\n    return VarianceScaling.className;\n  }, t.className = \"GlorotNormal\", t;\n}(VarianceScaling);\n\nserialization.registerClass(GlorotNormal);\n\nvar HeNormal = function (e) {\n  function t(t) {\n    return e.call(this, {\n      scale: 2,\n      mode: \"fanIn\",\n      distribution: \"normal\",\n      seed: null == t ? null : t.seed\n    }) || this;\n  }\n\n  return __extends(t, e), t.prototype.getClassName = function () {\n    return VarianceScaling.className;\n  }, t.className = \"HeNormal\", t;\n}(VarianceScaling);\n\nserialization.registerClass(HeNormal);\n\nvar HeUniform = function (e) {\n  function t(t) {\n    return e.call(this, {\n      scale: 2,\n      mode: \"fanIn\",\n      distribution: \"uniform\",\n      seed: null == t ? null : t.seed\n    }) || this;\n  }\n\n  return __extends(t, e), t.prototype.getClassName = function () {\n    return VarianceScaling.className;\n  }, t.className = \"HeUniform\", t;\n}(VarianceScaling);\n\nserialization.registerClass(HeUniform);\n\nvar LeCunNormal = function (e) {\n  function t(t) {\n    return e.call(this, {\n      scale: 1,\n      mode: \"fanIn\",\n      distribution: \"normal\",\n      seed: null == t ? null : t.seed\n    }) || this;\n  }\n\n  return __extends(t, e), t.prototype.getClassName = function () {\n    return VarianceScaling.className;\n  }, t.className = \"LeCunNormal\", t;\n}(VarianceScaling);\n\nserialization.registerClass(LeCunNormal);\n\nvar LeCunUniform = function (e) {\n  function t(t) {\n    return e.call(this, {\n      scale: 1,\n      mode: \"fanIn\",\n      distribution: \"uniform\",\n      seed: null == t ? null : t.seed\n    }) || this;\n  }\n\n  return __extends(t, e), t.prototype.getClassName = function () {\n    return VarianceScaling.className;\n  }, t.className = \"LeCunNormal\", t;\n}(VarianceScaling);\n\nserialization.registerClass(LeCunUniform);\n\nvar Orthogonal = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    if (n.DEFAULT_GAIN = 1, n.gain = null == t.gain ? n.DEFAULT_GAIN : t.gain, n.seed = t.seed, null != n.seed) throw new NotImplementedError(\"Random seed is not implemented for Orthogonal Initializer yet.\");\n    return n;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      if (2 !== e.length) throw new NotImplementedError(\"The Orthogonal Initializer does not support non-2D shapes yet.\");\n      e[0] * e[1] > 2e3 && console.warn(\"Orthogonal initializer is being called on a matrix with more than 2000 (\" + e[0] * e[1] + \") elements: Slowness may result.\");\n      var t = randomNormal$1(e[0] > e[1] ? [e[1], e[0]] : e, 0, 1, \"float32\"),\n          r = linalg.gramSchmidt(t);\n      return e[0] > e[1] && (r = r.transpose()), mul(getScalar(n.gain), r);\n    });\n  }, t.prototype.getConfig = function () {\n    return {\n      gain: this.gain,\n      seed: this.seed\n    };\n  }, t.className = \"Orthogonal\", t;\n}(Initializer);\n\nserialization.registerClass(Orthogonal);\nvar INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP = {\n  constant: \"Constant\",\n  glorotNormal: \"GlorotNormal\",\n  glorotUniform: \"GlorotUniform\",\n  heNormal: \"HeNormal\",\n  heUniform: \"HeUniform\",\n  identity: \"Identity\",\n  leCunNormal: \"LeCunNormal\",\n  leCunUniform: \"LeCunUniform\",\n  ones: \"Ones\",\n  orthogonal: \"Orthogonal\",\n  randomNormal: \"RandomNormal\",\n  randomUniform: \"RandomUniform\",\n  truncatedNormal: \"TruncatedNormal\",\n  varianceScaling: \"VarianceScaling\",\n  zeros: \"Zeros\"\n};\n\nfunction deserializeInitializer(e, t) {\n  return void 0 === t && (t = {}), deserializeKerasObject(e, serialization.SerializationMap.getMap().classNameMap, t, \"initializer\");\n}\n\nfunction serializeInitializer(e) {\n  return serializeKerasObject(e);\n}\n\nfunction getInitializer(e) {\n  if (\"string\" == typeof e) {\n    var t = e in INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP ? INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP[e] : e;\n    return \"GlorotNormal\" === t ? new GlorotNormal() : \"GlorotUniform\" === t ? new GlorotUniform() : \"HeNormal\" === t ? new HeNormal() : \"HeUniform\" === t ? new HeUniform() : \"LeCunNormal\" === t ? new LeCunNormal() : \"LeCunUniform\" === t ? new LeCunUniform() : deserializeInitializer({\n      className: t,\n      config: {}\n    });\n  }\n\n  return e instanceof Initializer ? e : deserializeInitializer(e);\n}\n\nfunction zeros$1() {\n  return new Zeros();\n}\n\nfunction ones$1() {\n  return new Ones();\n}\n\nfunction constant(e) {\n  return new Constant(e);\n}\n\nfunction randomUniform$1(e) {\n  return new RandomUniform(e);\n}\n\nfunction randomNormal$2(e) {\n  return new RandomNormal(e);\n}\n\nfunction truncatedNormal$1(e) {\n  return new TruncatedNormal(e);\n}\n\nfunction identity(e) {\n  return new Identity(e);\n}\n\nfunction varianceScaling(e) {\n  return new VarianceScaling(e);\n}\n\nfunction glorotUniform(e) {\n  return new GlorotUniform(e);\n}\n\nfunction glorotNormal(e) {\n  return new GlorotNormal(e);\n}\n\nfunction heNormal(e) {\n  return new HeNormal(e);\n}\n\nfunction heUniform(e) {\n  return new HeUniform(e);\n}\n\nfunction leCunNormal(e) {\n  return new LeCunNormal(e);\n}\n\nfunction leCunUniform(e) {\n  return new LeCunUniform(e);\n}\n\nfunction orthogonal(e) {\n  return new Orthogonal(e);\n}\n\nvar exports_initializers = Object.freeze({\n  zeros: zeros$1,\n  ones: ones$1,\n  constant: constant,\n  randomUniform: randomUniform$1,\n  randomNormal: randomNormal$2,\n  truncatedNormal: truncatedNormal$1,\n  identity: identity,\n  varianceScaling: varianceScaling,\n  glorotUniform: glorotUniform,\n  glorotNormal: glorotNormal,\n  heNormal: heNormal,\n  heUniform: heUniform,\n  leCunNormal: leCunNormal,\n  leCunUniform: leCunUniform,\n  orthogonal: orthogonal\n});\n\nfunction isArrayOfShapes(e) {\n  return Array.isArray(e) && Array.isArray(e[0]);\n}\n\nfunction normalizeShapeList(e) {\n  return 0 === e.length ? [] : Array.isArray(e[0]) ? e : [e];\n}\n\nfunction getExactlyOneTensor(e) {\n  var t;\n\n  if (Array.isArray(e)) {\n    if (1 !== e.length) throw new ValueError(\"Expected Tensor length to be 1; got \" + e.length);\n    t = e[0];\n  } else t = e;\n\n  return t;\n}\n\nfunction getExactlyOneShape(e) {\n  if (Array.isArray(e) && Array.isArray(e[0])) {\n    if (1 === e.length) return (e = e)[0];\n    throw new ValueError(\"Expected exactly 1 Shape; got \" + e.length);\n  }\n\n  return e;\n}\n\nfunction countParamsInWeights(e) {\n  for (var t = 0, n = 0, r = e; n < r.length; n++) {\n    var i = r[n];\n    0 === i.shape.length ? t += 1 : t += i.shape.reduce(function (e, t) {\n      return e * t;\n    });\n  }\n\n  return t;\n}\n\nvar DEFAULT_VARIABLE_NAME_PREFIX = \"Variable\",\n    LayerVariable = function () {\n  function e(e, t, n, r, i) {\n    void 0 === t && (t = \"float32\"), void 0 === n && (n = DEFAULT_VARIABLE_NAME_PREFIX), void 0 === r && (r = !0), void 0 === i && (i = null), this.dtype = null == t ? \"float32\" : t, this.shape = e.shape, this.id = getNextUniqueTensorId(), n = null == n ? DEFAULT_VARIABLE_NAME_PREFIX : n, this.originalName = getScopedTensorName(n), this.name = getUniqueTensorName(this.originalName), this.trainable = r, this.constraint = i, this.val = variable(e, this.trainable, this.name, this.dtype);\n  }\n\n  return e.prototype.read = function () {\n    return this.assertNotDisposed(), this.val;\n  }, e.prototype.write = function (e) {\n    return this.assertNotDisposed(), checkShapesMatch(this.val, e), this.val.id !== e.id && (this.val.assign(e), null != this.constraint && this.val.assign(this.constraint.apply(this.val))), this;\n  }, e.prototype.dispose = function () {\n    this.assertNotDisposed(), this.val.dispose();\n  }, e.prototype.assertNotDisposed = function () {\n    if (this.val.isDisposed) throw new Error(\"LayersVariable \" + this.name + \" is already disposed.\");\n  }, e;\n}();\n\nfunction checkShapesMatch(e, t) {\n  if (e.shape.toString() !== t.shape.toString()) throw new Error(\"Shape mismatch: \" + JSON.stringify(e.shape) + \" vs. \" + JSON.stringify(t.shape));\n}\n\nfunction batchGetValue(e) {\n  return e.map(function (e) {\n    return e.read();\n  });\n}\n\nfunction batchSetValue(e) {\n  e.forEach(function (e) {\n    e[0].write(e[1]);\n  });\n}\n\nvar InputSpec = function () {\n  return function (e) {\n    this.dtype = e.dtype, this.shape = e.shape, null != e.shape ? this.ndim = e.shape.length : this.ndim = e.ndim, this.maxNDim = e.maxNDim, this.minNDim = e.minNDim, this.axes = e.axes || {};\n  };\n}(),\n    SymbolicTensor = function () {\n  return function (e, t, n, r, i, a, o) {\n    this.dtype = e, this.shape = t, this.sourceLayer = n, this.inputs = r, this.callArgs = i, this.outputTensorIndex = o, this.id = getNextUniqueTensorId(), null != a && (this.originalName = getScopedTensorName(a), this.name = getUniqueTensorName(this.originalName)), this.rank = t.length;\n  };\n}(),\n    _nextNodeID = 0,\n    Node = function () {\n  function e(e, t) {\n    this.callArgs = t, this.id = _nextNodeID++, this.outboundLayer = e.outboundLayer, this.inboundLayers = e.inboundLayers, this.nodeIndices = e.nodeIndices, this.tensorIndices = e.tensorIndices, this.inputTensors = e.inputTensors, this.outputTensors = e.outputTensors, this.inputMasks = e.inputMasks, this.outputMasks = e.outputMasks, this.inputShapes = e.inputShapes, this.outputShapes = e.outputShapes;\n\n    for (var n = 0, r = e.inboundLayers; n < r.length; n++) {\n      var i = r[n];\n      null != i && i.outboundNodes.push(this);\n    }\n\n    e.outboundLayer.inboundNodes.push(this);\n  }\n\n  return e.prototype.getConfig = function () {\n    for (var e = [], t = 0, n = this.inboundLayers; t < n.length; t++) {\n      var r = n[t];\n      null != r ? e.push(r.name) : e.push(null);\n    }\n\n    return {\n      outboundLayer: this.outboundLayer ? this.outboundLayer.name : null,\n      inboundLayers: e,\n      nodeIndices: this.nodeIndices,\n      tensorIndices: this.tensorIndices\n    };\n  }, e;\n}(),\n    _nextLayerID = 0,\n    Layer = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    n._callHook = null, n._addedWeightNames = [], n._stateful = !1, n.id = _nextLayerID++, n.activityRegularizer = null, n.inputSpec = null, n.supportsMasking = !1, n._trainableWeights = [], n._nonTrainableWeights = [], n._losses = [], n._updates = [], n._built = !1, n.inboundNodes = [], n.outboundNodes = [];\n    var r = t.name;\n\n    if (!r) {\n      var i = n.getClassName();\n      r = toSnakeCase(i) + \"_\" + getUid(i);\n    }\n\n    if (n.name = r, n.trainable = null == t.trainable || t.trainable, n.updatable = null == t.updatable || t.updatable, null != t.inputShape || null != t.batchInputShape) {\n      var a = void 0;\n      if (null != t.batchInputShape) a = t.batchInputShape;else if (null != t.inputShape) {\n        var o = null;\n        null != t.batchSize && (o = t.batchSize), a = [o].concat(t.inputShape);\n      }\n      n.batchInputShape = a;\n      var s = t.dtype;\n      null == s && (s = t.inputDType), null == s && (s = \"float32\"), n.dtype = s;\n    }\n\n    return null != t.weights ? n.initialWeights = t.weights : n.initialWeights = null, n._refCount = null, n.fastWeightInitDuringBuild = !1, n;\n  }\n\n  return __extends(t, e), t.nodeKey = function (e, t) {\n    return e.name + \"_ib-\" + t.toString();\n  }, t.prototype.getNodeAtIndex = function (e, t) {\n    if (0 === this.inboundNodes.length) throw new RuntimeError(\"The layer has never been called and thus has no defined \" + t + \".\");\n    if (this.inboundNodes.length <= e) throw new ValueError(\"Asked to get \" + t + \" at node \" + e + \", but the layer has only \" + this.inboundNodes.length + \" inbound nodes.\");\n    return this.inboundNodes[e];\n  }, t.prototype.getInputAt = function (e) {\n    return singletonOrArray(this.getNodeAtIndex(e, \"input\").inputTensors);\n  }, t.prototype.getOutputAt = function (e) {\n    return singletonOrArray(this.getNodeAtIndex(e, \"output\").outputTensors);\n  }, Object.defineProperty(t.prototype, \"input\", {\n    get: function get() {\n      if (this.inboundNodes.length > 1) throw new AttributeError(\"Layer \" + this.name + ' has multiple inbound nodes, hence the notion of \"layer input\" is ill-defined. Use `getInputAt(nodeIndex)` instead.');\n      if (0 === this.inboundNodes.length) throw new AttributeError(\"Layer \" + this.name + \" is not connected, no input to return.\");\n      return singletonOrArray(this.getNodeAtIndex(0, \"input\").inputTensors);\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"output\", {\n    get: function get() {\n      if (0 === this.inboundNodes.length) throw new AttributeError(\"Layer \" + this.name + \" has no inbound nodes.\");\n      if (this.inboundNodes.length > 1) throw new AttributeError(\"Layer \" + this.name + ' has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `getOutputAt(nodeIndex)` instead.');\n      return singletonOrArray(this.getNodeAtIndex(0, \"output\").outputTensors);\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"losses\", {\n    get: function get() {\n      return this._losses;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.calculateLosses = function () {\n    return this.losses.map(function (e) {\n      return e();\n    });\n  }, Object.defineProperty(t.prototype, \"updates\", {\n    get: function get() {\n      return this._updates;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"built\", {\n    get: function get() {\n      return this._built;\n    },\n    set: function set(e) {\n      this._built = e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"trainableWeights\", {\n    get: function get() {\n      return this.trainable ? this._trainableWeights : [];\n    },\n    set: function set(e) {\n      this._trainableWeights = e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"nonTrainableWeights\", {\n    get: function get() {\n      return this.trainable ? this._nonTrainableWeights : this._trainableWeights.concat(this._nonTrainableWeights);\n    },\n    set: function set(e) {\n      this._nonTrainableWeights = e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"weights\", {\n    get: function get() {\n      return this.trainableWeights.concat(this.nonTrainableWeights);\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"stateful\", {\n    get: function get() {\n      return this._stateful;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.resetStates = function () {\n    if (!this.stateful) throw new Error(\"Cannot call the resetStates() method of a non-stateful Layer object.\");\n  }, t.prototype.assertInputCompatibility = function (e) {\n    if (e = toList(e), null != this.inputSpec && 0 !== this.inputSpec.length) {\n      var t = toList(this.inputSpec);\n      if (e.length !== t.length) throw new ValueError(\"Layer \" + this.name + \" expects \" + t.length + \" inputs, but it received \" + e.length + \" input tensors. Input received: \" + e);\n\n      for (var n = 0; n < e.length; n++) {\n        var r = e[n],\n            i = t[n];\n\n        if (null != i) {\n          var a = r.rank;\n          if (null != i.ndim && a !== i.ndim) throw new ValueError(\"Input \" + n + \" is incompatible with layer \" + this.name + \": expected ndim=\" + i.ndim + \", found ndim=\" + a);\n          if (null != i.maxNDim && a > i.maxNDim) throw new ValueError(\"Input \" + n + \" is incompatible with layer \" + this.name + \": expected max_ndim=\" + i.maxNDim + \", found ndim=\" + a);\n          if (null != i.minNDim && a < i.minNDim) throw new ValueError(\"Input \" + n + \" is incompatible with layer \" + this.name + \": expected min_ndim=\" + i.minNDim + \", found ndim=\" + a + \".\");\n          if (null != i.dtype && r.dtype !== i.dtype) throw new ValueError(\"Input \" + n + \" is incompatible with layer \" + this.name + \" : expected dtype=\" + i.dtype + \", found dtype=\" + r.dtype + \".\");\n\n          if (i.axes) {\n            var o = r.shape;\n\n            for (var s in i.axes) {\n              var l = Number(s),\n                  u = i.axes[s],\n                  c = l >= 0 ? o[l] : o[o.length + l];\n              if (null != u && -1 === [u, null].indexOf(c)) throw new ValueError(\"Input \" + n + \" is incompatible with layer \" + this.name + \": expected axis \" + l + \" of input shape to have value \" + u + \" but got shape \" + o + \".\");\n            }\n          }\n\n          if (null != i.shape) for (var p = 0; p < i.shape.length; ++p) {\n            var h = i.shape[p],\n                d = r.shape[p];\n            if (null != h && null != d && h !== d) throw new ValueError(\"Input \" + n + \" is incompatible with layer \" + this.name + \": expected shape=\" + i.shape + \", found shape=${xShape}.\");\n          }\n        }\n      }\n    }\n  }, t.prototype.call = function (e, t) {\n    return e;\n  }, t.prototype.invokeCallHook = function (e, t) {\n    null != this._callHook && this._callHook(e, t);\n  }, t.prototype.setCallHook = function (e) {\n    this._callHook = e;\n  }, t.prototype.clearCallHook = function () {\n    this._callHook = null;\n  }, t.prototype.apply = function (e, t) {\n    var n = this;\n    t = t || {}, this.assertNotDisposed();\n\n    for (var r = toList(e), i = !0, a = 0, o = r; a < o.length; a++) {\n      if (!(o[a] instanceof SymbolicTensor)) {\n        i = !1;\n        break;\n      }\n    }\n\n    for (var s = !0, l = 0, u = r; l < u.length; l++) {\n      if (u[l] instanceof SymbolicTensor) {\n        s = !1;\n        break;\n      }\n    }\n\n    if (i === s) throw new ValueError(\"Arguments to apply() must be all SymbolicTensors or all Tensors\");\n    return nameScope(this.name, function () {\n      if (!n.built) {\n        n.assertInputCompatibility(e);\n\n        for (var i = [], a = 0, o = toList(e); a < o.length; a++) {\n          var l = o[a];\n          i.push(l.shape);\n        }\n\n        n.build(singletonOrArray(i)), n.built = !0, n.initialWeights && n.setWeights(n.initialWeights), null === n._refCount && s && (n._refCount = 1);\n      }\n\n      if (n.assertInputCompatibility(e), s) {\n        for (var u = [], c = 0, p = toList(g = n.call(e, t)); c < p.length; c++) {\n          var h = p[c];\n          -1 !== r.indexOf(h) && (h = h.clone()), u.push(h);\n        }\n\n        if (g = singletonOrArray(u), null != n.activityRegularizer) throw new NotImplementedError(\"Layer invocation in the presence of activity regularizer(s) is not supported yet.\");\n        return g;\n      }\n\n      var d = collectInputShape(e),\n          f = n.computeOutputShape(d),\n          g = void 0,\n          m = guessOutputDType(e);\n      if (n.warnOnIncompatibleInputShape(Array.isArray(e) ? d[0] : d), g = null != f && f.length > 0 && Array.isArray(f[0]) ? f.map(function (r, i) {\n        return new SymbolicTensor(m, r, n, toList(e), t, n.name, i);\n      }) : new SymbolicTensor(m, f, n, toList(e), t, n.name), n.addInboundNode(e, g, null, null, d, f, t), n._refCount++, null != n.activityRegularizer) throw new NotImplementedError(\"Layer invocation in the presence of activity regularizer(s) is not supported yet.\");\n      return g;\n    });\n  }, t.prototype.warnOnIncompatibleInputShape = function (e) {\n    if (null != this.batchInputShape) if (e.length !== this.batchInputShape.length) console.warn(\"The rank of the input tensor provided (shape: \" + JSON.stringify(e) + \") does not match that of the batchInputShape (\" + JSON.stringify(this.batchInputShape) + \") of the layer \" + this.name);else {\n      var t = !1;\n      this.batchInputShape.forEach(function (n, r) {\n        null != n && null != e[r] && e[r] !== n && (t = !0);\n      }), t && console.warn(\"The shape of the input tensor (\" + JSON.stringify(e) + \") does not match the expectation of layer \" + this.name + \": \" + JSON.stringify(this.batchInputShape));\n    }\n  }, Object.defineProperty(t.prototype, \"outputShape\", {\n    get: function get() {\n      if (null == this.inboundNodes || 0 === this.inboundNodes.length) throw new AttributeError(\"The layer \" + this.name + \" has never been called and thus has no defined output shape.\");\n\n      for (var e = [], t = 0, n = this.inboundNodes; t < n.length; t++) {\n        var r = n[t],\n            i = JSON.stringify(r.outputShapes);\n        -1 === e.indexOf(i) && e.push(i);\n      }\n\n      if (1 === e.length) {\n        var a = this.inboundNodes[0].outputShapes;\n        return Array.isArray(a) && Array.isArray(a[0]) && 1 === a.length ? a[0] : a;\n      }\n\n      throw new AttributeError(\"The layer \" + this.name + ' has multiple inbound nodes with different output shapes. Hence the notion of \"outut shape\" is ill-defined for the layer.');\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.countParams = function () {\n    if (!this.built) throw new RuntimeError(\"You tried to call countParams() on \" + this.name + \", but the layer is not built yet. Build it first by calling build(batchInputShape).\");\n    return countParamsInWeights(this.weights);\n  }, t.prototype.build = function (e) {\n    this.built = !0;\n  }, t.prototype.getWeights = function (e) {\n    return void 0 === e && (e = !1), batchGetValue(e ? this.trainableWeights : this.weights);\n  }, t.prototype.setWeights = function (e) {\n    var t = this;\n    tidy(function () {\n      var n = t.weights;\n      if (n.length !== e.length) throw new ValueError('You called setWeights(weights) on layer \"' + t.name + '\" with a weight list of length ' + e.length + \", but the layer was expecting \" + n.length + \" weights. Provided weights: \" + e + \"...\");\n\n      if (0 !== n.length) {\n        for (var r = [], i = batchGetValue(n), a = 0; a < i.length; ++a) {\n          var o = i[a],\n              s = n[a],\n              l = e[a];\n          if (!util.arraysEqual(o.shape, l.shape)) throw new ValueError(\"Layer weight shape \" + o.shape + \" not compatible with provided weight shape \" + l.shape);\n          r.push([s, l]);\n        }\n\n        batchSetValue(r);\n      }\n    });\n  }, t.prototype.addWeight = function (e, t, n, r, i, a, o) {\n    if (-1 !== this._addedWeightNames.indexOf(e)) throw new ValueError(\"Duplicate weight name \" + e + \" for layer \" + this.name);\n    this._addedWeightNames.push(e), null == n && (n = \"float32\"), this.fastWeightInitDuringBuild && (r = getInitializer(\"zeros\"));\n    var s = r.apply(t, n),\n        l = new LayerVariable(s, n, e, a, o);\n    return s.dispose(), null != i && this.addLoss(function () {\n      return i.apply(l.read());\n    }), null == a && (a = !0), a ? this._trainableWeights.push(l) : this._nonTrainableWeights.push(l), l;\n  }, t.prototype.setFastWeightInitDuringBuild = function (e) {\n    this.fastWeightInitDuringBuild = e;\n  }, t.prototype.addLoss = function (e) {\n    var t;\n    null == e || Array.isArray(e) && 0 === e.length || (e = toList(e), void 0 !== this._losses && null !== this._losses && (t = this.losses).push.apply(t, e));\n  }, t.prototype.computeOutputShape = function (e) {\n    return e;\n  }, t.prototype.computeMask = function (e, t) {\n    var n = this;\n\n    if (!this.supportsMasking) {\n      if (null != t) {\n        if (!Array.isArray(t)) throw new TypeError(\"Layer \" + this.name + \" does not support masking, but was passed an inputMask.\");\n        t.forEach(function (e) {\n          if (null != e) throw new TypeError(\"Layer \" + n.name + \" does not support masking, but was passed an inputMask.\");\n        });\n      }\n\n      return null;\n    }\n\n    return t;\n  }, t.prototype.addInboundNode = function (e, t, n, r, i, a, o) {\n    void 0 === o && (o = null);\n    var s = toList(e);\n    t = toList(t), n = toList(n), r = toList(r), i = normalizeShapeList(i), a = normalizeShapeList(a);\n\n    for (var l = [], u = [], c = [], p = 0, h = s; p < h.length; p++) {\n      var d = h[p];\n      l.push(d.sourceLayer), u.push(d.nodeIndex), c.push(d.tensorIndex);\n    }\n\n    new Node({\n      outboundLayer: this,\n      inboundLayers: l,\n      nodeIndices: u,\n      tensorIndices: c,\n      inputTensors: s,\n      outputTensors: t,\n      inputMasks: n,\n      outputMasks: r,\n      inputShapes: i,\n      outputShapes: a\n    }, o);\n\n    for (var f = 0; f < t.length; f++) {\n      t[f].sourceLayer = this, t[f].nodeIndex = this.inboundNodes.length - 1, t[f].tensorIndex = f;\n    }\n  }, t.prototype.getConfig = function () {\n    var e = {\n      name: this.name,\n      trainable: this.trainable\n    };\n    return null != this.batchInputShape && (e.batchInputShape = this.batchInputShape), null != this.dtype && (e.dtype = this.dtype), e;\n  }, t.prototype.disposeWeights = function () {\n    return this.weights.forEach(function (e) {\n      return e.dispose();\n    }), this.weights.length;\n  }, t.prototype.assertNotDisposed = function () {\n    if (0 === this._refCount) throw new Error(\"Layer '\" + this.name + \"' is already disposed.\");\n  }, t.prototype.dispose = function () {\n    if (!this.built) throw new Error(\"Cannot dispose Layer \" + this.name + \" because it has not been built yet.\");\n    if (null === this._refCount) throw new Error(\"Cannot dispose Layer \" + this.name + \" because it has not been used yet.\");\n    this.assertNotDisposed();\n    var e = 0;\n    return 0 == --this._refCount && (e = this.disposeWeights()), {\n      refCountAfterDispose: this._refCount,\n      numDisposedVariables: e\n    };\n  }, t;\n}(serialization.Serializable);\n\nfunction collectInputShape(e) {\n  for (var t = [], n = 0, r = e = toList(e); n < r.length; n++) {\n    var i = r[n];\n    t.push(i.shape);\n  }\n\n  return singletonOrArray(t);\n}\n\nfunction guessOutputDType(e) {\n  return \"float32\";\n}\n\nfunction getSourceInputs(e, t, n) {\n  if ((null == t || null != n && n > 0) && (t = e.sourceLayer, n = e.nodeIndex), 0 === t.inboundNodes.length) return [e];\n  var r = t.inboundNodes[n];\n  if (0 === r.inboundLayers.length) return r.inputTensors;\n\n  for (var i = [], a = 0; a < r.inboundLayers.length; a++) {\n    for (var o = 0, s = getSourceInputs(r.inputTensors[a], r.inboundLayers[a], r.nodeIndices[a]); o < s.length; o++) {\n      var l = s[o];\n      -1 === i.indexOf(l) && i.push(l);\n    }\n  }\n\n  return i;\n}\n\nvar ModelLoggingVerbosity,\n    InputLayer = function (e) {\n  function t(t) {\n    var n = e.call(this, {\n      dtype: t.dtype,\n      name: null != t.name ? t.name : getUid(\"input\").toString()\n    }) || this;\n    if (null == t.batchSize && (t.batchSize = null), null == t.sparse && (t.sparse = !1), n.trainable = !1, n.built = !0, n.sparse = t.sparse, null != t.inputShape && null != t.batchInputShape) throw new ValueError(\"Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.\");\n    var r = t.batchInputShape;\n\n    if (null == r) {\n      if (null == t.inputShape) throw new ValueError(\"An InputLayer should be passed either a `batchInputShape` or an `inputShape`.\");\n      r = [t.batchSize].concat(t.inputShape);\n    } else if (null != t.batchSize) throw new ValueError(\"Cannot specify batchSize if batchInputShape isspecified when creating an InputLayer.\");\n\n    var i = t.dtype || \"float32\";\n    n.batchInputShape = r, n.dtype = i, n.inputSpec = [{\n      shape: r\n    }];\n    var a = new SymbolicTensor(n.dtype, n.batchInputShape, n, [], {}, n.name);\n    return a.nodeIndex = 0, a.tensorIndex = 0, new Node({\n      outboundLayer: n,\n      inboundLayers: [],\n      nodeIndices: [],\n      tensorIndices: [],\n      inputTensors: [a],\n      outputTensors: [a],\n      inputMasks: [null],\n      outputMasks: [null],\n      inputShapes: [r],\n      outputShapes: [r]\n    }), n;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e, t) {\n    throw new ValueError(\"Cannot pass any input to an InputLayer's apply() method. InputLayer name: \" + this.name);\n  }, t.prototype.dispose = function () {\n    return {\n      refCountAfterDispose: this._refCount,\n      numDisposedVariables: 0\n    };\n  }, t.prototype.getConfig = function () {\n    return {\n      batchInputShape: this.batchInputShape,\n      dtype: this.dtype,\n      sparse: this.sparse,\n      name: this.name\n    };\n  }, t.className = \"InputLayer\", t;\n}(Layer);\n\nfunction Input(e) {\n  if (null == e.batchShape && null == e.shape) throw new Error(\"Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.\");\n  if (null != e.batchShape && null != e.shape) throw new ValueError(\"Please provide either a `shape` or `batchShape` argument to Input, but not both.\");\n  var t = e.batchShape;\n  null != e.shape && null == t && (t = [null].concat(e.shape));\n  var n = e.dtype;\n  return null == n && (n = \"float32\"), new InputLayer({\n    batchInputShape: t,\n    name: e.name,\n    dtype: n,\n    sparse: e.sparse\n  }).inboundNodes[0].outputTensors[0];\n}\n\nfunction resolveScalarsInLogs(e) {\n  return __awaiter(this, void 0, void 0, function () {\n    var t, n, r, i, a, o, s, l;\n    return __generator(this, function (u) {\n      switch (u.label) {\n        case 0:\n          if (null == e) return [2];\n\n          for (i in t = [], n = [], r = [], e) {\n            \"number\" != typeof (a = e[i]) && (o = a, t.push(o.data()), n.push(i), r.push(o));\n          }\n\n          return [4, Promise.all(t)];\n\n        case 1:\n          for (s = u.sent(), l = 0; l < s.length; ++l) {\n            e[n[l]] = s[l][0];\n          }\n\n          return dispose(r), [2];\n      }\n    });\n  });\n}\n\nfunction disposeTensorsInLogs(e) {\n  if (null != e) for (var t in e) {\n    var n = e[t];\n    \"number\" != typeof n && n.dispose();\n  }\n}\n\nserialization.registerClass(InputLayer), function (e) {\n  e[e.SILENT = 0] = \"SILENT\", e[e.VERBOSE = 1] = \"VERBOSE\";\n}(ModelLoggingVerbosity || (ModelLoggingVerbosity = {}));\n\nvar BaseCallback = function () {\n  function e() {\n    this.validationData = null;\n  }\n\n  return e.prototype.setParams = function (e) {\n    this.params = e;\n  }, e.prototype.onEpochBegin = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return [2];\n      });\n    });\n  }, e.prototype.onEpochEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return [2];\n      });\n    });\n  }, e.prototype.onBatchBegin = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return [2];\n      });\n    });\n  }, e.prototype.onBatchEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return [2];\n      });\n    });\n  }, e.prototype.onTrainBegin = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return [2];\n      });\n    });\n  }, e.prototype.onTrainEnd = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return [2];\n      });\n    });\n  }, e.prototype.setModel = function (e) {}, e;\n}(),\n    CallbackList = function () {\n  function e(e, t) {\n    void 0 === t && (t = 10), null == e && (e = []), this.callbacks = e, this.queueLength = t;\n  }\n\n  return e.prototype.append = function (e) {\n    this.callbacks.push(e);\n  }, e.prototype.setParams = function (e) {\n    for (var t = 0, n = this.callbacks; t < n.length; t++) {\n      n[t].setParams(e);\n    }\n  }, e.prototype.setModel = function (e) {\n    for (var t = 0, n = this.callbacks; t < n.length; t++) {\n      n[t].setModel(e);\n    }\n  }, e.prototype.onEpochBegin = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n, r;\n      return __generator(this, function (i) {\n        switch (i.label) {\n          case 0:\n            null == t && (t = {}), n = 0, r = this.callbacks, i.label = 1;\n\n          case 1:\n            return n < r.length ? [4, r[n].onEpochBegin(e, t)] : [3, 4];\n\n          case 2:\n            i.sent(), i.label = 3;\n\n          case 3:\n            return n++, [3, 1];\n\n          case 4:\n            return [2];\n        }\n      });\n    });\n  }, e.prototype.onEpochEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n, r;\n      return __generator(this, function (i) {\n        switch (i.label) {\n          case 0:\n            null == t && (t = {}), n = 0, r = this.callbacks, i.label = 1;\n\n          case 1:\n            return n < r.length ? [4, r[n].onEpochEnd(e, t)] : [3, 4];\n\n          case 2:\n            i.sent(), i.label = 3;\n\n          case 3:\n            return n++, [3, 1];\n\n          case 4:\n            return [2];\n        }\n      });\n    });\n  }, e.prototype.onBatchBegin = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n, r;\n      return __generator(this, function (i) {\n        switch (i.label) {\n          case 0:\n            null == t && (t = {}), n = 0, r = this.callbacks, i.label = 1;\n\n          case 1:\n            return n < r.length ? [4, r[n].onBatchBegin(e, t)] : [3, 4];\n\n          case 2:\n            i.sent(), i.label = 3;\n\n          case 3:\n            return n++, [3, 1];\n\n          case 4:\n            return [2];\n        }\n      });\n    });\n  }, e.prototype.onBatchEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n, r;\n      return __generator(this, function (i) {\n        switch (i.label) {\n          case 0:\n            null == t && (t = {}), n = 0, r = this.callbacks, i.label = 1;\n\n          case 1:\n            return n < r.length ? [4, r[n].onBatchEnd(e, t)] : [3, 4];\n\n          case 2:\n            i.sent(), i.label = 3;\n\n          case 3:\n            return n++, [3, 1];\n\n          case 4:\n            return [2];\n        }\n      });\n    });\n  }, e.prototype.onTrainBegin = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      var t, n;\n      return __generator(this, function (r) {\n        switch (r.label) {\n          case 0:\n            null == e && (e = {}), t = 0, n = this.callbacks, r.label = 1;\n\n          case 1:\n            return t < n.length ? [4, n[t].onTrainBegin(e)] : [3, 4];\n\n          case 2:\n            r.sent(), r.label = 3;\n\n          case 3:\n            return t++, [3, 1];\n\n          case 4:\n            return [2];\n        }\n      });\n    });\n  }, e.prototype.onTrainEnd = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      var t, n;\n      return __generator(this, function (r) {\n        switch (r.label) {\n          case 0:\n            null == e && (e = {}), t = 0, n = this.callbacks, r.label = 1;\n\n          case 1:\n            return t < n.length ? [4, n[t].onTrainEnd(e)] : [3, 4];\n\n          case 2:\n            r.sent(), r.label = 3;\n\n          case 3:\n            return t++, [3, 1];\n\n          case 4:\n            return [2];\n        }\n      });\n    });\n  }, e;\n}(),\n    ModelTrainingYielder = function () {\n  function e(e) {\n    this.yieldEvery = e, this.batchCount = 0, this.batchDurationsMillis = [], this.autoYieldEveryBatches = null, this.batchStartMillis = util.now();\n  }\n\n  return e.prototype.maybeYieldOnBatch = function (t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var t, n;\n      return __generator(this, function (r) {\n        switch (r.label) {\n          case 0:\n            return \"auto\" !== this.yieldEvery ? [3, 5] : (this.batchCount++, null != this.autoYieldEveryBatches ? [3, 2] : (t = util.now(), [4, nextFrame()]));\n\n          case 1:\n            return r.sent(), this.batchCount > e.SKIP_FIRST_BATCHES && (this.batchDurationsMillis.push(t - this.batchStartMillis), this.batchDurationsMillis.length >= e.DECISION_BATCH_COUNT && (n = this.batchDurationsMillis.reduce(function (e, t) {\n              return e + t;\n            }) / this.batchDurationsMillis.length, this.autoYieldEveryBatches = Math.round(e.THRESHOLD_MILLIS / n), this.autoYieldEveryBatches < 1 && (this.autoYieldEveryBatches = 1))), this.batchStartMillis = util.now(), this.lastYieldBatchCount = this.batchCount, [3, 4];\n\n          case 2:\n            return this.batchCount - this.lastYieldBatchCount >= this.autoYieldEveryBatches ? [4, nextFrame()] : [3, 4];\n\n          case 3:\n            r.sent(), this.lastYieldBatchCount = this.batchCount, r.label = 4;\n\n          case 4:\n            return [3, 7];\n\n          case 5:\n            return \"batch\" !== this.yieldEvery ? [3, 7] : [4, nextFrame()];\n\n          case 6:\n            r.sent(), r.label = 7;\n\n          case 7:\n            return [2];\n        }\n      });\n    });\n  }, e.prototype.maybeYieldOnEpoch = function () {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        switch (e.label) {\n          case 0:\n            return \"epoch\" !== this.yieldEvery ? [3, 2] : [4, nextFrame()];\n\n          case 1:\n            e.sent(), e.label = 2;\n\n          case 2:\n            return [2];\n        }\n      });\n    });\n  }, e.SKIP_FIRST_BATCHES = 1, e.DECISION_BATCH_COUNT = 2, e.THRESHOLD_MILLIS = 16, e;\n}(),\n    BaseLogger = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    return n.yieldEvery = t || \"auto\", n;\n  }\n\n  return __extends(t, e), t.prototype.onTrainBegin = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return this.autoYielder = new ModelTrainingYielder(this.yieldEvery), [2];\n      });\n    });\n  }, t.prototype.onEpochBegin = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return this.seen = 0, this.totals = {}, [2];\n      });\n    });\n  }, t.prototype.onBatchEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var e,\n          n,\n          r,\n          i,\n          a = this;\n      return __generator(this, function (o) {\n        switch (o.label) {\n          case 0:\n            return [4, this.autoYielder.maybeYieldOnBatch(t)];\n\n          case 1:\n            for (i in o.sent(), null == t && (t = {}), e = null == t.size ? 0 : t.size, this.seen += e, n = function n(_n) {\n              var i = t[_n];\n              if (\"number\" == typeof i) r.totals.hasOwnProperty(_n) || (r.totals[_n] = 0), r.totals[_n] = r.totals[_n] + i * e;else {\n                var o = void 0;\n                _n in r.totals ? o = r.totals[_n] : r.totals[_n] = getScalar(0), r.totals[_n] = tidy(function () {\n                  return add(a.totals[_n], mul(i, getScalar(e)));\n                }), null != o && o.dispose();\n              }\n            }, r = this, t) {\n              n(i);\n            }\n\n            return [2];\n        }\n      });\n    });\n  }, t.prototype.onEpochEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var e,\n          n,\n          r,\n          i,\n          a,\n          o = this;\n      return __generator(this, function (s) {\n        switch (s.label) {\n          case 0:\n            return [4, this.autoYielder.maybeYieldOnEpoch()];\n\n          case 1:\n            if (s.sent(), null != t) for (e = function e(_e) {\n              if (null == n.totals[_e]) return \"continue\";\n              \"number\" == typeof n.totals[_e] ? t[_e] = n.totals[_e] / n.seen : tidy(function () {\n                t[_e] = mul(div(getScalar(1), getScalar(o.seen)), o.totals[_e]), o.totals[_e].dispose(), keep(t[_e]);\n              });\n            }, n = this, r = 0, i = this.params.metrics; r < i.length; r++) {\n              a = i[r], e(a);\n            }\n            return [2];\n        }\n      });\n    });\n  }, t;\n}(BaseCallback),\n    History = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.onTrainBegin = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (e) {\n        return this.epoch = [], this.history = {}, [2];\n      });\n    });\n  }, t.prototype.onEpochEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n;\n      return __generator(this, function (r) {\n        for (n in null == t && (t = {}), this.epoch.push(e), t) {\n          null == this.history[n] && (this.history[n] = []), this.history[n].push(t[n]);\n        }\n\n        return [2];\n      });\n    });\n  }, t.prototype.syncData = function () {\n    return __awaiter(this, void 0, void 0, function () {\n      var e, t, n, r, i, a, o, s, l;\n      return __generator(this, function (u) {\n        switch (u.label) {\n          case 0:\n            for (r in e = [], t = [], n = [], this.history) {\n              for (i = this.history[r], a = 0; a < i.length; ++a) {\n                \"number\" != typeof i[a] && (o = i[a], e.push(o.data()), t.push(r), n.push(a));\n              }\n            }\n\n            return [4, Promise.all(e)];\n\n          case 1:\n            for (s = u.sent(), l = 0; l < s.length; ++l) {\n              this.history[t[l]][n[l]].dispose(), this.history[t[l]][n[l]] = s[l][0];\n            }\n\n            return [2];\n        }\n      });\n    });\n  }, t;\n}(BaseCallback),\n    CustomCallback = function (e) {\n  function t(t) {\n    var n = e.call(this) || this;\n    return n.trainBegin = t.onTrainBegin, n.trainEnd = t.onTrainEnd, n.epochBegin = t.onEpochBegin, n.epochEnd = t.onEpochEnd, n.batchBegin = t.onBatchBegin, n.batchEnd = t.onBatchEnd, n;\n  }\n\n  return __extends(t, e), t.prototype.onEpochBegin = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        switch (n.label) {\n          case 0:\n            return null == this.epochBegin ? [3, 3] : [4, resolveScalarsInLogs(t)];\n\n          case 1:\n            return n.sent(), [4, this.epochBegin(e, t)];\n\n          case 2:\n            n.sent(), n.label = 3;\n\n          case 3:\n            return [2];\n        }\n      });\n    });\n  }, t.prototype.onEpochEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        switch (n.label) {\n          case 0:\n            return null == this.epochEnd ? [3, 3] : [4, resolveScalarsInLogs(t)];\n\n          case 1:\n            return n.sent(), [4, this.epochEnd(e, t)];\n\n          case 2:\n            n.sent(), n.label = 3;\n\n          case 3:\n            return [2];\n        }\n      });\n    });\n  }, t.prototype.onBatchBegin = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        switch (n.label) {\n          case 0:\n            return null == this.batchBegin ? [3, 3] : [4, resolveScalarsInLogs(t)];\n\n          case 1:\n            return n.sent(), [4, this.batchBegin(e, t)];\n\n          case 2:\n            n.sent(), n.label = 3;\n\n          case 3:\n            return [2];\n        }\n      });\n    });\n  }, t.prototype.onBatchEnd = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        switch (n.label) {\n          case 0:\n            return null == this.batchEnd ? [3, 3] : [4, resolveScalarsInLogs(t)];\n\n          case 1:\n            return n.sent(), [4, this.batchEnd(e, t)];\n\n          case 2:\n            n.sent(), n.label = 3;\n\n          case 3:\n            return [2];\n        }\n      });\n    });\n  }, t.prototype.onTrainBegin = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (t) {\n        switch (t.label) {\n          case 0:\n            return null == this.trainBegin ? [3, 3] : [4, resolveScalarsInLogs(e)];\n\n          case 1:\n            return t.sent(), [4, this.trainBegin(e)];\n\n          case 2:\n            t.sent(), t.label = 3;\n\n          case 3:\n            return [2];\n        }\n      });\n    });\n  }, t.prototype.onTrainEnd = function (e) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (t) {\n        switch (t.label) {\n          case 0:\n            return null == this.trainEnd ? [3, 3] : [4, resolveScalarsInLogs(e)];\n\n          case 1:\n            return t.sent(), [4, this.trainEnd(e)];\n\n          case 2:\n            t.sent(), t.label = 3;\n\n          case 3:\n            return [2];\n        }\n      });\n    });\n  }, t;\n}(BaseCallback);\n\nfunction standardizeCallbacks(e) {\n  return null == e ? null : e instanceof BaseCallback ? [e] : Array.isArray(e) && e[0] instanceof BaseCallback ? e : toList(e).map(function (e) {\n    return new CustomCallback(e);\n  });\n}\n\nvar CallbackConstructorRegistry = function () {\n  function e() {}\n\n  return e.registerCallbackConstructor = function (t, n) {\n    util.assert(t >= 0 && Number.isInteger(t), \"Verbosity level is expected to be an integer >= 0, but got \" + t), e.checkForDuplicate(n), null == e.constructors[t] && (e.constructors[t] = []), e.constructors[t].push(n);\n  }, e.checkForDuplicate = function (t) {\n    for (var n in e.constructors) {\n      e.constructors[+n].forEach(function (e) {\n        if (e === t) throw new ValueError(\"Duplicate callback constructor.\");\n      });\n    }\n  }, e.clear = function () {\n    e.constructors = {};\n  }, e.createCallbacks = function (t) {\n    var n = [];\n\n    for (var r in e.constructors) {\n      var i = +r;\n      t >= i && n.push.apply(n, e.constructors[i]);\n    }\n\n    return n.map(function (e) {\n      return new e();\n    });\n  }, e.constructors = {}, e;\n}();\n\nfunction configureCallbacks(e, t, n, r, i, a, o, s, l, u) {\n  var c = new History(),\n      p = [new BaseLogger(t)].concat(CallbackConstructorRegistry.createCallbacks(n));\n  null != e && p.push.apply(p, e), p.push(c);\n  var h = new CallbackList(p);\n  return h.setParams({\n    epochs: r,\n    initialEpoch: i,\n    samples: a,\n    steps: o,\n    batchSize: s,\n    verbose: n,\n    doValidation: l,\n    metrics: u\n  }), {\n    callbackList: h,\n    history: c\n  };\n}\n\nfunction l2Normalize(e, t) {\n  return tidy(function () {\n    var n = sum(square(e), t, !0),\n        r = mul(scalar(epsilon()), onesLike(e)),\n        i = sqrt(maximum(n, r));\n    return div(e, i);\n  });\n}\n\nfunction meanSquaredError(e, t) {\n  return tidy(function () {\n    return mean(square(sub(t, e)), -1);\n  });\n}\n\nfunction meanAbsoluteError(e, t) {\n  return tidy(function () {\n    return mean(abs(sub(t, e)), -1);\n  });\n}\n\nfunction meanAbsolutePercentageError(e, t) {\n  return tidy(function () {\n    var n = sub(e, t),\n        r = clipByValue(abs(e), epsilon(), Number.MAX_VALUE),\n        i = abs(div(n, r));\n    return mul(getScalar(100), mean(i, -1));\n  });\n}\n\nfunction meanSquaredLogarithmicError(e, t) {\n  return tidy(function () {\n    var n = getScalar(1),\n        r = clipByValue(t, epsilon(), Number.MAX_VALUE),\n        i = log(add(n, r)),\n        a = clipByValue(e, epsilon(), Number.MAX_VALUE),\n        o = log(add(n, a));\n    return mean(square(sub(i, o)), -1);\n  });\n}\n\nfunction squaredHinge(e, t) {\n  return tidy(function () {\n    var n = getScalar(0),\n        r = getScalar(1),\n        i = maximum(n, sub(r, mul(e, t)));\n    return mean(square(i), -1);\n  });\n}\n\nfunction hinge(e, t) {\n  return tidy(function () {\n    var n = getScalar(0),\n        r = getScalar(1),\n        i = maximum(n, sub(r, mul(e, t)));\n    return mean(i, -1);\n  });\n}\n\nfunction categoricalHinge(e, t) {\n  return tidy(function () {\n    var n = getScalar(0),\n        r = getScalar(1),\n        i = sum(mul(e, t), -1),\n        a = max(mul(sub(r, e), t), -1);\n    return maximum(n, add(r, sub(a, i)));\n  });\n}\n\nfunction logcosh(e, t) {\n  return tidy(function () {\n    var n = getScalar(Math.log(2)),\n        r = sub(t, e),\n        i = sub(add(r, softplus(mul(getScalar(-2), r))), n);\n    return mean(i, -1);\n  });\n}\n\nfunction categoricalCrossentropy(e, t, n) {\n  return void 0 === n && (n = !1), tidy(function () {\n    if (n) t = softmax(t);else {\n      var r = sum(t, t.shape.length - 1, !0);\n      t = div(t, r);\n    }\n    return t = clipByValue(t, epsilon(), 1 - epsilon()), neg(sum(mul(e.toFloat(), log(t)), t.shape.length - 1));\n  });\n}\n\nfunction sparseCategoricalCrossentropy(e, t) {\n  return tidy(function () {\n    var n = floor(flatten(e)).toInt(),\n        r = (t = clipByValue(t, epsilon(), 1 - epsilon())).shape;\n    return categoricalCrossentropy(oneHot(n, r[r.length - 1]).reshape(r), t, !1);\n  });\n}\n\nfunction sigmoidCrossEntropyWithLogits(e, t) {\n  if (!util.arraysEqual(e.shape, t.shape)) throw new ValueError(\"logits and labels must have the same shape, but got shapes \" + JSON.stringify(e.shape) + \" and \" + JSON.stringify(t.shape));\n  return tidy(function () {\n    var n = t.relu(),\n        r = t.abs().neg();\n    return n.sub(t.mul(e)).add(r.exp().log1p());\n  });\n}\n\nfunction binaryCrossentropy(e, t) {\n  return tidy(function () {\n    var n;\n    return n = clipByValue(t, epsilon(), 1 - epsilon()), n = log(div(n, sub(getScalar(1), n))), mean(sigmoidCrossEntropyWithLogits(e, n), -1);\n  });\n}\n\nfunction kullbackLeiblerDivergence(e, t) {\n  return tidy(function () {\n    var n = clipByValue(e, epsilon(), 1),\n        r = clipByValue(t, epsilon(), 1);\n    return sum(mul(e, log(div(n, r))), -1);\n  });\n}\n\nfunction poisson(e, t) {\n  return tidy(function () {\n    var n = log(add(getScalar(epsilon()), t));\n    return mean(sub(t, mul(e, n)), -1);\n  });\n}\n\nfunction cosineProximity(e, t) {\n  return tidy(function () {\n    var n = l2Normalize(e, -1),\n        r = l2Normalize(t, -1),\n        i = mul(n, r);\n    return neg(sum(i, -1));\n  });\n}\n\nfunction get(e) {\n  var t = {\n    meanSquaredError: meanSquaredError,\n    meanAbsoluteError: meanAbsoluteError,\n    meanAbsolutePercentageError: meanAbsolutePercentageError,\n    meanSquaredLogarithmicError: meanSquaredLogarithmicError,\n    squaredHinge: squaredHinge,\n    hinge: hinge,\n    categoricalHinge: categoricalHinge,\n    logcosh: logcosh,\n    categoricalCrossentropy: categoricalCrossentropy,\n    sparseCategoricalCrossentropy: sparseCategoricalCrossentropy,\n    binaryCrossentropy: binaryCrossentropy,\n    kullbackLeiblerDivergence: kullbackLeiblerDivergence,\n    poisson: poisson,\n    cosineProximity: cosineProximity\n  };\n\n  if (\"string\" == typeof e) {\n    if (e in t) return t[e];\n    var n = \"Unknown loss \" + e;\n    throw e.toLowerCase().includes(\"softmaxcrossentropy\") && (n = \"Unknown loss \" + e + '. Use \"categoricalCrossentropy\" as the string name for tf.losses.softmaxCrossEntropy'), new ValueError(n);\n  }\n\n  return e;\n}\n\nfunction binaryAccuracy(e, t) {\n  return tidy(function () {\n    var n = mul(getScalar(.5), onesLike(t)),\n        r = cast$1(greater(t, n), e.dtype);\n    return mean(equal(e, r), -1);\n  });\n}\n\nfunction categoricalAccuracy(e, t) {\n  return tidy(function () {\n    return cast$1(equal(argMax(e, -1), argMax(t, -1)), \"float32\");\n  });\n}\n\nfunction truePositives(e, t) {\n  return tidy(function () {\n    var n = getScalar(1);\n    return logicalAnd(e.equal(n), t.equal(n)).sum().cast(\"float32\");\n  });\n}\n\nfunction falseNegatives(e, t) {\n  return tidy(function () {\n    var n = getScalar(1),\n        r = getScalar(0);\n    return logicalAnd(e.equal(n), t.equal(r)).sum().cast(\"float32\");\n  });\n}\n\nfunction falsePositives(e, t) {\n  return tidy(function () {\n    var n = getScalar(1),\n        r = getScalar(0);\n    return logicalAnd(e.equal(r), t.equal(n)).sum().cast(\"float32\");\n  });\n}\n\nfunction precision(e, t) {\n  return tidy(function () {\n    var n = getScalar(0),\n        r = truePositives(e, t),\n        i = falsePositives(e, t),\n        a = r.add(i);\n    return where(greater(a, n), r.div(a), n).cast(\"float32\");\n  });\n}\n\nfunction recall(e, t) {\n  return tidy(function () {\n    var n = getScalar(0),\n        r = truePositives(e, t),\n        i = falseNegatives(e, t),\n        a = r.add(i);\n    return where(greater(a, n), r.div(a), n).cast(\"float32\");\n  });\n}\n\nfunction binaryCrossentropy$1(e, t) {\n  return binaryCrossentropy(e, t);\n}\n\nfunction sparseCategoricalAccuracy(e, t) {\n  return e.rank === t.rank && (e = e.squeeze([e.rank - 1])), (t = t.argMax(-1)).dtype !== e.dtype && (t = t.asType(e.dtype)), equal(e, t).asType(\"float32\");\n}\n\nvar mse$1 = meanSquaredError,\n    MSE$1 = meanSquaredError,\n    mae$1 = meanAbsoluteError,\n    MAE$1 = meanAbsoluteError,\n    mape$1 = meanAbsolutePercentageError,\n    MAPE$1 = meanAbsolutePercentageError,\n    categoricalCrossentropy$1 = categoricalCrossentropy,\n    cosine$1 = cosineProximity,\n    sparseCategoricalCrossentropy$1 = sparseCategoricalCrossentropy;\n\nfunction get$1(e) {\n  var t = {\n    binaryAccuracy: binaryAccuracy,\n    categoricalAccuracy: categoricalAccuracy,\n    precision: precision,\n    categoricalCrossentropy: categoricalCrossentropy$1,\n    sparseCategoricalCrossentropy: sparseCategoricalCrossentropy$1,\n    mse: mse$1,\n    MSE: MSE$1,\n    mae: mae$1,\n    MAE: MAE$1,\n    mape: mape$1,\n    MAPE: MAPE$1,\n    cosine: cosine$1\n  };\n  if (\"string\" == typeof e && e in t) return t[e];\n  if (\"string\" != typeof e && null != e) return e;\n  throw new ValueError(\"Unknown metric \" + e);\n}\n\nfunction getOptimizer(e) {\n  var t = {\n    Adagrad: function Adagrad() {\n      return train.adagrad(.01);\n    },\n    Adadelta: function Adadelta() {\n      return train.adadelta(1, .95, epsilon());\n    },\n    Adam: function Adam() {\n      return train.adam(.001, .9, .999, epsilon());\n    },\n    Adamax: function Adamax() {\n      return train.adamax(.002, .9, .999, epsilon(), 0);\n    },\n    RMSProp: function RMSProp() {\n      return train.rmsprop(.001, .9, 0, epsilon());\n    },\n    SGD: function SGD() {\n      return train.sgd(.01);\n    }\n  };\n  if (t.adagrad = t.Adagrad, t.adadelta = t.Adadelta, t.adam = t.Adam, t.adamax = t.Adamax, t.rmsprop = t.RMSProp, t.sgd = t.SGD, e in t) return t[e]();\n  throw new ValueError(\"Unknown Optimizer \" + e);\n}\n\nfunction printSummary(e, t, n, r) {\n  void 0 === r && (r = console.log);\n  var i,\n      a = isModelSequentialLike(e),\n      o = [\"Layer (type)\", \"Output shape\", \"Param #\"];\n  if (a ? (t = t || 65, n = n || [.45, .85, 1]) : (t = t || 98, n = n || [.33, .55, .67, 1]), n[n.length - 1] <= 1 && (n = n.map(function (e) {\n    return Math.floor(t * e);\n  })), !a) for (var s in o.push(\"Receives inputs\"), i = [], e.nodesByDepth) {\n    i.push.apply(i, e.nodesByDepth[s]);\n  }\n  r(\"_\".repeat(t)), printRow(o, n, r), r(\"=\".repeat(t));\n\n  for (var l = e.layers, u = 0; u < l.length; ++u) {\n    a ? printLayerSummary(l[u], n, r) : printLayerSummaryWithConnections(l[u], n, i, r), r((u === l.length - 1 ? \"=\" : \"_\").repeat(t));\n  }\n\n  e.checkTrainableWeightsConsistency();\n  var c = countTrainableParams(e),\n      p = countParamsInWeights(e.nonTrainableWeights);\n  r(\"Total params: \" + (c + p)), r(\"Trainable params: \" + c), r(\"Non-trainable params: \" + p), r(\"_\".repeat(t));\n}\n\nfunction countTrainableParams(e) {\n  return null != e.collectedTrainableWeights ? countParamsInWeights(e.collectedTrainableWeights) : countParamsInWeights(e.trainableWeights);\n}\n\nfunction isModelSequentialLike(e) {\n  var t = !0,\n      n = [],\n      r = [];\n\n  for (var i in e.nodesByDepth) {\n    n.push(e.nodesByDepth[i]);\n  }\n\n  for (var a = 0, o = n; a < o.length; a++) {\n    var s = o[a];\n\n    if (s.length > 1 || 1 === s.length && s[0].inboundLayers.length > 1) {\n      t = !1;\n      break;\n    }\n\n    r.push.apply(r, s);\n  }\n\n  if (t) for (var l = 0, u = e.layers; l < u.length; l++) {\n    for (var c = !1, p = 0, h = u[l].inboundNodes; p < h.length; p++) {\n      var d = h[p];\n\n      if (-1 !== r.indexOf(d)) {\n        if (c) {\n          t = !1;\n          break;\n        }\n\n        c = !0;\n      }\n    }\n\n    if (!t) break;\n  }\n  return t;\n}\n\nfunction printRow(e, t, n) {\n  void 0 === n && (n = console.log);\n\n  for (var r = \"\", i = 0; i < e.length; ++i) {\n    i > 0 && (r = r.slice(0, r.length - 1) + \" \"), r = (r += e[i]).slice(0, t[i]), r += \" \".repeat(t[i] - r.length);\n  }\n\n  n(r);\n}\n\nfunction printLayerSummary(e, t, n) {\n  var r;\n\n  try {\n    r = JSON.stringify(e.outputShape);\n  } catch (e) {\n    r = \"multiple\";\n  }\n\n  printRow([e.name + \" (\" + e.getClassName() + \")\", r, e.countParams().toString()], t, n);\n}\n\nfunction printLayerSummaryWithConnections(e, t, n, r) {\n  var i;\n\n  try {\n    i = JSON.stringify(e.outputShape);\n  } catch (e) {\n    i = \"multiple\";\n  }\n\n  for (var a = [], o = 0, s = e.inboundNodes; o < s.length; o++) {\n    var l = s[o];\n    if (!(null != n && n.length > 0 && -1 === n.indexOf(l))) for (var u = 0; u < l.inboundLayers.length; ++u) {\n      var c = l.inboundLayers[u].name,\n          p = l.nodeIndices[u],\n          h = l.tensorIndices[u];\n      a.push(c + \"[\" + p + \"][\" + h + \"]\");\n    }\n  }\n\n  var d = e.name,\n      f = e.getClassName(),\n      g = 0 === a.length ? \"\" : a[0];\n  printRow([d + \" (\" + f + \")\", i, e.countParams().toString(), g], t, r);\n\n  for (u = 1; u < a.length; ++u) {\n    printRow([\"\", \"\", \"\", a[u]], t, r);\n  }\n}\n\nfunction deserialize(e, t, n) {\n  return void 0 === t && (t = {}), void 0 === n && (n = !1), deserializeKerasObject(e, serialization.SerializationMap.getMap().classNameMap, t, \"layer\", n);\n}\n\nfunction isArrayItemInputOrOutputName(e, t, n) {\n  return (\"inboundNodes\" === e || \"outputLayers\" === e || \"inputLayers\" === e) && 0 === t && \"string\" == typeof n;\n}\n\nfunction convertPythonicToTs(e, t) {\n  if (null === e) return null;\n  if (\"string\" == typeof e) return toCamelCase(e);\n  if (\"number\" == typeof e || \"boolean\" == typeof e) return e;\n\n  if (e instanceof Array) {\n    for (var n = [], r = e.length, i = 0; i < r; ++i) {\n      var a = e[i];\n      isArrayItemInputOrOutputName(t, i, a) ? n.push(a) : n.push(convertPythonicToTs(a, t));\n    }\n\n    return n;\n  }\n\n  for (var o = {}, s = 0, l = Object.keys(e); s < l.length; s++) {\n    var u = l[s],\n        c = e[u];\n    if (\"name\" === u && \"string\" == typeof c) o[u] = c;else {\n      var p = toCamelCase(u);\n      o[p] = convertPythonicToTs(c, p);\n    }\n  }\n\n  return o;\n}\n\nfunction convertTsToPythonic(e, t) {\n  if (null === e || void 0 === e) return null;\n  if (\"string\" == typeof e) return toSnakeCase(e);\n  if (\"number\" == typeof e || \"boolean\" == typeof e) return e;\n\n  if (e instanceof Array) {\n    for (var n = [], r = e.length, i = 0; i < r; ++i) {\n      var a = e[i];\n      isArrayItemInputOrOutputName(t, i, a) ? n.push(a) : n.push(convertTsToPythonic(a, t));\n    }\n\n    return n;\n  }\n\n  for (var o = {}, s = 0, l = Object.keys(e); s < l.length; s++) {\n    var u = l[s],\n        c = e[u],\n        p = toSnakeCase(u);\n    o[p] = \"name\" !== u && \"className\" !== u || \"string\" != typeof c ? convertTsToPythonic(c, u) : c;\n  }\n\n  return o;\n}\n\nvar version = \"0.10.3\";\n\nfunction assertFeedCompatibility(e, t) {\n  if (null == e.dtype || e.dtype === t.dtype) return t;\n\n  try {\n    return cast(t, e.dtype);\n  } catch (n) {\n    throw new ValueError(\"The dtype of the feed (\" + t.dtype + \") can not be cast to the dtype of the key '\" + e.name + \"' (\" + e.dtype + \").\");\n  }\n}\n\nvar FeedDict = function () {\n  function e(t) {\n    if (this.id2Value = {}, this.id2Mask = {}, this.name2Id = {}, t instanceof e) for (var n in t.id2Value) {\n      this.id2Value[n] = t.id2Value[n], n in t.id2Mask && (this.id2Mask[n] = t.id2Mask[n]);\n    } else {\n      if (null == t) return;\n\n      for (var r = 0, i = t; r < i.length; r++) {\n        var a = i[r];\n        this.add(a.key, a.value);\n      }\n    }\n  }\n\n  return e.prototype.add = function (e, t, n) {\n    if (null != this.id2Value[e.id]) throw new ValueError(\"Duplicate key: name=\" + e.name + \", id=\" + e.id);\n    return this.id2Value[e.id] = assertFeedCompatibility(e, t), this.name2Id[e.name] = e.id, null != n && (this.id2Mask[e.id] = n), this;\n  }, e.prototype.addFeed = function (e) {\n    this.add(e.key, e.value);\n  }, e.prototype.hasKey = function (e) {\n    return null != this.id2Value[e.id];\n  }, e.prototype.names = function () {\n    return Object.keys(this.name2Id);\n  }, e.prototype.getValue = function (e) {\n    if (e instanceof SymbolicTensor) {\n      if (null == this.id2Value[e.id]) throw new ValueError(\"Nonexistent key: \" + e.name);\n      return this.id2Value[e.id];\n    }\n\n    var t = this.name2Id[e];\n    if (null == t) throw new ValueError(\"Feed dict has no SymbolicTensor name: \" + e);\n    return this.id2Value[t];\n  }, e.prototype.getMask = function (e) {\n    if (e instanceof SymbolicTensor) {\n      if (null == this.id2Value[e.id]) throw new ValueError(\"Nonexistent key: \" + e.name);\n      return this.id2Mask[e.id];\n    }\n\n    var t = this.name2Id[e];\n    if (null == t) throw new ValueError(\"Feed dict has no SymbolicTensor name: \" + e);\n    return this.id2Mask[t];\n  }, e.prototype.disposeMasks = function () {\n    null != this.id2Mask && dispose(this.id2Mask);\n  }, e;\n}(),\n    cachedSorted = {},\n    cachedRecipientCounts = {};\n\nfunction execute(e, t, n, r) {\n  for (var i = null != n && n.training, a = Array.isArray(e), o = a ? e : [e], s = o.map(function (e) {\n    return e.name;\n  }), l = [], u = t.names(), c = 0, p = s; c < p.length; c++) {\n    var h = p[c];\n    -1 !== u.indexOf(h) ? l.push(t.getValue(h)) : l.push(null);\n  }\n\n  null != r && (r.maxNumTensors = -1 / 0, r.minNumTensors = 1 / 0);\n  var d,\n      f,\n      g = s.join(\",\") + \"|\" + t.names().join(\",\");\n\n  if (null == cachedSorted[g]) {\n    var m = getTopologicalSortAndRecipientCounts(o, t);\n    d = m.sorted, f = m.recipientCounts, cachedSorted[g] = d, cachedRecipientCounts[g] = f;\n  }\n\n  d = cachedSorted[g], f = {}, i || Object.assign(f, cachedRecipientCounts[g]);\n\n  for (var y = new FeedDict(t), v = 0; v < d.length; ++v) {\n    if (null != r) {\n      var b = memory().numTensors;\n      b > r.maxNumTensors && (r.maxNumTensors = b), b < r.minNumTensors && (r.minNumTensors = b);\n    }\n\n    var w = d[v],\n        z = w.sourceLayer;\n\n    if (!(z instanceof InputLayer)) {\n      for (var S = [], I = [], A = [], C = !1, N = 0, _ = w.inputs; N < _.length; N++) {\n        var E = _[N],\n            k = y.getValue(E),\n            L = y.getMask(E);\n        S.push(k), I.push(L), null != L && (C = !0), i || (f[E.name]--, 0 !== f[E.name] || t.hasKey(E) || -1 !== s.indexOf(E.name) || k.isDisposed || A.push(k));\n      }\n\n      C && ((n = n || {}).mask = I[0]);\n      var x = toList(z.apply(S, n)),\n          T = null;\n      z.supportsMasking && (T = z.computeMask(S, I));\n\n      for (var D = getNodeOutputs(w), O = Array.isArray(D) ? D : [D], R = 0; R < O.length; ++R) {\n        y.hasKey(O[R]) || y.add(O[R], x[R], Array.isArray(T) ? T[0] : T);\n        var M = s.indexOf(O[R].name);\n        -1 !== M && (l[M] = x[R]);\n      }\n\n      i || dispose(A);\n    }\n  }\n\n  return y.disposeMasks(), a ? l : l[0];\n}\n\nfunction getTopologicalSortAndRecipientCounts(e, t) {\n  util.assert(null != e && e.length > 0, \"Exepcted at least one fetch, got none\");\n  var n = [],\n      r = {};\n\n  if (1 === e.length) {\n    var i = getTopologicalSortAndRecipientCountsForOneFetch(e[0], t);\n    n = i.sorted, r = i.recipientMap;\n  } else for (var a = new Set(), o = 0, s = e; o < s.length; o++) {\n    for (var l = getTopologicalSortAndRecipientCountsForOneFetch(s[o], t), u = l.sorted, c = l.recipientMap, p = 0, h = u; p < h.length; p++) {\n      var d = h[p];\n      a.has(d.name) || (n.push(d), a.add(d.name));\n    }\n\n    var f = function f(e) {\n      null == r[e] && (r[e] = new Set()), c[e].forEach(function (t) {\n        return r[e].add(t);\n      });\n    };\n\n    for (var g in c) {\n      f(g);\n    }\n  }\n\n  return {\n    sorted: n,\n    recipientCounts: recipientMap2Counts(r)\n  };\n}\n\nfunction recipientMap2Counts(e) {\n  var t = {};\n\n  for (var n in e) {\n    t[n] = e[n].size;\n  }\n\n  return t;\n}\n\nfunction getTopologicalSortAndRecipientCountsForOneFetch(e, t) {\n  for (var n = new Set(), r = [], i = {}, a = 0, o = t.names(); a < o.length; a++) {\n    var s = o[a];\n    n.add(s);\n  }\n\n  var l = [],\n      u = [];\n\n  for (l.push(e); l.length > 0;) {\n    var c = l[l.length - 1];\n    if (n.has(c.name)) l.pop();else {\n      var p = u[u.length - 1] === l.length - 1;\n      if (0 === c.inputs.length || p) l.pop(), r.push(c), n.add(c.name), p && u.pop();else {\n        u.push(l.length - 1);\n\n        for (var h = 0, d = c.inputs; h < d.length; h++) {\n          var f = d[h];\n          null == i[f.name] && (i[f.name] = new Set()), i[f.name].add(c.name), n.has(f.name) || l.push(f);\n        }\n      }\n    }\n  }\n\n  return {\n    sorted: r,\n    recipientMap: i\n  };\n}\n\nfunction getNodeOutputs(e) {\n  var t;\n  if (1 === e.sourceLayer.inboundNodes.length) t = e.sourceLayer.output;else {\n    for (var n = null, r = 0; r < e.sourceLayer.inboundNodes.length; ++r) {\n      for (var i = 0, a = e.sourceLayer.inboundNodes[r].outputTensors; i < a.length; i++) {\n        if (a[i].id === e.id) {\n          n = r;\n          break;\n        }\n      }\n    }\n\n    t = e.sourceLayer.getOutputAt(n);\n  }\n  return t;\n}\n\nfunction preprocessWeightsForLoading(e, t, n, r) {\n  if (!n.startsWith(\"2.\")) throw new ValueError(\"Unsupported Keras version in weights being loaded: \" + n);\n  return t;\n}\n\nfunction loadTensor(e, t, n) {\n  var r = stringToDType(e);\n  return Tensor.make(t, {\n    values: 0 === t.length ? n : util.flatten(n)\n  }, r);\n}\n\nfunction loadWeightsFromJson(e, t, n) {\n  void 0 === n && (n = !1);\n\n  for (var r = e.keras_version, i = e.backend, a = t.map(function (e) {\n    return e.name;\n  }), o = {}, s = 0, l = t; s < l.length; s++) {\n    null != (b = l[s]).name && (null == o[b.name] && (o[b.name] = []), o[b.name].push(b));\n  }\n\n  for (var u = e.weights, c = [], p = 0; p < a.length; ++p) {\n    var h = a[p],\n        d = u[h];\n    null == d && (d = []);\n\n    for (var f = [], g = 0; g < d.length; ++g) {\n      var m = d[g];\n      f.push(new LayerVariable(loadTensor(m.dtype, m.shape, m.value)));\n    }\n\n    for (var y = 0, v = o[h]; y < v.length; y++) {\n      var b,\n          w = (b = v[y]).weights;\n\n      if ((f = preprocessWeightsForLoading(b, f, r, i)).length !== w.length) {\n        if (!n) throw new ValueError(\"Layer #\" + p + ' (named \"' + b.name + '\") expects ' + w.length + \" weight(s), but the saved weights have \" + f.length + \" element(s).\");\n        console.warn(\"Skipping loading of weights of layer \" + b.name + \" due to mismatch in number of weights: (\" + f.length + \" vs \" + w.length + \").\");\n      }\n\n      for (var z = 0; z < f.length; ++z) {\n        !n || util.arraysEqual(w[z].shape, f[z].shape) ? c.push([w[z], f[z].read()]) : console.warn(\"Skipping loading of weights for layer \" + b.name + \" due to mismatch in shape (\" + w[z].shape + \" vs \" + f[z].shape + \")\");\n      }\n    }\n  }\n\n  batchSetValue(c);\n}\n\nfunction loadWeightsFromNamedTensorMap(e, t, n) {\n  void 0 === n && (n = !0);\n\n  for (var r = {}, i = 0, a = 0, o = t; a < o.length; a++) {\n    for (var s = 0, l = o[a].weights; s < l.length; s++) {\n      var u = l[s];\n      if (null != r[u.originalName]) throw new ValueError(\"Duplicate weight name: \" + u.originalName);\n      r[u.originalName] = u, i++;\n    }\n  }\n\n  var c = [];\n\n  for (var p in e) {\n    if (null != r[p]) c.push([r[p], e[p]]);else if (n) throw new ValueError(\"Provided weight data has no target variable: \" + p);\n    delete r[p];\n  }\n\n  if (n) {\n    var h = [];\n\n    for (var d in r) {\n      h.push(d);\n    }\n\n    if (h.length > 0) throw new ValueError(h.length + \" of \" + i + \" weights are not set: \" + h);\n  }\n\n  batchSetValue(c);\n}\n\nvar Container = function (e) {\n  function t(n) {\n    var r = e.call(this, {}) || this;\n\n    if (r.containerNodes = new Set(), r.name = n.name, null == r.name) {\n      var i = r.getClassName().toLowerCase();\n      r.name = getUid(i);\n    }\n\n    if (r.supportsMasking = !1, r.trainable = !0, r.updatable = !0, Array.isArray(n.inputs) ? r.inputs = n.inputs.slice() : r.inputs = [n.inputs], Array.isArray(n.outputs) ? r.outputs = n.outputs.slice() : r.outputs = [n.outputs], unique(r.inputs).length !== r.inputs.length) throw new ValueError(\"The list of inputs passed to the model is redundant. All inputs should only appear once. Found: \" + r.inputs.map(function (e) {\n      return e.name;\n    }));\n    unique(r.outputs).length !== r.outputs.length && console.warn(\"The list of outputs passed to the model is redundant. All outputs should only appear once. Found: \" + r.outputs.map(function (e) {\n      return e.name;\n    })), r.inputLayers = [], r.inputLayersNodeIndices = [], r.inputLayersTensorIndices = [], r.outputLayers = [], r.outputLayersNodeIndices = [], r.outputLayersTensorIndices = [], r.layers = [];\n\n    for (var a = 0, o = r.outputs; a < o.length; a++) {\n      var s = (N = o[a]).sourceLayer,\n          l = N.nodeIndex,\n          u = N.tensorIndex;\n      r.outputLayers.push(s), r.outputLayersNodeIndices.push(l), r.outputLayersTensorIndices.push(u);\n    }\n\n    for (var c = 0, p = r.inputs; c < p.length; c++) {\n      s = (N = p[c]).sourceLayer, l = N.nodeIndex, u = N.tensorIndex;\n      assert(0 === l, \"input layer has >1 nodes\"), assert(0 === u, \"input layer has >1 tensors\"), r.inputLayers.push(s), r.inputLayersNodeIndices.push(l), r.inputLayersTensorIndices.push(u);\n    }\n\n    r.inputNames = [], r.outputNames = [], r.feedInputShapes = [], r.feedInputNames = [], r.feedOutputNames = [];\n\n    for (var h = 0; h < r.inputLayers.length; h++) {\n      if (!((s = r.inputLayers[h]) instanceof InputLayer)) throw new TypeError(\"Input layers to a Model must be InputLayer objects. Received inputs: \" + n.inputs + \". Input \" + h + \" (0-based) originates from layer type \" + s.getClassName() + \".\");\n      r.inputNames.push(s.name), r.feedInputShapes.push(s.batchInputShape), r.feedInputNames.push(s.name);\n    }\n\n    for (var d = 0, f = r.outputLayers; d < f.length; d++) {\n      s = f[d];\n      r.outputNames.push(s.name);\n    }\n\n    r.internalInputShapes = r.inputs.map(function (e) {\n      return e.shape;\n    }), r.internalOutputShapes = r.outputs.map(function (e) {\n      return e.shape;\n    });\n\n    for (var g = {}, m = {}, y = {}, v = {}, b = {}, w = [], z = function z(e, n, i, a, o, s) {\n      null != a && null != o && null != s || (a = e.sourceLayer, o = e.nodeIndex, s = e.tensorIndex);\n      var l = a.inboundNodes[o];\n      if (-1 !== i.indexOf(l)) throw new RuntimeError(\"The tensor \" + e.name + ' at layer \"' + a.name + '\" is part of a cycle.');\n\n      if (-1 === n.indexOf(l)) {\n        r.containerNodes.add(t.nodeKey(a, o)), (a.id in b) || (b[a.id] = Object.keys(b).length), -1 === i.indexOf(l) && i.push(l);\n\n        for (var u = l.inboundLayers.length, c = 0; c < u; c++) {\n          var p = l.inputTensors[c],\n              h = l.inboundLayers[c],\n              d = l.nodeIndices[c],\n              f = l.tensorIndices[c];\n          z(p, n, i, h, d, f);\n        }\n\n        for (n.push(l); i.indexOf(l) >= 0;) {\n          i.splice(i.indexOf(l), 1);\n        }\n\n        w.push(l);\n      }\n    }, S = [], I = [], A = 0, C = r.outputs; A < C.length; A++) {\n      var N = C[A];\n      z(N, S, I);\n    }\n\n    for (var _ = 0, E = w.slice().reverse(); _ < E.length; _++) {\n      m[(K = E[_]).id] = K, K.id in g || (g[K.id] = 0);\n      var k = g[K.id],\n          L = null == y[K.outboundLayer.id] ? 0 : y[K.outboundLayer.id];\n      k = Math.max(k, L), y[K.outboundLayer.id] = k, v[K.outboundLayer.id] = K.outboundLayer, g[K.id] = k;\n\n      for (h = 0; h < K.inboundLayers.length; h++) {\n        var x = K.inboundLayers[h],\n            T = (l = K.nodeIndices[h], x.inboundNodes[l]),\n            D = null == g[T.id] ? 0 : g[T.id];\n        g[T.id] = Math.max(k + 1, D), m[T.id] = T;\n      }\n    }\n\n    var O = {};\n\n    for (var R in g) {\n      (k = g[R]) in O || (O[k] = []), O[k].push(m[R]);\n    }\n\n    var M = {};\n\n    for (var P in y) {\n      (k = y[P]) in M || (M[k] = []), M[k].push(v[P]);\n    }\n\n    var F = Object.keys(M).map(function (e) {\n      return parseInt(e, 10);\n    }).sort(reverseNumberCompare);\n    r.layers = [];\n\n    for (var V = 0, B = F; V < B.length; V++) {\n      var U = M[k = B[V]];\n      U.sort(function (e, t) {\n        var n = b[e.id],\n            r = b[t.id];\n        return n < r ? -1 : n > r ? 1 : 0;\n      });\n\n      for (var j = 0, W = U; j < W.length; j++) {\n        s = W[j];\n        r.layers.push(s);\n      }\n    }\n\n    r.layersByDepth = M, F = Object.keys(O).map(function (e) {\n      return parseInt(e, 10);\n    }).sort(reverseNumberCompare);\n\n    for (var $ = r.inputs.slice(), q = [], G = 0, H = F; G < H.length; G++) {\n      for (var J = 0, Z = O[k = H[G]]; J < Z.length; J++) {\n        var K;\n\n        if (null != (s = (K = Z[J]).outboundLayer)) {\n          for (var Y = 0, X = K.inputTensors; Y < X.length; Y++) {\n            N = X[Y];\n            if (-1 === $.indexOf(N)) throw new RuntimeError(\"Graph disconnected: cannot obtain value for tensor \" + N + ' at layer \"' + s.name + '\". The following previous layers were accessed without issue: ' + q);\n          }\n\n          for (var Q = 0, ee = K.outputTensors; Q < ee.length; Q++) {\n            N = ee[Q];\n            $.push(N);\n          }\n\n          q.push(s.name);\n        }\n      }\n    }\n\n    r.nodesByDepth = O;\n\n    for (var te = r.layers.map(function (e) {\n      return e.name;\n    }), ne = function ne(e) {\n      var t = te.filter(function (t) {\n        return t === e;\n      }).length;\n      if (1 !== t) throw new RuntimeError('The name \"' + e + '\" is used ' + t + \" times in the model. All layer names should be unique. Layer names: \" + JSON.stringify(te));\n    }, re = 0, ie = te; re < ie.length; re++) {\n      ne(ie[re]);\n    }\n\n    return r.outboundNodes = [], r.inboundNodes = [], new Node({\n      outboundLayer: r,\n      inboundLayers: [],\n      nodeIndices: [],\n      tensorIndices: [],\n      inputTensors: r.inputs,\n      outputTensors: r.outputs,\n      inputMasks: r.inputs.map(function (e) {\n        return null;\n      }),\n      outputMasks: r.outputs.map(function (e) {\n        return null;\n      }),\n      inputShapes: r.inputs.map(function (e) {\n        return e.shape;\n      }),\n      outputShapes: r.outputs.map(function (e) {\n        return e.shape;\n      })\n    }), r.built = !0, r._refCount = 1, r;\n  }\n\n  return __extends(t, e), t.prototype.assertNotDisposed = function () {\n    if (0 === this._refCount) throw new Error(\"Container '\" + this.name + \"' is already disposed.\");\n  }, t.prototype.dispose = function () {\n    this.assertNotDisposed();\n    var e = {\n      refCountAfterDispose: null,\n      numDisposedVariables: 0\n    };\n    if (0 == --this._refCount) for (var t = 0, n = this.layers; t < n.length; t++) {\n      var r = n[t];\n      e.numDisposedVariables += r.dispose().numDisposedVariables;\n    }\n    return e.refCountAfterDispose = this._refCount, e;\n  }, Object.defineProperty(t.prototype, \"trainableWeights\", {\n    get: function get() {\n      if (this._trainableWeights.length > 0) throw new ValueError(\"Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.\");\n      if (!this.trainable) return [];\n\n      for (var e = [], t = 0, n = this.layers; t < n.length; t++) {\n        var r = n[t];\n        e = e.concat(r.trainableWeights);\n      }\n\n      return e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"nonTrainableWeights\", {\n    get: function get() {\n      for (var e = [], t = 0, n = this.layers; t < n.length; t++) {\n        var r = n[t];\n        e.push.apply(e, r.nonTrainableWeights);\n      }\n\n      if (!this.trainable) {\n        for (var i = [], a = 0, o = this.layers; a < o.length; a++) {\n          r = o[a];\n          i.push.apply(i, r.trainableWeights);\n        }\n\n        return i.concat(e);\n      }\n\n      return e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"weights\", {\n    get: function get() {\n      return this.trainableWeights.concat(this.nonTrainableWeights);\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.loadWeights = function (e, t, n, r) {\n    void 0 === t && (t = !1), void 0 === n && (n = !1), void 0 === r && (r = !0), n ? loadWeightsFromNamedTensorMap(e, this.layers, r) : loadWeightsFromJson(e, this.layers, t);\n  }, t.prototype.updatedConfig = function () {\n    var e = this.getConfig();\n    return {\n      className: this.getClassName(),\n      config: e,\n      kerasVersion: \"tfjs-layers \" + version,\n      backend: \"TensorFlow.js\"\n    };\n  }, t.prototype.toJSON = function (e, t) {\n    void 0 === t && (t = !0);\n    var n = convertTsToPythonic(this.updatedConfig());\n    return t ? JSON.stringify(n) : n;\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      e = toList(e);\n\n      for (var r = new FeedDict(), i = 0; i < n.inputs.length; ++i) {\n        r.add(n.inputs[i], e[i]);\n      }\n\n      return execute(n.outputs, r, t);\n    });\n  }, t.prototype.computeMask = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      var r;\n      return e = toList(e), r = null == t ? pyListRepeat(null, e.length) : toList(t), n.runInternalGraph(e, r)[1];\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    var t = normalizeShapeList(e);\n    if (t.length !== this.inputLayers.length) throw new ValueError(\"Invalid inputShape argument \" + e + \": model has \" + this.inputLayers.length + \" tensor inputs.\");\n\n    for (var n = {}, r = 0; r < t.length; r++) {\n      var i = this.inputLayers[r],\n          a = t[r];\n      n[I = i.name + \"_0_0\"] = a;\n    }\n\n    var o = Object.keys(this.nodesByDepth).map(function (e) {\n      return parseInt(e, 10);\n    }).sort(reverseNumberCompare);\n    if (o.length > 1) for (var s = 0, l = o; s < l.length; s++) {\n      for (var u = l[s], c = 0, p = this.nodesByDepth[u]; c < p.length; c++) {\n        var h = p[c];\n        i = h.outboundLayer;\n\n        if (-1 === this.inputLayers.map(function (e) {\n          return e.id;\n        }).indexOf(i.id)) {\n          for (var d = [], f = 0; f < h.inboundLayers.length; f++) {\n            var g = h.inboundLayers[f],\n                m = h.nodeIndices[f],\n                y = h.tensorIndices[f],\n                v = n[I = g.name + \"_\" + m + \"_\" + y];\n            d.push(v);\n          }\n\n          var b = normalizeShapeList(i.computeOutputShape(singletonOrArray(d))),\n              w = i.inboundNodes.indexOf(h);\n\n          for (f = 0; f < b.length; f++) {\n            n[I = i.name + \"_\" + w + \"_\" + f] = b[f];\n          }\n        }\n      }\n    }\n    var z = [],\n        S = [];\n\n    for (r = 0; r < this.outputLayers.length; r++) {\n      i = this.outputLayers[r], w = this.outputLayersNodeIndices[r], y = this.outputLayersTensorIndices[r];\n      var I = i.name + \"_\" + w + \"_\" + y;\n      S.push(I);\n    }\n\n    for (r = 0; r < S.length; r++) {\n      var A = S[r];\n      assert(A in n), z.push(n[A]);\n    }\n\n    return singletonOrArray(z);\n  }, t.prototype.runInternalGraph = function (e, t) {\n    null == t && (t = pyListRepeat(null, e.length));\n\n    for (var n = {}, r = 0; r < this.inputs.length; ++r) {\n      var i = this.inputs[r],\n          a = e[r],\n          o = t[r];\n      n[i.id] = [a, o];\n    }\n\n    for (var s = 0, l = Object.keys(this.nodesByDepth).map(function (e) {\n      return parseInt(e, 10);\n    }).sort(reverseNumberCompare); s < l.length; s++) {\n      for (var u = l[s], c = 0, p = this.nodesByDepth[u]; c < p.length; c++) {\n        for (var h = p[c], d = h.outboundLayer, f = h.inputTensors, g = h.outputTensors, m = new Array(), y = 0, v = f; y < v.length; y++) {\n          (i = v[y]).id in n && m.push(n[i.id]);\n        }\n\n        if (m.length === f.length) {\n          var b = {},\n              w = void 0,\n              z = void 0,\n              S = void 0,\n              I = void 0;\n\n          if (null != h.callArgs && (b = h.callArgs), 1 === m.length) {\n            var A = m[0],\n                C = A[0],\n                N = A[1];\n            null == b.mask && (b.mask = N), S = toList(d.call(C, b)), I = toList(d.computeMask(C, N)), w = [C], z = [N];\n          } else w = m.map(function (e) {\n            return e[0];\n          }), z = m.map(function (e) {\n            return e[1];\n          }), null == b.mask && (b.mask = z), S = toList(d.call(w, b)), I = toList(d.computeMask(w, z));\n\n          if (d.activityRegularizer) throw new NotImplementedError(\"Model invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.\");\n\n          for (r = 0; r < g.length; ++r) {\n            i = g[r], a = S[r], o = I[r];\n            n[i.id] = [a, o];\n          }\n        }\n      }\n    }\n\n    for (var _ = [], E = [], k = [], L = 0, x = this.outputs; L < x.length; L++) {\n      assert((i = x[L]).id in n, \"Could not compute output \" + i.name + \" : \" + i.id);\n      var T = n[i.id],\n          D = T[0];\n      o = T[1];\n      k.push(D.shape), _.push(D), E.push(o);\n    }\n\n    return [_, E, k];\n  }, t.prototype.buildNodeConversionMap = function (e) {\n    for (var n, r = {}, i = 0, a = this.layers; i < a.length; i++) {\n      var o = a[i];\n      n = o instanceof t ? 1 : 0;\n\n      for (var s = 0; s < o.inboundNodes.length; s++) {\n        var l = t.nodeKey(o, s);\n        this.containerNodes.has(l) && (r[l] = n, n += 1);\n      }\n    }\n\n    return r;\n  }, t.prototype.getLayer = function (e, t) {\n    if (null != t) {\n      if (this.layers.length <= t) throw new ValueError(\"Was asked to retrieve layer at index \" + t + \", but model only has \" + this.layers.length + \" layer(s).\");\n      return this.layers[t];\n    }\n\n    if (null == e) throw new ValueError(\"Provide either a layer name or layer index\");\n\n    for (var n = 0, r = this.layers; n < r.length; n++) {\n      var i = r[n];\n      if (i.name === e) return i;\n    }\n\n    throw new ValueError(\"No such layer: \" + e);\n  }, t.prototype.calculateLosses = function () {\n    var e = this;\n    return tidy(function () {\n      for (var n = [], r = 0, i = e.layers; r < i.length; r++) {\n        for (var a = i[r], o = 0; o < a.inboundNodes.length; ++o) {\n          var s = t.nodeKey(a, o);\n          e.containerNodes.has(s) && n.push.apply(n, a.calculateLosses());\n        }\n      }\n\n      return n;\n    });\n  }, t.prototype.getConfig = function () {\n    for (var e = {\n      name: this.name\n    }, n = this.buildNodeConversionMap(this.layers), r = [], i = 0, a = this.layers; i < a.length; i++) {\n      for (var o = (b = a[i]).getClassName(), s = b.getConfig(), l = [], u = 0; u < b.inboundNodes.length; u++) {\n        var c = b.inboundNodes[u],\n            p = t.nodeKey(b, u),\n            h = {};\n\n        if (this.containerNodes.has(p)) {\n          if (c.callArgs) try {\n            JSON.stringify(c.callArgs), h = c.callArgs;\n          } catch (e) {\n            console.warn(\"Layer \" + b.name + \" was passed non-serializable keyword arguments: \" + c.callArgs + \". They will not be included in the serialized model (and thus will be missing at deserialization time).\"), h = {};\n          }\n\n          if (c.inboundLayers.length > 0) {\n            for (var d = [], f = 0; f < c.inboundLayers.length; f++) {\n              var g = c.inboundLayers[f],\n                  m = c.nodeIndices[f],\n                  y = c.tensorIndices[f];\n              null == (z = n[t.nodeKey(g, m)]) && (z = 0), d.push([g.name, z, y, h]);\n            }\n\n            l.push(d);\n          }\n        }\n      }\n\n      r.push({\n        name: b.name,\n        className: o,\n        config: s,\n        inboundNodes: l\n      });\n    }\n\n    e.layers = r;\n    var v = [];\n\n    for (f = 0; f < this.inputLayers.length; f++) {\n      var b = this.inputLayers[f];\n      m = this.inputLayersNodeIndices[f], p = t.nodeKey(b, m);\n\n      if (this.containerNodes.has(p)) {\n        null !== (z = n[p]) && void 0 !== z || (z = 0);\n        y = this.inputLayersTensorIndices[f];\n        v.push([b.name, z, y]);\n      }\n    }\n\n    e.inputLayers = v;\n    var w = [];\n\n    for (f = 0; f < this.outputLayers.length; f++) {\n      b = this.outputLayers[f], m = this.outputLayersNodeIndices[f], p = t.nodeKey(b, m);\n\n      if (this.containerNodes.has(p)) {\n        var z;\n        null !== (z = n[p]) && void 0 !== z || (z = 0);\n        y = this.outputLayersTensorIndices[f];\n        w.push([b.name, z, y]);\n      }\n    }\n\n    return e.outputLayers = w, e;\n  }, t.fromConfig = function (e, t, n, r) {\n    void 0 === n && (n = {}), void 0 === r && (r = !1);\n    var i = {},\n        a = {};\n\n    function o(e, t) {\n      e.name in a ? a[e.name].push(t) : a[e.name] = [t];\n    }\n\n    function s(e, t) {\n      for (var n, r = [], a = 0, s = t; a < s.length; a++) {\n        var l = s[a],\n            u = l[0],\n            c = l[1],\n            p = l[2];\n        if (3 === l.length) n = {};else {\n          if (4 !== l.length) throw new ValueError(\"Improperly formatted model config for layer \" + JSON.stringify(e) + \": \" + JSON.stringify(l));\n          n = l[3];\n        }\n        if (!(u in i)) return void o(e, t);\n        var h = i[u];\n        if (h.inboundNodes.length <= c) return void o(e, t);\n        var d = h.inboundNodes[c];\n        r.push(d.outputTensors[p]);\n      }\n\n      r.length > 0 && e.apply(singletonOrArray(r), n);\n    }\n\n    function l(e) {\n      var n = e.name,\n          a = deserialize(e, null != t.customObjects ? t.customObjects : {});\n      a.setFastWeightInitDuringBuild(r), i[n] = a;\n\n      for (var s = 0, l = e.inboundNodes; s < l.length; s++) {\n        var u = l[s];\n        if (!(u instanceof Array)) throw new ValueError(\"Corrupted configuration, expected array for nodeData: \" + u);\n        o(a, u);\n      }\n    }\n\n    for (var u = t.name, c = t.layers, p = 0, h = c; p < h.length; p++) {\n      l(g = h[p]);\n    }\n\n    for (; !isObjectEmpty(a);) {\n      for (var d = 0, f = c; d < f.length; d++) {\n        var g = f[d];\n\n        if ((k = i[g.name]).name in a) {\n          var m = a[k.name];\n          delete a[k.name];\n\n          for (var y = 0, v = m; y < v.length; y++) {\n            s(k, v[y]);\n          }\n        }\n      }\n    }\n\n    for (var b = [], w = [], z = 0, S = t.inputLayers; z < S.length; z++) {\n      var I = (g = S[z])[0],\n          A = g[1],\n          C = g[2];\n      assert(I in i);\n      var N = (k = i[I]).inboundNodes[A].outputTensors;\n      b.push(N[C]);\n    }\n\n    for (var _ = 0, E = t.outputLayers; _ < E.length; _++) {\n      I = (g = E[_])[0], A = g[1], C = g[2];\n      assert(I in i);\n      var k;\n      N = (k = i[I]).inboundNodes[A].outputTensors;\n      w.push(N[C]);\n    }\n\n    return new e({\n      inputs: b,\n      outputs: w,\n      name: u\n    });\n  }, Object.defineProperty(t.prototype, \"stateful\", {\n    get: function get() {\n      if (this._stateful) throw new ValueError(\"Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.\");\n\n      for (var e = 0, t = this.layers; e < t.length; e++) {\n        if (t[e].stateful) return !0;\n      }\n\n      return !1;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.resetStates = function () {\n    var e = this;\n    tidy(function () {\n      e.layers.forEach(function (e) {\n        e.stateful && e.resetStates();\n      });\n    });\n  }, t;\n}(Layer),\n    DEFAULT_VALIDATION_BATCH_SIZE = 32;\n\nfunction standardizeDataIteratorOutput(e, t) {\n  if (e.outputs.length > 1) throw new NotImplementedError(\"Support for training a model with multiple output tensors with a dataset object is not implemented yet.\");\n  util.assert(Array.isArray(t) && 2 === t.length, \"Dataset iterator for fitDataset() is expected to generate an Array of length 2: `[xs, ys]`, but instead generates \" + t);\n  var n = t,\n      r = n[0],\n      i = n[1];\n  if (r instanceof Tensor) return util.assert(1 === e.inputs.length, \"Model has multiple \" + e.inputs.length + \" inputs, hence it expects the input dataset to generate a dictionary of tensors  (with keys \" + JSON.stringify(e.inputNames) + \", but received a single tensor.\"), util.assert(r.shape[0] === i.shape[0], \"Mismatch in batch size between x and y tensors (\" + r.shape[0] + \" vs. \" + i.shape[0] + \")\"), [r, i];\n  var a = void 0;\n  r = r;\n\n  for (var o = [], s = 0, l = e.inputNames; s < l.length; s++) {\n    var u = l[s];\n    if (null == r[u]) throw new ValueError(\"The feature data generated by the dataset lacks the required input key '\" + u + \"'.\");\n    o.push(r[u]), null == a ? a = r[u].shape[0] : util.assert(r[u].shape[0] === a, \"Mismatch in batch size between x and y tensors (\" + r[u].shape[0] + \" vs. \" + i.shape[0] + \")\");\n  }\n\n  return o.concat(i);\n}\n\nfunction standardizeTensorValidationData(e) {\n  if (3 === e.length) throw new NotImplementedError(\"Validation with sample weights is not implemented yet.\");\n  return {\n    xs: e[0],\n    ys: e[1]\n  };\n}\n\nfunction fitDataset(e, t, n) {\n  return __awaiter(this, void 0, void 0, function () {\n    var r, i, a, o, s, l, u, c, p, h, d, f, g, m, y, v, b, w, z, S, I, A, C, N, _, E, k;\n\n    return __generator(this, function (L) {\n      switch (L.label) {\n        case 0:\n          if (r = null != n.batchesPerEpoch, util.assert(null != e.optimizer, \"You must compile a model before training/testing. Use Model.compile(modelCompileConfig).\"), util.assert(null != n, \"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.\"), util.assert(null != n.epochs && n.epochs > 0 && Number.isInteger(n.epochs), \"For fitDataset(), config.epochs is expected to be a positive integer, but got \" + n.epochs), util.assert(!r || n.batchesPerEpoch > 0 && Number.isInteger(n.batchesPerEpoch), \"For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got \" + n.batchesPerEpoch), util.assert(null == n.validationSplit, \"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.\"), e.isTraining) throw new Error(\"Cannot start training because another fit() call is ongoing.\");\n          e.isTraining = !0, L.label = 1;\n\n        case 1:\n          return L.trys.push([1,, 22, 23]), i = null != n.validationData, a = void 0, o = void 0, i && (isDatasetObject(n.validationData) ? util.assert(null == n.validationBatches || n.validationBatches > 0 && Number.isInteger(n.validationBatches), \"For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got \" + n.validationBatches) : (s = standardizeTensorValidationData(n.validationData), a = s.xs, o = s.ys)), l = e.makeTrainFunction(), u = e.getDedupedMetricsNames(), c = void 0, c = i ? u.slice().concat(u.map(function (e) {\n            return \"val_\" + e;\n          })) : u.slice(), p = standardizeCallbacks(n.callbacks), h = null == n.verbose ? 1 : n.verbose, d = configureCallbacks(p, n.yieldEvery, h, n.epochs, null, null, getStepsPerEpoch(t, n), null, i, c), f = d.callbackList, g = d.history, f.setModel(e), e.history = g, [4, f.onTrainBegin()];\n\n        case 2:\n          return L.sent(), e.stopTraining_ = !1, m = null == n.initialEpoch ? 0 : n.initialEpoch, [4, t.iterator()];\n\n        case 3:\n          y = L.sent(), L.label = 4;\n\n        case 4:\n          return m < n.epochs ? (v = {}, [4, f.onEpochBegin(m)]) : [3, 19];\n\n        case 5:\n          return L.sent(), b = 0, w = 0, r ? [3, 7] : [4, t.iterator()];\n\n        case 6:\n          y = L.sent(), L.label = 7;\n\n        case 7:\n          return !r || b < n.batchesPerEpoch ? [4, y.next()] : [3, 17];\n\n        case 8:\n          return z = L.sent(), r && z.done ? (console.warn(\"You provided `batchesPerEpoch` as \" + n.batchesPerEpoch + \", but your dataset iterator ran out of data after \" + b + \" batches; interrupting training. Make sure that your dataset can generate at least `batchesPerEpoch * epochs` batches (in this case, \" + n.batchesPerEpoch * n.epochs + \" batches). You may need to use the repeat() function when building your dataset.\"), [3, 17]) : null == z.value ? [3, 11] : (S = standardizeDataIteratorOutput(e, z.value), (I = {}).batch = w, I.size = S[0].shape[0], [4, f.onBatchBegin(w, I)]);\n\n        case 9:\n          for (L.sent(), A = l(S), dispose(S), k = 0; k < u.length; ++k) {\n            C = u[k], N = A[k], I[C] = N, keep(N);\n          }\n\n          return [4, f.onBatchEnd(w, I)];\n\n        case 10:\n          L.sent(), disposeTensorsInLogs(I), w++, b++, L.label = 11;\n\n        case 11:\n          return (r ? b >= n.batchesPerEpoch : z.done) ? i ? (_ = void 0, isDatasetObject(n.validationData) ? (E = toList, [4, e.evaluateDataset(n.validationData, {\n            batches: n.validationBatches\n          })]) : [3, 13]) : [3, 15] : [3, 16];\n\n        case 12:\n          return _ = E.apply(void 0, [L.sent()]), [3, 14];\n\n        case 13:\n          _ = toList(e.evaluate(a, o, {\n            batchSize: null == n.validationBatchSize ? DEFAULT_VALIDATION_BATCH_SIZE : n.validationBatchSize,\n            verbose: 0\n          })), L.label = 14;\n\n        case 14:\n          for (k = 0; k < e.metricsNames.length; ++k) {\n            v[\"val_\" + e.metricsNames[k]] = _[k];\n          }\n\n          L.label = 15;\n\n        case 15:\n          return [3, 17];\n\n        case 16:\n          return e.stopTraining_ ? [3, 17] : [3, 7];\n\n        case 17:\n          return [4, f.onEpochEnd(m, v)];\n\n        case 18:\n          return L.sent(), m++, e.stopTraining_ ? [3, 19] : [3, 4];\n\n        case 19:\n          return [4, f.onTrainEnd()];\n\n        case 20:\n          return L.sent(), [4, e.history.syncData()];\n\n        case 21:\n          return L.sent(), [2, e.history];\n\n        case 22:\n          return e.isTraining = !1, [7];\n\n        case 23:\n          return [2];\n      }\n    });\n  });\n}\n\nfunction getStepsPerEpoch(e, t) {\n  var n = null;\n  return null != t.batchesPerEpoch ? n = t.batchesPerEpoch : Number.isFinite(e.size) && (n = e.size), n;\n}\n\nfunction isDatasetObject(e) {\n  return \"function\" == typeof e.iterator;\n}\n\nfunction isLazyIteratorObject(e) {\n  return \"function\" == typeof e.next;\n}\n\nfunction evaluateDataset(e, t, n) {\n  return __awaiter(this, void 0, void 0, function () {\n    var r, i, a, o, s, l, u, c, p, h;\n    return __generator(this, function (d) {\n      switch (d.label) {\n        case 0:\n          if (r = null != (n = n || {}).batches, i = e.testFunction, a = [], n.verbose > 0) throw new NotImplementedError(\"Verbose mode is not implemented yet.\");\n          return util.assert(!r || n.batches > 0 && Number.isInteger(n.batches), \"Test loop expects `batches` to be a positive integer, but received \" + JSON.stringify(n.batches)), isLazyIteratorObject(t) ? (s = t, [3, 3]) : [3, 1];\n\n        case 1:\n          return [4, t.iterator()];\n\n        case 2:\n          s = d.sent(), d.label = 3;\n\n        case 3:\n          o = s, l = 0, u = 0, c = function c() {\n            var t, s, c, p, h, d;\n            return __generator(this, function (f) {\n              switch (f.label) {\n                case 0:\n                  return [4, o.next()];\n\n                case 1:\n                  if ((t = f.sent()).value) {\n                    if (s = standardizeDataIteratorOutput(e, t.value), c = tidy(function () {\n                      return i(s);\n                    }), dispose(s), 0 === u) for (d = 0; d < c.length; ++d) {\n                      a.push(getScalar(0));\n                    }\n\n                    for (p = s[0].shape[0], h = function h(e) {\n                      var t = c[e],\n                          n = a[e];\n                      a[e] = tidy(function () {\n                        return add(a[e], mul(getScalar(p), t));\n                      }), u > 0 && dispose(n);\n                    }, d = 0; d < c.length; ++d) {\n                      h(d);\n                    }\n\n                    dispose(c), l += p, ++u;\n                  }\n\n                  return t.done ? (r && console.warn(\"Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least `batches` batches (in this case, \" + n.batches + \" batches). You may need to use the repeat() function when building your dataset.\"), [2, \"break\"]) : [2];\n              }\n            });\n          }, d.label = 4;\n\n        case 4:\n          return !r || u < n.batches ? [5, c()] : [3, 6];\n\n        case 5:\n          return \"break\" === d.sent() ? [3, 6] : [3, 4];\n\n        case 6:\n          for (p = function p(e) {\n            var t = a[e];\n            a[e] = tidy(function () {\n              return div(a[e], getScalar(l));\n            }), dispose(t);\n          }, h = 0; h < a.length; ++h) {\n            p(h);\n          }\n\n          return [2, singletonOrArray(a)];\n      }\n    });\n  });\n}\n\nfunction checkBatchSize(e) {\n  util.assert(e > 0 && Number.isInteger(e), \"batchSize is required to be a positive integer, but got \" + e);\n}\n\nfunction sliceArrays(e, t, n) {\n  return null == e ? [null] : Array.isArray(e) ? e.map(function (e) {\n    return sliceAlongFirstAxis(e, t, n - t);\n  }) : sliceAlongFirstAxis(e, t, n - t);\n}\n\nfunction sliceArraysByIndices(e, t) {\n  return tidy(function () {\n    return null == e ? null : Array.isArray(e) ? e.map(function (e) {\n      return sliceArraysByIndices(e, t);\n    }) : gather$1(e, \"int32\" === t.dtype ? t : t.toInt());\n  });\n}\n\nfunction makeBatches(e, t) {\n  for (var n = [], r = 0, i = null; r < e;) {\n    (i = r + t) >= e && (i = e), n.push([r, i]), r = i;\n  }\n\n  return n;\n}\n\nfunction fitLoop(e, t, n, r, i, a, o, s, l, u, c, p, h, d, f, g) {\n  return __awaiter(this, void 0, void 0, function () {\n    var m, y, v, b, w, z, S, I;\n    return __generator(this, function (A) {\n      switch (A.label) {\n        case 0:\n          if (null == i && (i = 32), null == a && (a = 1), null == c && (c = !0), null == h && (h = 0), m = !1, null != l && null != u && (m = !0), null != f && (m = !0, null == d)) throw new ValueError(\"Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.\");\n          return null != (y = e.checkNumSamples(n, i, d, \"steps_per_epoch\")) && (v = range(0, y)), null == o && (o = 1), b = configureCallbacks(s, g, o, a, h, y, d, i, m, p), w = b.callbackList, z = b.history, w.setModel(e), e.history = z, [4, w.onTrainBegin()];\n\n        case 1:\n          A.sent(), e.stopTraining_ = !1, S = function S(a) {\n            var o, s, p, h, f;\n            return __generator(this, function (g) {\n              switch (g.label) {\n                case 0:\n                  return [4, w.onEpochBegin(a)];\n\n                case 1:\n                  if (g.sent(), o = {}, null == d) return [3, 2];\n                  throw new NotImplementedError(\"stepsPerEpoch mode is not implemented yet.\");\n\n                case 2:\n                  if (\"batch\" === c) throw new NotImplementedError(\"batch shuffling is not implemneted yet\");\n                  c && util.shuffle(v), s = tensor1d(v), p = makeBatches(y, i), h = function h(a) {\n                    var c;\n                    return __generator(this, function (h) {\n                      switch (h.label) {\n                        case 0:\n                          return c = {}, [4, w.onBatchBegin(a, c)];\n\n                        case 1:\n                          return h.sent(), tidy(function () {\n                            var h = p[a][0],\n                                d = p[a][1],\n                                f = sliceAlongFirstAxis(s, h, d - h);\n                            c.batch = a, c.size = d - h;\n\n                            for (var g = sliceArraysByIndices(n, f), y = t(g), v = 0; v < r.length; ++v) {\n                              var b = r[v],\n                                  w = y[v];\n                              c[b] = w, keep(w);\n                            }\n\n                            if (a === p.length - 1 && m) {\n                              var z = e.testLoop(l, u, i);\n\n                              for (v = 0; v < r.length; ++v) {\n                                b = r[v], w = z[v];\n                                keep(w), o[\"val_\" + b] = w;\n                              }\n                            }\n                          }), [4, w.onBatchEnd(a, c)];\n\n                        case 2:\n                          return h.sent(), disposeTensorsInLogs(c), e.stopTraining_ ? [2, \"break\"] : [2];\n                      }\n                    });\n                  }, f = 0, g.label = 3;\n\n                case 3:\n                  return f < p.length ? [5, h(f)] : [3, 6];\n\n                case 4:\n                  if (\"break\" === g.sent()) return [3, 6];\n                  g.label = 5;\n\n                case 5:\n                  return ++f, [3, 3];\n\n                case 6:\n                  s.dispose(), g.label = 7;\n\n                case 7:\n                  return [4, w.onEpochEnd(a, o)];\n\n                case 8:\n                  return g.sent(), e.stopTraining_ ? [2, \"break\"] : [2];\n              }\n            });\n          }, I = h, A.label = 2;\n\n        case 2:\n          return I < a ? [5, S(I)] : [3, 5];\n\n        case 3:\n          if (\"break\" === A.sent()) return [3, 5];\n          A.label = 4;\n\n        case 4:\n          return ++I, [3, 2];\n\n        case 5:\n          return [4, w.onTrainEnd()];\n\n        case 6:\n          return A.sent(), [4, e.history.syncData()];\n\n        case 7:\n          return A.sent(), [2, e.history];\n      }\n    });\n  });\n}\n\nfunction fitTensors(e, t, n, r) {\n  return void 0 === r && (r = {}), __awaiter(this, void 0, void 0, function () {\n    var i, a, o, s, l, u, c, p, h, d, f, g, m, y, v, b, w, z, S;\n    return __generator(this, function (I) {\n      switch (I.label) {\n        case 0:\n          if (e.isTraining) throw new Error(\"Cannot start training because another fit() call is ongoing.\");\n          e.isTraining = !0, I.label = 1;\n\n        case 1:\n          if (I.trys.push([1,, 3, 4]), checkBatchSize(c = null == r.batchSize ? 32 : r.batchSize), p = e.standardizeUserData(t, n, !1, c), i = p[0], a = p[1], h = !1, d = void 0, null != r.validationData && r.validationData.length > 0) {\n            if (h = !0, 2 !== r.validationData.length) throw 3 === r.validationData.length ? new NotImplementedError(\"validationData including sample weights is not supported yet.\") : new ValueError(\"When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; \" + r.validationData + \" is invalid.\");\n            o = r.validationData[0], s = r.validationData[1], f = e.standardizeUserData(o, s, !0, c), l = f[0], u = f[1], d = l.concat(u);\n          } else null != r.validationSplit && r.validationSplit > 0 && r.validationSplit < 1 ? (h = !0, g = Math.floor(i[0].shape[0] * (1 - r.validationSplit)), m = i[0].shape[0], l = sliceArrays(i, g, m), i = sliceArrays(i, 0, g), u = sliceArrays(a, g, m), a = sliceArrays(a, 0, g), d = l.concat(u)) : null != r.validationSteps && (h = !0);\n\n          return y = i.concat(a), e.checkTrainableWeightsConsistency(), v = e.makeTrainFunction(), b = e.getDedupedMetricsNames(), w = void 0, z = void 0, h ? (e.makeTestFunction(), w = e.testFunction, z = b.slice().concat(b.map(function (e) {\n            return \"val_\" + e;\n          }))) : (w = null, d = [], z = b.slice()), S = standardizeCallbacks(r.callbacks), [4, fitLoop(e, v, y, b, c, r.epochs, r.verbose, S, w, d, r.shuffle, z, r.initialEpoch, null, null, r.yieldEvery)];\n\n        case 2:\n          return [2, I.sent()];\n\n        case 3:\n          return e.isTraining = !1, disposeNewTensors(i, t), disposeNewTensors(a, n), disposeNewTensors(l, o), disposeNewTensors(u, s), [7];\n\n        case 4:\n          return [2];\n      }\n    });\n  });\n}\n\nfunction ensureTensorsRank2OrHigher(e) {\n  var t = [];\n  e instanceof Tensor && (e = [e]);\n\n  for (var n = 0; n < e.length; ++n) {\n    var r = e[n];\n    if (1 === r.rank) t.push(expandDims$1(r, 1));else {\n      if (0 === r.rank) throw new Error(\"Expected tensor to be at least 1D, but received a 0D tensor (scalar).\");\n      t.push(r);\n    }\n  }\n\n  return t;\n}\n\nfunction disposeNewTensors(e, t) {\n  if (null != e) {\n    var n = [];\n    if (t instanceof Tensor) n.push(t.id);else if (Array.isArray(t)) t.forEach(function (e) {\n      return n.push(e.id);\n    });else if (null != t) for (var r in t) {\n      var i = t[r];\n      n.push(i.id);\n    }\n    var a = [];\n    if (e instanceof Tensor) -1 === n.indexOf(e.id) && a.push(e);else if (Array.isArray(e)) e.forEach(function (e) {\n      -1 === n.indexOf(e.id) && a.push(e);\n    });else if (null != e) for (var o in e) {\n      var s = e[o];\n      -1 === n.indexOf(s.id) && a.push(s);\n    }\n    a.forEach(function (e) {\n      e.isDisposed || e.dispose();\n    });\n  }\n}\n\nfunction isDataTensor(e) {\n  return e instanceof Tensor;\n}\n\nfunction isDataArray(e) {\n  return Array.isArray(e);\n}\n\nfunction isDataDict(e) {\n  return !isDataTensor(e) && !isDataArray(e);\n}\n\nfunction standardizeInputData(e, t, n, r, i) {\n  if (void 0 === r && (r = !0), void 0 === i && (i = \"\"), null == t || 0 === t.length) {\n    if (null != e) {\n      var a = !1;\n      if (isDataArray(e) && e.length > 0) a = !0;else if (isDataDict(e)) {\n        for (var o in e) {\n          if (e.hasOwnProperty(o)) {\n            a = !0;\n            break;\n          }\n        }\n      } else a = !0;\n      if (a) throw new ValueError(\"Error when checking model \" + i + \" expected no data, but got \" + e);\n    }\n\n    return [];\n  }\n\n  if (null == e) return t.map(function (e) {\n    return null;\n  });\n  var s;\n\n  if (isDataDict(e)) {\n    e = e, s = [];\n\n    for (var l = 0, u = t; l < u.length; l++) {\n      var c = u[l];\n      if (null == e[c]) throw new ValueError('No data provided for \"' + c + '\". Need data for each key in: ' + t);\n      s.push(e[c]);\n    }\n  } else if (isDataArray(e)) {\n    if ((e = e).length !== t.length) throw new ValueError(\"Error when checking model \" + i + \": the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see \" + t.length + \" Tensor(s), but instead got the following list of Tensor(s): \" + e);\n    s = e;\n  } else {\n    if (e = e, t.length > 1) throw new ValueError(\"The model \" + i + \" expects \" + t.length + \" Tensor(s), but only received one Tensor. Found: Tensor with shape \" + e.shape);\n    s = [e];\n  }\n\n  if (s = ensureTensorsRank2OrHigher(s), null != n) for (var p = 0; p < t.length; ++p) {\n    if (null != n[p]) {\n      var h = s[p];\n      if (h.shape.length !== n[p].length) throw new ValueError(\"Error when checking \" + i + \": expected \" + t[p] + \" to have \" + n[p].length + \" dimension(s). but got array with shape \" + h.shape);\n\n      for (var d = 0; d < n[p].length; ++d) {\n        if (0 !== d || r) {\n          var f = h.shape[d],\n              g = n[p][d];\n          if (null != g && g >= 0 && f !== g) throw new ValueError(\"Error when checking \" + i + \": expected \" + t[p] + \" to have shape [\" + n[p] + \"], but got array with shape [\" + h.shape + \"].\");\n        }\n      }\n    }\n  }\n  return s;\n}\n\nfunction checkArrayLengths(e, t, n) {\n  var r = unique(e.map(function (e) {\n    return e.shape[0];\n  }));\n  r.sort();\n  var i = unique(t.map(function (e) {\n    return e.shape[0];\n  }));\n  if (i.sort(), r.length > 1) throw new ValueError(\"All input Tensors (x) should have the same number of samples. Got array shapes: \" + JSON.stringify(e.map(function (e) {\n    return e.shape;\n  })));\n  if (i.length > 1) throw new ValueError(\"All target Tensors (y) should have the same number of samples. Got array shapes: \" + JSON.stringify(t.map(function (e) {\n    return e.shape;\n  })));\n  if (r.length > 0 && i.length > 0 && !util.arraysEqual(r, i)) throw new ValueError(\"Input Tensors should have the same number of samples as target Tensors. Found \" + r[0] + \" input sample(s) and \" + i[0] + \" target sample(s).\");\n}\n\nfunction checkLossAndTargetCompatibility(e, t, n) {\n  for (var r = [meanSquaredError, binaryCrossentropy, categoricalCrossentropy], i = 0; i < e.length; ++i) {\n    var a = e[i],\n        o = t[i],\n        s = n[i];\n\n    if (null != o) {\n      if (o === categoricalCrossentropy && 1 === a.shape[a.shape.length - 1]) throw new ValueError(\"You are passing a target array of shape \" + a.shape + \" while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].\");\n      if (-1 !== r.indexOf(o)) for (var l = a.shape.slice(1), u = s.slice(1), c = 0; c < l.length; ++c) {\n        var p = l[c],\n            h = u[c];\n        if (null != h && p !== h) throw new ValueError(\"A target Tensor with shape \" + a.shape + \" was passed for an output of shape \" + s + \", while using a loss function that expects targets to have the same shape as the output.\");\n      }\n    }\n  }\n}\n\nfunction checkInputData(e, t, n, r, i) {\n  var a;\n\n  if (void 0 === r && (r = !0), void 0 === i && (i = \"\"), Array.isArray(e)) {\n    if (e.length !== t.length) throw new ValueError(\"Error when checking model \" + i + \": the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see \" + t.length + \" Tensor(s), but instead got \" + e.length + \" Tensors(s).\");\n    a = e;\n  } else {\n    if (t.length > 1) throw new ValueError(\"The model expects \" + t.length + \" \" + i + \" Tensors, but only received one Tensor. Found: array with shape \" + JSON.stringify(e.shape) + \".\");\n    a = [e];\n  }\n\n  if (null != n) for (var o = 0; o < t.length; ++o) {\n    if (null != n[o]) {\n      var s = a[o];\n      if (s.shape.length !== n[o].length) throw new ValueError(\"Error when checking \" + i + \": expected \" + t[o] + \" to have \" + n[o].length + \" dimension(s), but got array with shape \" + JSON.stringify(s.shape));\n\n      for (var l = 0; l < n[o].length; ++l) {\n        if (0 !== l || r) {\n          var u = s.shape[l],\n              c = n[o][l];\n          if (null != c && c !== u) throw new ValueError(\"Error when checking \" + i + \": expected \" + t[o] + \" to have shape \" + JSON.stringify(n[o]) + \" but got array with shape \" + JSON.stringify(s.shape) + \".\");\n        }\n      }\n    }\n  }\n}\n\nfunction collectMetrics(e, t) {\n  if (null == e || Array.isArray(e) && 0 === e.length) return t.map(function (e) {\n    return [];\n  });\n  if (Array.isArray(e)) return t.map(function (t) {\n    return e;\n  });\n\n  if (null != e) {\n    for (var n = [], r = 0, i = t; r < i.length; r++) {\n      var a = i[r],\n          o = e.hasOwnProperty(a) ? e[a] : [];\n      Array.isArray(o) || (o = [o]), n.push(o);\n    }\n\n    return n;\n  }\n\n  throw new TypeError(\"Type of metrics argument not understood. Expected an Array or Object, found: \" + e);\n}\n\nvar Model = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.isTraining = !1, n;\n  }\n\n  return __extends(t, e), t.prototype.summary = function (e, t, n) {\n    if (void 0 === n && (n = console.log), !this.built) throw new ValueError(\"This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).\");\n    printSummary(this, e, t, n);\n  }, t.prototype.compile = function (e) {\n    var t = this;\n    if (null == e.loss && (e.loss = []), this.loss = e.loss, \"string\" == typeof e.optimizer) this.optimizer = getOptimizer(e.optimizer);else {\n      if (!(e.optimizer instanceof Optimizer)) throw new ValueError(\"User-defined optimizer must be an instance of tf.Optimizer.\");\n      this.optimizer = e.optimizer;\n    }\n    var n = [];\n    if (Array.isArray(e.loss) || \"string\" == typeof e.loss || \"function\" == typeof e.loss) {\n      if (Array.isArray(e.loss)) {\n        if (e.loss.length !== this.outputs.length) throw new ValueError(\"When passing an Array as loss, it should have one entry per model output. The model has \" + this.outputs.length + \" output(s), but you passed loss=\" + e.loss + \".\");\n        var r = e.loss;\n        n = r.map(function (e) {\n          return get(e);\n        });\n      } else {\n        var i = get(e.loss);\n        this.outputs.forEach(function (e) {\n          n.push(i);\n        });\n      }\n    } else {\n      for (var a in e.loss = e.loss, e.loss) {\n        if (-1 === this.outputNames.indexOf(a)) throw new ValueError('Unknown entry in loss dictionary: \"' + a + '\". Only expected the following keys: ' + this.outputNames);\n      }\n\n      for (var o = 0, s = this.outputNames; o < s.length; o++) {\n        var l = s[o];\n        null == e.loss[l] && console.warn('Output \"' + l + '\" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ' + l + \" during training\"), n.push(get(e.loss[l]));\n      }\n    }\n    this.lossFunctions = n, this.feedOutputNames = [], this.feedOutputShapes = [], this.feedLossFns = [];\n\n    for (var u = 0; u < this.outputs.length; ++u) {\n      var c = this.internalOutputShapes[u],\n          p = this.outputNames[u];\n      this.feedOutputNames.push(p), this.feedOutputShapes.push(c), this.feedLossFns.push(this.lossFunctions[u]);\n    }\n\n    var h = [];\n    this.metrics = e.metrics, this.metricsNames = [\"loss\"], this.metricsTensors = [], nameScope(\"loss\", function () {\n      for (var e = 0; e < t.outputs.length; ++e) {\n        if (-1 === h.indexOf(e)) {\n          var n = t.lossFunctions[e];\n          t.outputs.length > 1 && (t.metricsTensors.push([n, e]), t.metricsNames.push(t.outputNames[e] + \"_loss\"));\n        }\n      }\n    });\n    var d = collectMetrics(e.metrics, this.outputNames);\n    nameScope(\"metric\", function () {\n      for (var e = function e(_e2) {\n        if (-1 !== h.indexOf(_e2)) return \"continue\";\n        !function (n) {\n          for (var r, i, a, o = function o(n) {\n            if (-1 !== [\"accuracy\", \"acc\", \"crossentropy\", \"ce\"].indexOf(n)) {\n              var o = t.internalOutputShapes[_e2];\n              1 === o[o.length - 1] || t.lossFunctions[_e2] === binaryCrossentropy ? -1 !== [\"accuracy\", \"acc\"].indexOf(n) ? i = binaryAccuracy : -1 !== [\"crossentropy\", \"ce\"].indexOf(n) && (i = binaryCrossentropy$1) : t.lossFunctions[_e2] === sparseCategoricalCrossentropy ? -1 !== [\"accuracy\", \"acc\"].indexOf(n) ? i = sparseCategoricalAccuracy : -1 !== [\"crossentropy\", \"ce\"].indexOf(n) && (i = sparseCategoricalCrossentropy$1) : -1 !== [\"accuracy\", \"acc\"].indexOf(n) ? i = categoricalAccuracy : -1 !== [\"crossentropy\", \"ce\"].indexOf(n) && (i = categoricalCrossentropy$1);\n              var s = void 0;\n              -1 !== [\"accuracy\", \"acc\"].indexOf(n) ? s = \"acc\" : -1 !== [\"crossentropy\", \"ce\"].indexOf(n) && (s = \"ce\"), a = i, r = \"\" + s;\n            } else {\n              var l = get$1(n);\n              a = l, r = \"\" + n;\n            }\n\n            var u;\n            nameScope(r, function () {\n              u = a;\n            }), function (e, n, r) {\n              t.outputNames.length > 1 && (n = t.outputNames[e] + \"_\" + n), t.metricsNames.push(n), t.metricsTensors.push([r, e]);\n            }(_e2, r, u);\n          }, s = 0, l = n; s < l.length; s++) {\n            o(l[s]);\n          }\n        }(d[_e2]);\n      }, n = 0; n < t.outputs.length; ++n) {\n        e(n);\n      }\n    }), this.collectedTrainableWeights = this.trainableWeights;\n  }, t.prototype.checkTrainableWeightsConsistency = function () {\n    null != this.collectedTrainableWeights && this.trainableWeights.length !== this.collectedTrainableWeights.length && console.warn(\"Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?\");\n  }, t.prototype.evaluate = function (e, t, n) {\n    void 0 === n && (n = {});\n    var r = null == n.batchSize ? 32 : n.batchSize;\n    checkBatchSize(r);\n    var i = this.standardizeUserData(e, t, !0, r);\n\n    try {\n      var a = i[0].concat(i[1]);\n      this.makeTestFunction();\n      var o = this.testFunction;\n      return singletonOrArray(this.testLoop(o, a, r, n.verbose, n.steps));\n    } finally {\n      disposeNewTensors(i[0], e), disposeNewTensors(i[1], t);\n    }\n  }, t.prototype.evaluateDataset = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        return this.makeTestFunction(), [2, evaluateDataset(this, e, t)];\n      });\n    });\n  }, t.prototype.checkNumSamples = function (e, t, n, r) {\n    var i;\n\n    if (void 0 === r && (r = \"steps\"), null != n) {\n      if (i = null, null != t) throw new ValueError(\"If \" + r + \" is set, batchSize must be null or undefined.Got batchSize = \" + t);\n    } else {\n      if (null == e) throw new ValueError(\"Either the input data should have a defined shape, or \" + r + \" shoud be specified.\");\n      i = Array.isArray(e) ? e[0].shape[0] : e.shape[0];\n    }\n\n    return i;\n  }, t.prototype.execute = function (e, t) {\n    if (Array.isArray(t) && 0 === t.length) throw new ValueError(\"`outputs` is an empty Array, which is not allowed.\");\n    var n = Array.isArray(t),\n        r = n ? t : [t],\n        i = this.retrieveSymbolicTensors(r),\n        a = new FeedDict();\n\n    if (e instanceof Tensor && (e = [e]), Array.isArray(e)) {\n      if (e.length !== this.inputs.length) throw new ValueError(\"The number of inputs provided (\" + e.length + \") does not match the number of inputs of this model (\" + this.inputs.length + \").\");\n\n      for (var o = 0; o < this.inputs.length; ++o) {\n        a.add(this.inputs[o], e[o]);\n      }\n    } else for (var s = 0, l = this.inputs; s < l.length; s++) {\n      var u = l[s],\n          c = e[u.name];\n      if (null == c) throw new ValueError(\"No value is provided for the model's input \" + u.name);\n      a.add(u, c);\n    }\n\n    var p = execute(i, a);\n    return n ? p : p[0];\n  }, t.prototype.retrieveSymbolicTensors = function (e) {\n    for (var t = pyListRepeat(null, e.length), n = e.length, r = 0, i = this.layers; r < i.length; r++) {\n      for (var a = i[r], o = Array.isArray(a.output) ? a.output : [a.output], s = o.map(function (e) {\n        return e.name;\n      }), l = 0; l < e.length; ++l) {\n        var u = s.indexOf(e[l]);\n        if (-1 !== u && (t[l] = o[u], n--), 0 === n) break;\n      }\n\n      if (0 === n) break;\n    }\n\n    if (n > 0) {\n      var c = [];\n      throw t.forEach(function (t, n) {\n        null == t && c.push(e[n]);\n      }), new ValueError(\"Cannot find SymbolicTensors for output name(s): \" + JSON.stringify(c));\n    }\n\n    return t;\n  }, t.prototype.predictLoop = function (e, t, n) {\n    var r = this;\n    return void 0 === t && (t = 32), void 0 === n && (n = !1), tidy(function () {\n      var i = r.checkNumSamples(e);\n      if (n) throw new NotImplementedError(\"Verbose predictLoop() is not implemented yet.\");\n\n      for (var a = makeBatches(i, t), o = r.outputs.map(function (e) {\n        return [];\n      }), s = function s(t) {\n        tidy(function () {\n          var n = a[t][0],\n              i = a[t][1],\n              o = sliceArrays(e, n, i),\n              s = [];\n          if (Array.isArray(o)) for (var l = 0; l < o.length; ++l) {\n            s.push({\n              key: r.inputs[l],\n              value: o[l]\n            });\n          } else s.push({\n            key: r.inputs[0],\n            value: o\n          });\n          var u = new FeedDict(s);\n          return execute(r.outputs, u);\n        }).forEach(function (e, t) {\n          return o[t].push(e);\n        });\n      }, l = 0; l < a.length; ++l) {\n        s(l);\n      }\n\n      return singletonOrArray(o.map(function (e) {\n        return concat(e, 0);\n      }));\n    });\n  }, t.prototype.predict = function (e, t) {\n    void 0 === t && (t = {});\n    var n = ensureTensorsRank2OrHigher(e);\n    checkInputData(n, this.inputNames, this.feedInputShapes, !1);\n\n    try {\n      var r = null == t.batchSize ? 32 : t.batchSize;\n      return checkBatchSize(r), this.predictLoop(n, r);\n    } finally {\n      disposeNewTensors(n, e);\n    }\n  }, t.prototype.predictOnBatch = function (e) {\n    return checkInputData(e, this.inputNames, this.feedInputShapes, !0), this.predictLoop(e, e.shape[0]);\n  }, t.prototype.standardizeUserData = function (e, t, n, r) {\n    if (void 0 === n && (n = !0), null == this.optimizer) throw new RuntimeError(\"You must compile a model before training/testing. Use Model.compile(modelCompileArgs).\");\n\n    for (var i = [], a = 0; a < this.feedOutputShapes.length; ++a) {\n      var o = this.feedOutputShapes[a];\n      this.feedLossFns[a] === sparseCategoricalCrossentropy ? i.push(o.slice(0, o.length - 1).concat([1])) : i.push(o);\n    }\n\n    if (checkArrayLengths(e = standardizeInputData(e, this.feedInputNames, this.feedInputShapes, !1, \"input\"), t = standardizeInputData(t, this.feedOutputNames, i, !1, \"target\"), null), checkLossAndTargetCompatibility(t, this.feedLossFns, this.feedOutputShapes), this.stateful && null != r && r > 0 && e[0].shape[0] % r != 0) throw new ValueError(\"In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size \" + r + \". Found: \" + e[0].shape[0] + \" sample(s).\");\n    return [e, t, null];\n  }, t.prototype.testLoop = function (e, t, n, r, i) {\n    var a = this;\n    return void 0 === r && (r = 0), tidy(function () {\n      var o = a.checkNumSamples(t, n, i, \"steps\"),\n          s = [];\n      if (r > 0) throw new NotImplementedError(\"Verbose mode is not implemented yet.\");\n      if (null != i) throw new NotImplementedError(\"steps mode in testLoop() is not implemented yet\");\n\n      for (var l = makeBatches(o, n), u = tensor1d(range(0, o)), c = 0; c < l.length; ++c) {\n        var p = l[c][0],\n            h = l[c][1],\n            d = sliceAlongFirstAxis(u, p, h - p),\n            f = sliceArraysByIndices(t, d),\n            g = e(f);\n        if (0 === c) for (var m = 0; m < g.length; ++m) {\n          s.push(getScalar(0));\n        }\n\n        for (m = 0; m < g.length; ++m) {\n          var y = g[m];\n          s[m] = add(s[m], mul(getScalar(h - p), y));\n        }\n      }\n\n      for (m = 0; m < s.length; ++m) {\n        s[m] = div(s[m], getScalar(o));\n      }\n\n      return s;\n    });\n  }, t.prototype.getDedupedMetricsNames = function () {\n    for (var e = this.metricsNames, t = [], n = 0; n < e.length; ++n) {\n      var r = e[n],\n          i = r;\n      if (count(e, r) > 1) i += \"_\" + count(e.slice(0, n), r);\n      t.push(i);\n    }\n\n    return t;\n  }, t.prototype.makeTrainFunction = function () {\n    var e = this;\n    return function (t) {\n      var n = t.slice(0, e.inputs.length),\n          r = t.slice(e.inputs.length, e.inputs.length + e.outputs.length),\n          i = [],\n          a = e.collectedTrainableWeights.map(function (e) {\n        return e.read();\n      });\n      return [e.optimizer.minimize(function () {\n        for (var t = [], a = 0; a < e.inputs.length; ++a) {\n          t.push({\n            key: e.inputs[a],\n            value: n[a]\n          });\n        }\n\n        var o,\n            s = new FeedDict(t),\n            l = execute(e.outputs, s, {\n          training: !0\n        });\n\n        for (a = 0; a < e.lossFunctions.length; ++a) {\n          var u = (0, e.lossFunctions[a])(r[a], l[a]);\n          mean(u), o = 0 === a ? u : add(o, u);\n        }\n\n        for (a = 0; a < e.metricsTensors.length; ++a) {\n          var c = e.metricsTensors[a][0],\n              p = e.metricsTensors[a][1],\n              h = mean(c(r[p], l[p]));\n          keep(h), i.push(h);\n        }\n\n        return o = mean(o), e.calculateLosses().forEach(function (e) {\n          o = add(o, e);\n        }), o;\n      }, !0, a)].concat(i);\n    };\n  }, t.prototype.makeTestFunction = function () {\n    var e = this;\n\n    this.testFunction = function (t) {\n      return tidy(function () {\n        for (var n, r = [], i = t.slice(0, e.inputs.length), a = t.slice(e.inputs.length, e.inputs.length + e.outputs.length), o = [], s = 0; s < e.inputs.length; ++s) {\n          o.push({\n            key: e.inputs[s],\n            value: i[s]\n          });\n        }\n\n        var l = new FeedDict(o),\n            u = execute(e.outputs, l);\n\n        for (s = 0; s < e.lossFunctions.length; ++s) {\n          var c = e.lossFunctions[s],\n              p = mean(c(a[s], u[s]));\n          n = 0 === s ? p : add(n, p), r.push(n);\n        }\n\n        for (s = 0; s < e.metricsTensors.length; ++s) {\n          var h = e.metricsTensors[s][0],\n              d = e.metricsTensors[s][1],\n              f = mean(h(a[d], u[d]));\n          r.push(f);\n        }\n\n        return r;\n      });\n    };\n  }, t.prototype.fit = function (e, t, n) {\n    return void 0 === n && (n = {}), __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (r) {\n        return [2, fitTensors(this, e, t, n)];\n      });\n    });\n  }, t.prototype.fitDataset = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        return [2, fitDataset(this, e, t)];\n      });\n    });\n  }, t.prototype.trainOnBatch = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n, r, i, a, o, s, l, u, c, p, h;\n      return __generator(this, function (d) {\n        switch (d.label) {\n          case 0:\n            n = this.standardizeUserData(e, t), r = n[0], i = n[1], a = this.makeTrainFunction(), o = a(r.concat(i)), s = [], l = 0, u = o, d.label = 1;\n\n          case 1:\n            return l < u.length ? (c = u[l], h = (p = s).push, [4, c.data()]) : [3, 4];\n\n          case 2:\n            h.apply(p, [d.sent()[0]]), d.label = 3;\n\n          case 3:\n            return l++, [3, 1];\n\n          case 4:\n            return dispose(o), [2, singletonOrArray(s)];\n        }\n      });\n    });\n  }, t.prototype.getNamedWeights = function (e) {\n    for (var t = {}, n = null != e && e.trainableOnly, r = n ? this.trainableWeights : this.weights, i = this.getWeights(n), a = 0; a < r.length; ++a) {\n      n && !r[a].trainable || (t[r[a].originalName] = i[a]);\n    }\n\n    return t;\n  }, Object.defineProperty(t.prototype, \"stopTraining\", {\n    set: function set(e) {\n      this.stopTraining_ = e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.save = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      var n, r, i, a, o;\n      return __generator(this, function (s) {\n        switch (s.label) {\n          case 0:\n            if (\"string\" == typeof e) {\n              if (0 === (n = io.getSaveHandlers(e)).length) throw new ValueError(\"Cannot find any save handlers for URL '\" + e + \"'\");\n              if (n.length > 1) throw new ValueError(\"Found more than one (\" + n.length + \") save handlers for URL '\" + e + \"'\");\n              e = n[0];\n            }\n\n            if (null == e.save) throw new ValueError(\"Model.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.\");\n            return [4, io.encodeWeights(this.getNamedWeights(t))];\n\n          case 1:\n            return r = s.sent(), i = !1, a = null, o = this.toJSON(a, i), [2, e.save({\n              modelTopology: o,\n              weightData: r.data,\n              weightSpecs: r.specs\n            })];\n        }\n      });\n    });\n  }, t.className = \"Model\", t;\n}(Container);\n\nfunction modelFromJSON(e, t) {\n  return __awaiter(this, void 0, void 0, function () {\n    var n, r, i, a, o, s, l, u, c, p;\n    return __generator(this, function (h) {\n      switch (h.label) {\n        case 0:\n          return \"modelTopology\" in e || (e = {\n            modelTopology: e\n          }), null != (n = (e = e).modelTopology).model_config && (n = n.model_config), r = convertPythonicToTs(n), i = deserialize(r, t), null == e.weightsManifest ? [3, 2] : [4, io.loadWeights(e.weightsManifest, e.pathPrefix, i.weights.map(function (e) {\n            return e.originalName;\n          }))];\n\n        case 1:\n          for (a = h.sent(), o = {}, s = 0, l = i.weights; s < l.length; s++) {\n            u = l[s], o[u.originalName] = a[u.originalName];\n          }\n\n          c = null, p = !0, i.loadWeights(o, c, p), h.label = 2;\n\n        case 2:\n          return [2, i];\n      }\n    });\n  });\n}\n\nfunction loadModelInternal(e, t) {\n  return __awaiter(this, void 0, void 0, function () {\n    var n;\n    return __generator(this, function (r) {\n      if (null == t && (t = {}), \"string\" == typeof e) {\n        if (0 === (n = io.getLoadHandlers(e)).length) n.push(io.browserHTTPRequest(e, null, null, null, t.onProgress));else if (n.length > 1) throw new ValueError(\"Found more than one (\" + n.length + \") load handlers for URL '\" + e + \"'\");\n        e = n[0];\n      }\n\n      return [2, loadModelFromIOHandler(e, void 0, t)];\n    });\n  });\n}\n\nfunction loadModelFromIOHandler(e, t, n) {\n  return __awaiter(this, void 0, void 0, function () {\n    var r, i, a, o, s, l, u;\n    return __generator(this, function (c) {\n      switch (c.label) {\n        case 0:\n          if (null == n && (n = {}), null == e.load) throw new ValueError(\"Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.\");\n          return [4, e.load()];\n\n        case 1:\n          if (r = c.sent(), null != (i = r.modelTopology).model_config && (i = i.model_config), a = null == n.strict || n.strict, o = null != r.weightData && null != r.weightSpecs && a, s = deserialize(convertPythonicToTs(i), t, o), null != r.weightData) {\n            if (null == r.weightSpecs) throw new ValueError(\"Model artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.\");\n            l = !1, u = !0, s.loadWeights(io.decodeWeights(r.weightData, r.weightSpecs), l, u, a);\n          }\n\n          return [2, s];\n      }\n    });\n  });\n}\n\nserialization.registerClass(Model);\n\nvar Sequential = function (e) {\n  function t(t) {\n    var n = e.call(this, {\n      inputs: [],\n      outputs: []\n    }) || this;\n    if (t = t || {}, n.trainable = !0, n._updatable = !0, n.built = !1, n.name = null != t.name ? t.name : getUid(\"sequential_\"), null != t.layers) for (var r = 0, i = t.layers; r < i.length; r++) {\n      var a = i[r];\n      n.add(a);\n    }\n    return n;\n  }\n\n  return __extends(t, e), t.prototype.checkShape = function (e) {\n    if (e.inboundNodes[0].outputTensors[0].shape.some(function (e) {\n      return e < 0;\n    })) throw new ValueError(\"Negative dimension size caused by adding layer \" + e.name + \" with input shape [\" + e.inboundNodes[0].inputTensors[0].shape + \"]\");\n  }, t.prototype.add = function (e) {\n    var n,\n        r = e instanceof t || e instanceof Model;\n\n    if (r) {\n      if (1 !== (n = e).outputs.length) throw new ValueError(\"All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\");\n      if (1 !== n.inputs.length) throw new ValueError(\"All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.\");\n    }\n\n    if (0 === this.outputs.length) {\n      if (0 === e.inboundNodes.length) {\n        if (null == e.batchInputShape) throw new ValueError(\"The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.\");\n        var i = Input({\n          batchShape: e.batchInputShape,\n          dtype: e.dtype,\n          name: e.name + \"_input\"\n        });\n        e.apply(i);\n      }\n\n      if (r) this.outputs = n.outputs, this.inputs = n.inputs;else {\n        if (1 !== e.inboundNodes.length) throw new ValueError(\"A layer added to a Sequential model must not already be connected somewhere else. Model received layer \" + e.name + \" which has \" + e.inboundNodes.length + \" pre-existing inbound connections.\");\n        if (1 !== e.inboundNodes[0].outputTensors.length) throw new ValueError(\"All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\");\n        this.checkShape(e), this.outputs = [e.inboundNodes[0].outputTensors[0]], this.inputs = getSourceInputs(this.outputs[0]);\n      }\n      this.inboundNodes = [], new Node({\n        outboundLayer: this,\n        inboundLayers: [],\n        nodeIndices: [],\n        tensorIndices: [],\n        inputTensors: this.inputs,\n        outputTensors: this.outputs,\n        inputMasks: pyListRepeat(null, this.inputs.length),\n        outputMasks: [null],\n        inputShapes: this.inputs.map(function (e) {\n          return e.shape;\n        }),\n        outputShapes: this.outputs[0].shape\n      });\n    } else {\n      var a = e.apply(this.outputs[0]);\n      if (Array.isArray(a)) throw new TypeError(\"All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\");\n      this.checkShape(e), this.outputs = [a], this.inboundNodes[0].outputTensors = this.outputs, this.inboundNodes[0].outputShapes = [this.outputs[0].shape];\n    }\n\n    this.layers.push(e), this.built = !1;\n  }, t.prototype.pop = function () {\n    if (0 === this.layers.length) throw new TypeError(\"There are no layers in the model.\");\n    if (this.layers.pop(), 0 === this.layers.length) this.outputs = [], this.inboundNodes = [], this.outboundNodes = [];else {\n      var e = this.layers.length - 1;\n      this.layers[e].outboundNodes = [], this.outputs = [this.layers[e].output], this.inboundNodes[0].outputTensors = this.outputs, this.inboundNodes[0].outputShapes = [this.outputs[0].shape];\n    }\n  }, t.prototype.call = function (e, t) {\n    return null == this.model && this.build(), this.model.call(e, t);\n  }, t.prototype.build = function (e) {\n    if (getExactlyOneShape(e), 0 === this.inputs.length || 0 === this.outputs.length) throw new TypeError(\"Sequential model cannot be built: model is empty. Add some layers first.\");\n    this.model = new Model({\n      inputs: this.inputs,\n      outputs: this.outputs[0],\n      name: this.name + \"_model\"\n    }), this.model.trainable = this.trainable, this.model.updatable = this.updatable, this.supportsMasking = this.model.supportsMasking, this.inputLayers = this.model.inputLayers, this.inputLayersNodeIndices = this.model.inputLayersNodeIndices, this.inputLayersTensorIndices = this.model.inputLayersTensorIndices, this.outputLayers = this.model.outputLayers, this.outputLayersNodeIndices = this.model.outputLayersNodeIndices, this.outputLayersTensorIndices = this.model.outputLayersTensorIndices, this.nodesByDepth = this.model.nodesByDepth, this.containerNodes = this.model.containerNodes, this.outputNames = this.model.outputNames, this.inputNames = this.model.inputNames, this.built = !0;\n  }, t.prototype.countParams = function () {\n    return this.built || this.build(), e.prototype.countParams.call(this);\n  }, t.prototype.summary = function (t, n, r) {\n    void 0 === r && (r = console.log), this.built || this.build(), e.prototype.summary.call(this, t, n, r);\n  }, t.prototype.setWeights = function (e) {\n    null == this.model && this.build(), this.model.setWeights(e);\n  }, Object.defineProperty(t.prototype, \"updatable\", {\n    get: function get() {\n      return this._updatable;\n    },\n    set: function set(e) {\n      this.built && (this.model.updatable = e), this._updatable = e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.evaluate = function (e, t, n) {\n    if (void 0 === n && (n = {}), !this.built) throw new RuntimeError(\"The model needs to be compiled before being used.\");\n    return this.model.evaluate(e, t, n);\n  }, t.prototype.evaluateDataset = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        if (!this.built) throw new RuntimeError(\"The model needs to be compiled before being used.\");\n        return [2, this.model.evaluateDataset(e, t)];\n      });\n    });\n  }, t.prototype.predict = function (e, t) {\n    return void 0 === t && (t = {}), null == this.model && this.build(), this.model.predict(e, t);\n  }, t.prototype.predictOnBatch = function (e) {\n    return null == this.model && this.build(), this.model.predictOnBatch(e);\n  }, t.prototype.compile = function (e) {\n    this.build(), this.model.compile(e), this.optimizer = this.model.optimizer, this.loss = this.model.loss, this.metrics = this.model.metrics, this.metricsTensors = this.model.metricsTensors, this.metricsNames = this.model.metricsNames;\n  }, t.prototype.fit = function (e, t, n) {\n    return void 0 === n && (n = {}), __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (r) {\n        if (!this.built) throw new RuntimeError(\"The model needs to be compiled before being used.\");\n        return [2, this.model.fit(e, t, n)];\n      });\n    });\n  }, t.prototype.fitDataset = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        if (!this.built) throw new RuntimeError(\"The model needs to be compiled before being used.\");\n        return [2, this.model.fitDataset(e, t)];\n      });\n    });\n  }, t.prototype.trainOnBatch = function (e, t) {\n    return __awaiter(this, void 0, void 0, function () {\n      return __generator(this, function (n) {\n        return [2, this.model.trainOnBatch(e, t)];\n      });\n    });\n  }, t.fromConfig = function (e, n, r, i) {\n    var a;\n    void 0 === r && (r = {}), void 0 === i && (i = !1);\n    var o = {};\n\n    if (n instanceof Array) {\n      if (null == n[0].className || \"Merge\" === n[0].className) throw new ValueError(\"Legacy serialization format not supported yet.\");\n      a = n;\n    } else util.assert(null != n.layers, \"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.\"), a = n.layers, delete n.layers, o = n;\n\n    var s = new e(o);\n    if (!(s instanceof t)) throw new NotImplementedError(\"Sequential.fromConfig called on non-Sequential input: \" + s);\n\n    for (var l = 0, u = a; l < u.length; l++) {\n      var c = deserialize(u[l], void 0, i);\n      i && c.setFastWeightInitDuringBuild(!0), s.add(c);\n    }\n\n    return s;\n  }, Object.defineProperty(t.prototype, \"stopTraining\", {\n    set: function set(e) {\n      this.model.stopTraining = e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.getConfig = function () {\n    for (var e = [], t = 0, n = this.layers; t < n.length; t++) {\n      var r = n[t];\n      e.push({\n        className: r.getClassName(),\n        config: r.getConfig()\n      });\n    }\n\n    return e;\n  }, t.className = \"Sequential\", t;\n}(Model);\n\nfunction model(e) {\n  return new Model(e);\n}\n\nfunction sequential(e) {\n  return new Sequential(e);\n}\n\nfunction loadModel(e, t) {\n  return void 0 === t && (t = !0), deprecationWarn(\"tf.loadModel() is deprecated and will be removed in TensorFlow.js 1.0. Please switch to tf.loadLayersModel().\"), loadModelInternal(e, {\n    strict: t\n  });\n}\n\nfunction loadLayersModel(e, t) {\n  return null == t && (t = {}), loadModelInternal(e, t);\n}\n\nfunction input(e) {\n  return Input(e);\n}\n\nfunction registerCallbackConstructor(e, t) {\n  CallbackConstructorRegistry.registerCallbackConstructor(e, t);\n}\n\nserialization.registerClass(Sequential);\n\nvar Activation = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.getConfig = function () {\n    return {};\n  }, t;\n}(serialization.Serializable),\n    Elu = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e, t) {\n    return void 0 === t && (t = 1), elu$1(e, t);\n  }, t.className = \"elu\", t;\n}(Activation);\n\nserialization.registerClass(Elu);\n\nvar Selu = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    return selu(e);\n  }, t.className = \"selu\", t;\n}(Activation);\n\nserialization.registerClass(Selu);\n\nvar Relu = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    return relu(e);\n  }, t.className = \"relu\", t;\n}(Activation);\n\nserialization.registerClass(Relu);\n\nvar Relu6 = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    return tidy(function () {\n      return minimum(getScalar(6), relu(e));\n    });\n  }, t.className = \"relu6\", t;\n}(Activation);\n\nserialization.registerClass(Relu6);\n\nvar Linear = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    return e;\n  }, t.className = \"linear\", t;\n}(Activation);\n\nserialization.registerClass(Linear);\n\nvar Sigmoid = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    return sigmoid(e);\n  }, t.className = \"sigmoid\", t;\n}(Activation);\n\nserialization.registerClass(Sigmoid);\n\nvar HardSigmoid = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    return hardSigmoid(e);\n  }, t.className = \"hardSigmoid\", t;\n}(Activation);\n\nserialization.registerClass(HardSigmoid);\n\nvar Softplus = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    return softplus(e);\n  }, t.className = \"softplus\", t;\n}(Activation);\n\nserialization.registerClass(Softplus);\n\nvar Softsign = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    return softsign(e);\n  }, t.className = \"softsign\", t;\n}(Activation);\n\nserialization.registerClass(Softsign);\n\nvar Tanh = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    return tanh(e);\n  }, t.className = \"tanh\", t;\n}(Activation);\n\nserialization.registerClass(Tanh);\n\nvar Softmax = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e, t) {\n    return void 0 === t && (t = -1), softmax(e, t);\n  }, t.className = \"softmax\", t;\n}(Activation);\n\nfunction serializeActivation(e) {\n  return e.getClassName();\n}\n\nfunction deserializeActivation(e, t) {\n  return void 0 === t && (t = {}), deserializeKerasObject(e, serialization.SerializationMap.getMap().classNameMap, t, \"activation\");\n}\n\nfunction getActivation(e) {\n  return null == e ? deserializeActivation({\n    className: \"linear\",\n    config: {}\n  }) : \"string\" == typeof e ? deserializeActivation({\n    className: e,\n    config: {}\n  }) : e instanceof Activation ? e : deserializeActivation(e);\n}\n\nserialization.registerClass(Softmax);\n\nvar Regularizer = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t;\n}(serialization.Serializable),\n    L1L2 = function (e) {\n  function t(t) {\n    var n = e.call(this) || this,\n        r = null == t || null == t.l1 ? .01 : t.l1,\n        i = null == t || null == t.l2 ? .01 : t.l2;\n    return n.hasL1 = 0 !== r, n.hasL2 = 0 !== i, n.l1 = getScalar(r), n.l2 = getScalar(i), n;\n  }\n\n  return __extends(t, e), t.prototype.apply = function (e) {\n    var t = this;\n    return tidy(function () {\n      var n = zeros([1]);\n      return t.hasL1 && (n = add(n, sum(mul(t.l1, abs(e))))), t.hasL2 && (n = add(n, sum(mul(t.l2, square(e))))), n.asScalar();\n    });\n  }, t.prototype.getConfig = function () {\n    return {\n      l1: this.l1.dataSync()[0],\n      l2: this.l2.dataSync()[0]\n    };\n  }, t.fromConfig = function (e, t) {\n    return new e({\n      l1: t.l1,\n      l2: t.l2\n    });\n  }, t.className = \"L1L2\", t;\n}(Regularizer);\n\nfunction l1(e) {\n  return new L1L2({\n    l1: null != e ? e.l1 : null,\n    l2: 0\n  });\n}\n\nfunction l2(e) {\n  return new L1L2({\n    l2: null != e ? e.l2 : null,\n    l1: 0\n  });\n}\n\nserialization.registerClass(L1L2);\nvar REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP = {\n  l1l2: \"L1L2\"\n};\n\nfunction serializeRegularizer(e) {\n  return serializeKerasObject(e);\n}\n\nfunction deserializeRegularizer(e, t) {\n  return void 0 === t && (t = {}), deserializeKerasObject(e, serialization.SerializationMap.getMap().classNameMap, t, \"regularizer\");\n}\n\nfunction getRegularizer(e) {\n  return null == e ? null : \"string\" == typeof e ? deserializeRegularizer({\n    className: e in REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP ? REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP[e] : e,\n    config: {}\n  }) : e instanceof Regularizer ? e : deserializeRegularizer(e);\n}\n\nvar ReLU = function (e) {\n  function t(t) {\n    var n = e.call(this, null == t ? {} : t) || this;\n    return n.supportsMasking = !0, null != t && (n.maxValue = t.maxValue), n;\n  }\n\n  return __extends(t, e), t.prototype.call = function (e, t) {\n    e = getExactlyOneTensor(e);\n    var n = relu(e);\n    return null != this.maxValue && (n = clipByValue(n, 0, this.maxValue)), n;\n  }, t.prototype.computeOutputShape = function (e) {\n    return e;\n  }, t.prototype.getConfig = function () {\n    var t = {\n      maxValue: this.maxValue\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"ReLU\", t;\n}(Layer);\n\nserialization.registerClass(ReLU);\n\nvar LeakyReLU = function (e) {\n  function t(t) {\n    var n = e.call(this, null == t ? {} : t) || this;\n    return n.DEFAULT_ALPHA = .3, null == t && (t = {}), n.alpha = null == t.alpha ? n.DEFAULT_ALPHA : t.alpha, n;\n  }\n\n  return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = getExactlyOneTensor(e);\n    return leakyRelu(n, this.alpha);\n  }, t.prototype.computeOutputShape = function (e) {\n    return e;\n  }, t.prototype.getConfig = function () {\n    var t = {\n      alpha: this.alpha\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"LeakyReLU\", t;\n}(Layer);\n\nserialization.registerClass(LeakyReLU);\n\nvar PReLU = function (e) {\n  function t(t) {\n    var n = e.call(this, null == t ? {} : t) || this;\n    if (n.DEFAULT_ALPHA_INITIALIZER = \"zeros\", null == t && (t = {}), n.supportsMasking = !0, n.alphaInitializer = getInitializer(t.alphaInitializer || n.DEFAULT_ALPHA_INITIALIZER), n.alphaRegularizer = getRegularizer(t.alphaRegularizer), n.alphaConstraint = getConstraint(t.alphaConstraint), null == t.sharedAxes) n.sharedAxes = null;else if (Array.isArray(t.sharedAxes)) n.sharedAxes = t.sharedAxes;else {\n      if (\"number\" != typeof t.sharedAxes) throw new ValueError(\"Expected sharedAxes to be a number or an array of numbers, but got \" + t.sharedAxes);\n      n.sharedAxes = [t.sharedAxes];\n    }\n    return n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    var t = (e = getExactlyOneShape(e)).slice(1);\n    if (null != this.sharedAxes) for (var n = 0, r = this.sharedAxes; n < r.length; n++) {\n      t[(a = r[n]) - 1] = 1;\n    }\n    this.alpha = this.addWeight(\"alpha\", t, \"float32\", this.alphaInitializer, this.alphaRegularizer, !0, this.alphaConstraint);\n    var i = {};\n    if (null != this.sharedAxes) for (var a = 1; a < e.length; ++a) {\n      i[a] = e[a];\n    }\n    this.inputSpec = [new InputSpec({\n      ndim: e.length,\n      axes: i\n    })], this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    return e = getExactlyOneTensor(e), prelu(e, this.alpha.read());\n  }, t.prototype.getConfig = function () {\n    var t = {\n      alphaInitializer: serializeInitializer(this.alphaInitializer),\n      alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n      alphaConstraint: serializeConstraint(this.alphaConstraint),\n      sharedAxes: this.sharedAxes\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"PReLU\", t;\n}(Layer);\n\nserialization.registerClass(PReLU);\n\nvar ELU = function (e) {\n  function t(t) {\n    var n = e.call(this, null == t ? {} : t) || this;\n    if (n.DEFAULT_ALPHA = 1, null == t && (t = {}), null != t.alpha && t.alpha !== n.DEFAULT_ALPHA) throw new NotImplementedError(\"Non-default alpha value (\" + t.alpha + \") is not supported by the ELU layer yet.\");\n    return n.alpha = null == t.alpha ? n.DEFAULT_ALPHA : t.alpha, n;\n  }\n\n  return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = getExactlyOneTensor(e);\n    return elu(n);\n  }, t.prototype.computeOutputShape = function (e) {\n    return e;\n  }, t.prototype.getConfig = function () {\n    var t = {\n      alpha: this.alpha\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"ELU\", t;\n}(Layer);\n\nserialization.registerClass(ELU);\n\nvar ThresholdedReLU = function (e) {\n  function t(t) {\n    var n = e.call(this, null == t ? {} : t) || this;\n    return n.DEFAULT_THETA = 1, null == t && (t = {}), n.theta = null == t.theta ? n.DEFAULT_THETA : t.theta, n.thetaTensor = getScalar(n.theta), n;\n  }\n\n  return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = getExactlyOneTensor(e);\n    return n.mul(cast$1(n.greater(this.thetaTensor), \"float32\"));\n  }, t.prototype.computeOutputShape = function (e) {\n    return e;\n  }, t.prototype.getConfig = function () {\n    var t = {\n      theta: this.theta\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"ThresholdedReLU\", t;\n}(Layer);\n\nserialization.registerClass(ThresholdedReLU);\n\nvar Softmax$1 = function (e) {\n  function t(t) {\n    var n = e.call(this, null == t ? {} : t) || this;\n    return n.DEFAULT_AXIS = 1, null == t && (t = {}), n.softmax = new Softmax().apply, n.axis = null == t.axis ? n.DEFAULT_AXIS : t.axis, n;\n  }\n\n  return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = getExactlyOneTensor(e);\n    return this.softmax(n, this.axis);\n  }, t.prototype.computeOutputShape = function (e) {\n    return e;\n  }, t.prototype.getConfig = function () {\n    var t = {\n      axis: this.axis\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"Softmax\", t;\n}(Layer);\n\nfunction normalizeArray(e, t, n) {\n  if (\"number\" == typeof e) return pyListRepeat(e, t);\n  if (e.length !== t) throw new ValueError(\"The \" + n + \" argument must be a tuple of \" + t + \" integers. Received: \" + e.length + \" elements.\");\n\n  for (var r = 0; r < t; ++r) {\n    var i = e[r];\n    if (!isInteger(i)) throw new ValueError(\"The \" + n + \" argument must be a tuple of \" + t + \" integers. Received: \" + JSON.stringify(e) + \" including a non-integer number \" + i);\n  }\n\n  return e;\n}\n\nfunction convOutputLength(e, t, n, r, i) {\n  return void 0 === i && (i = 1), null == e ? e : (a = \"same\" === n ? e : e - (t + (t - 1) * (i - 1)) + 1, Math.floor((a + r - 1) / r));\n  var a;\n}\n\nfunction deconvLength(e, t, n, r) {\n  if (null == e) return null;\n  if (\"valid\" === r) e = e * t + max$1([n - t, 0]);else {\n    if (\"same\" !== r) throw new ValueError(\"Unsupport padding mode: \" + r + \".\");\n    e *= t;\n  }\n  return e;\n}\n\nfunction preprocessConv2DInput(e, t) {\n  return tidy(function () {\n    return checkDataFormat(t), \"channelsFirst\" === t ? transpose(e, [0, 2, 3, 1]) : e;\n  });\n}\n\nfunction conv1dWithBias(e, t, n, r, i, a, o) {\n  return void 0 === r && (r = 1), void 0 === i && (i = \"valid\"), void 0 === o && (o = 1), tidy(function () {\n    if (null == a && (a = imageDataFormat()), checkDataFormat(a), 3 !== e.shape.length) throw new ValueError(\"The input of a conv1dWithBias operation should be 3, but is \" + e.shape.length + \" instead.\");\n    if (3 !== t.shape.length) throw new ValueError(\"The kernel for a conv1dWithBias operation should be 3, but is \" + t.shape.length + \" instead\");\n    if (null != n && 1 !== n.shape.length) throw new ValueError(\"The bias for a conv1dWithBias operation should be 1, but is \" + t.shape.length + \" instead\");\n    if (\"channelsFirst\" === a && (e = transpose(e, [0, 2, 1])), \"causal\" === i) throw new NotImplementedError(\"The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.\");\n    var s = conv1d(e, t, r, \"same\" === i ? \"same\" : \"valid\", \"NWC\", o);\n    return null != n && (s = biasAdd(s, n)), s;\n  });\n}\n\nfunction conv2dWithBias(e, t, n, r, i, a, o) {\n  return void 0 === r && (r = [1, 1]), void 0 === i && (i = \"valid\"), tidy(function () {\n    if (null == a && (a = imageDataFormat()), checkDataFormat(a), 3 !== e.rank && 4 !== e.rank) throw new ValueError(\"conv2dWithBias expects input to be of rank 3 or 4, but received \" + e.rank + \".\");\n    if (3 !== t.rank && 4 !== t.rank) throw new ValueError(\"conv2dWithBias expects kernel to be of rank 3 or 4, but received \" + e.rank + \".\");\n    var s = preprocessConv2DInput(e, a);\n    if (\"causal\" === i) throw new NotImplementedError(\"The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.\");\n    return s = conv2d(s, t, r, \"same\" === i ? \"same\" : \"valid\", \"NHWC\", o), null != n && (s = biasAdd(s, n)), \"channelsFirst\" === a && (s = transpose(s, [0, 3, 1, 2])), s;\n  });\n}\n\nserialization.registerClass(Softmax$1);\n\nvar BaseConv = function (e) {\n  function t(n, r) {\n    var i = e.call(this, r) || this;\n    if (i.bias = null, i.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", i.DEFAULT_BIAS_INITIALIZER = \"zeros\", t.verifyArgs(r), i.rank = n, 1 !== i.rank && 2 !== i.rank) throw new NotImplementedError(\"Convolution layer for rank other than 1 or 2 (\" + i.rank + \") is not implemented yet.\");\n    if (i.kernelSize = normalizeArray(r.kernelSize, n, \"kernelSize\"), i.strides = normalizeArray(null == r.strides ? 1 : r.strides, n, \"strides\"), i.padding = null == r.padding ? \"valid\" : r.padding, checkPaddingMode(i.padding), i.dataFormat = null == r.dataFormat ? \"channelsLast\" : r.dataFormat, checkDataFormat(i.dataFormat), i.activation = getActivation(r.activation), i.useBias = null == r.useBias || r.useBias, i.biasInitializer = getInitializer(r.biasInitializer || i.DEFAULT_BIAS_INITIALIZER), i.biasConstraint = getConstraint(r.biasConstraint), i.biasRegularizer = getRegularizer(r.biasRegularizer), i.activityRegularizer = getRegularizer(r.activityRegularizer), i.dilationRate = normalizeArray(null == r.dilationRate ? 1 : r.dilationRate, n, \"dilationRate\"), 1 === i.rank && Array.isArray(i.dilationRate) && 1 !== i.dilationRate.length) throw new ValueError(\"dilationRate must be a number or an array of a single number for 1D convolution, but received \" + JSON.stringify(i.dilationRate));\n    if (2 === i.rank) if (\"number\" == typeof i.dilationRate) i.dilationRate = [i.dilationRate, i.dilationRate];else if (2 !== i.dilationRate.length) throw new ValueError(\"dilationRate must be a number or array of two numbers for 2D convolution, but received \" + JSON.stringify(i.dilationRate));\n    return i;\n  }\n\n  return __extends(t, e), t.verifyArgs = function (e) {\n    if (assert(\"kernelSize\" in e, \"required key 'kernelSize' not in config\"), \"number\" != typeof e.kernelSize && !checkArrayTypeAndLength(e.kernelSize, \"number\", 1, 2)) throw new ValueError(\"BaseConv expects config.kernelSize to be number or number[] with length 1 or 2, but received \" + JSON.stringify(e.kernelSize) + \".\");\n  }, t.prototype.getConfig = function () {\n    var t = {\n      kernelSize: this.kernelSize,\n      strides: this.strides,\n      padding: this.padding,\n      dataFormat: this.dataFormat,\n      dilationRate: this.dilationRate,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      biasConstraint: serializeConstraint(this.biasConstraint)\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t;\n}(Layer),\n    Conv = function (e) {\n  function t(n, r) {\n    var i = e.call(this, n, r) || this;\n    return i.kernel = null, t.verifyArgs(r), i.filters = r.filters, i.kernelInitializer = getInitializer(r.kernelInitializer || i.DEFAULT_KERNEL_INITIALIZER), i.kernelConstraint = getConstraint(r.kernelConstraint), i.kernelRegularizer = getRegularizer(r.kernelRegularizer), i;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    e = getExactlyOneShape(e);\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[t]) throw new ValueError(\"The channel dimension of the input should be defined. Found \" + e[t]);\n    var n,\n        r = e[t],\n        i = this.kernelSize.concat([r, this.filters]);\n    this.kernel = this.addWeight(\"kernel\", i, null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.filters], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint)), this.inputSpec = [{\n      ndim: this.rank + 2,\n      axes: (n = {}, n[t] = r, n)\n    }], this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      var t;\n      e = getExactlyOneTensor(e);\n      var r = null == n.bias ? null : n.bias.read();\n      if (1 === n.rank) t = conv1dWithBias(e, n.kernel.read(), r, n.strides[0], n.padding, n.dataFormat, n.dilationRate[0]);else if (2 === n.rank) t = conv2dWithBias(e, n.kernel.read(), r, n.strides, n.padding, n.dataFormat, n.dilationRate);else if (3 === n.rank) throw new NotImplementedError(\"3D convolution is not implemented yet.\");\n      return null != n.activation && (t = n.activation.apply(t)), t;\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    e = getExactlyOneShape(e);\n\n    for (var t = [], n = \"channelsLast\" === this.dataFormat ? e.slice(1, e.length - 1) : e.slice(2), r = 0; r < n.length; ++r) {\n      var i = convOutputLength(n[r], this.kernelSize[r], this.padding, this.strides[r], \"number\" == typeof this.dilationRate ? this.dilationRate : this.dilationRate[r]);\n      t.push(i);\n    }\n\n    var a = [e[0]];\n    return \"channelsLast\" === this.dataFormat ? (a = a.concat(t)).push(this.filters) : (a.push(this.filters), a = a.concat(t)), a;\n  }, t.prototype.getConfig = function () {\n    var t = {\n      filters: this.filters,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint)\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.verifyArgs = function (e) {\n    if (!(\"filters\" in e) || \"number\" != typeof e.filters || e.filters < 1) throw new ValueError(\"Convolution layer expected config.filters to be a 'number' > 0 but got \" + JSON.stringify(e.filters));\n  }, t;\n}(BaseConv),\n    Conv2D = function (e) {\n  function t(n) {\n    var r = e.call(this, 2, n) || this;\n    return t.verifyArgs(n), r;\n  }\n\n  return __extends(t, e), t.prototype.getConfig = function () {\n    var t = e.prototype.getConfig.call(this);\n    return delete t.rank, t;\n  }, t.verifyArgs = function (e) {\n    if (\"number\" != typeof e.kernelSize && !checkArrayTypeAndLength(e.kernelSize, \"number\", 1, 2)) throw new ValueError(\"Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received \" + JSON.stringify(e.kernelSize) + \".\");\n  }, t.className = \"Conv2D\", t;\n}(Conv);\n\nserialization.registerClass(Conv2D);\n\nvar Conv2DTranspose = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    if (n.inputSpec = [new InputSpec({\n      ndim: 4\n    })], \"same\" !== n.padding && \"valid\" !== n.padding) throw new ValueError(\"Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode \" + n.padding);\n    return n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    if (4 !== (e = getExactlyOneShape(e)).length) throw new ValueError(\"Input should have rank 4; Received input shape: \" + JSON.stringify(e));\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[t]) throw new ValueError(\"The channel dimension of the inputs should be defined. Found `None`.\");\n    var n,\n        r = e[t],\n        i = this.kernelSize.concat([this.filters, r]);\n    this.kernel = this.addWeight(\"kernel\", i, \"float32\", this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.filters], \"float32\", this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint)), this.inputSpec = [new InputSpec({\n      ndim: 4,\n      axes: (n = {}, n[t] = r, n)\n    })], this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      var t = getExactlyOneTensor(e);\n      if (4 !== t.shape.length) throw new ValueError(\"Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-\" + t.shape.length);\n      var r,\n          i,\n          a = t.shape,\n          o = a[0];\n      \"channelsFirst\" === n.dataFormat ? (r = 2, i = 3) : (r = 1, i = 2);\n      var s = a[r],\n          l = a[i],\n          u = n.kernelSize[0],\n          c = n.kernelSize[1],\n          p = n.strides[0],\n          h = n.strides[1],\n          d = [o, deconvLength(s, p, u, n.padding), deconvLength(l, h, c, n.padding), n.filters];\n      \"channelsLast\" !== n.dataFormat && (t = transpose(t, [0, 2, 3, 1]));\n      var f = conv2dTranspose(t, n.kernel.read(), d, n.strides, n.padding);\n      return \"channelsLast\" !== n.dataFormat && (f = transpose(f, [0, 3, 1, 2])), null != n.bias && (f = biasAdd(f, n.bias.read(), n.dataFormat)), null != n.activation && (f = n.activation.apply(f)), f;\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    var t,\n        n,\n        r,\n        i = (e = getExactlyOneShape(e)).slice();\n    \"channelsFirst\" === this.dataFormat ? (t = 1, n = 2, r = 3) : (t = 3, n = 1, r = 2);\n    var a = this.kernelSize[0],\n        o = this.kernelSize[1],\n        s = this.strides[0],\n        l = this.strides[1];\n    return i[t] = this.filters, i[n] = deconvLength(i[n], s, a, this.padding), i[r] = deconvLength(i[r], l, o, this.padding), i;\n  }, t.prototype.getConfig = function () {\n    var t = e.prototype.getConfig.call(this);\n    return delete t.dilationRate, t;\n  }, t.className = \"Conv2DTranspose\", t;\n}(Conv2D);\n\nserialization.registerClass(Conv2DTranspose);\n\nvar SeparableConv = function (e) {\n  function t(t, n) {\n    var r = e.call(this, t, n) || this;\n    if (r.DEFAULT_DEPTHWISE_INITIALIZER = \"glorotUniform\", r.DEFAULT_POINTWISE_INITIALIZER = \"glorotUniform\", r.depthwiseKernel = null, r.pointwiseKernel = null, null == n.filters) throw new ValueError(\"The `filters` configuration field is required by SeparableConv, but is unspecified.\");\n    if (null != n.kernelInitializer || null != n.kernelRegularizer || null != n.kernelConstraint) throw new ValueError(\"Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.\");\n    if (null != n.padding && \"same\" !== n.padding && \"valid\" !== n.padding) throw new ValueError(\"SeparableConv\" + r.rank + \"D supports only padding modes: 'same' and 'valid', but received \" + JSON.stringify(n.padding));\n    return r.depthMultiplier = null == n.depthMultiplier ? 1 : n.depthMultiplier, r.depthwiseInitializer = getInitializer(n.depthwiseInitializer || r.DEFAULT_DEPTHWISE_INITIALIZER), r.depthwiseRegularizer = getRegularizer(n.depthwiseRegularizer), r.depthwiseConstraint = getConstraint(n.depthwiseConstraint), r.pointwiseInitializer = getInitializer(n.depthwiseInitializer || r.DEFAULT_POINTWISE_INITIALIZER), r.pointwiseRegularizer = getRegularizer(n.pointwiseRegularizer), r.pointwiseConstraint = getConstraint(n.pointwiseConstraint), r;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    if ((e = getExactlyOneShape(e)).length < this.rank + 2) throw new ValueError(\"Inputs to SeparableConv\" + this.rank + \"D should have rank \" + (this.rank + 2) + \", but received input shape: \" + JSON.stringify(e));\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[t] || e[t] < 0) throw new ValueError(\"The channel dimension of the inputs should be defined, but found \" + JSON.stringify(e[t]));\n\n    for (var n = e[t], r = this.kernelSize.concat([n, this.depthMultiplier]), i = [], a = 0; a < this.rank; ++a) {\n      i.push(1);\n    }\n\n    i.push(n * this.depthMultiplier, this.filters);\n    var o;\n    this.depthwiseKernel = this.addWeight(\"depthwise_kernel\", r, \"float32\", this.depthwiseInitializer, this.depthwiseRegularizer, !0, this.depthwiseConstraint), this.pointwiseKernel = this.addWeight(\"pointwise_kernel\", i, \"float32\", this.pointwiseInitializer, this.pointwiseRegularizer, !0, this.pointwiseConstraint), this.useBias ? this.bias = this.addWeight(\"bias\", [this.filters], \"float32\", this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : this.bias = null, this.inputSpec = [new InputSpec({\n      ndim: this.rank + 2,\n      axes: (o = {}, o[t] = n, o)\n    })], this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      var t;\n      if (e = getExactlyOneTensor(e), 1 === n.rank) throw new NotImplementedError(\"1D separable convolution is not implemented yet.\");\n      return 2 === n.rank && (\"channelsFirst\" === n.dataFormat && (e = transpose(e, [0, 2, 3, 1])), t = separableConv2d(e, n.depthwiseKernel.read(), n.pointwiseKernel.read(), n.strides, n.padding, n.dilationRate, \"NHWC\")), n.useBias && (t = biasAdd(t, n.bias.read(), n.dataFormat)), null != n.activation && (t = n.activation.apply(t)), \"channelsFirst\" === n.dataFormat && (t = transpose(t, [0, 3, 1, 2])), t;\n    });\n  }, t.prototype.getConfig = function () {\n    var t = e.prototype.getConfig.call(this);\n    return delete t.rank, delete t.kernelInitializer, delete t.kernelRegularizer, delete t.kernelConstraint, t.depthwiseInitializer = serializeInitializer(this.depthwiseInitializer), t.pointwiseInitializer = serializeInitializer(this.pointwiseInitializer), t.depthwiseRegularizer = serializeRegularizer(this.depthwiseRegularizer), t.pointwiseRegularizer = serializeRegularizer(this.pointwiseRegularizer), t.depthwiseConstraint = serializeConstraint(this.depthwiseConstraint), t.pointwiseConstraint = serializeConstraint(this.pointwiseConstraint), t;\n  }, t.className = \"SeparableConv\", t;\n}(Conv),\n    SeparableConv2D = function (e) {\n  function t(t) {\n    return e.call(this, 2, t) || this;\n  }\n\n  return __extends(t, e), t.className = \"SeparableConv2D\", t;\n}(SeparableConv);\n\nserialization.registerClass(SeparableConv2D);\n\nvar Conv1D = function (e) {\n  function t(n) {\n    var r = e.call(this, 1, n) || this;\n    return t.verifyArgs(n), r.inputSpec = [{\n      ndim: 3\n    }], r;\n  }\n\n  return __extends(t, e), t.prototype.getConfig = function () {\n    var t = e.prototype.getConfig.call(this);\n    return delete t.rank, delete t.dataFormat, t;\n  }, t.verifyArgs = function (e) {\n    if (\"number\" != typeof e.kernelSize && !checkArrayTypeAndLength(e.kernelSize, \"number\", 1, 1)) throw new ValueError(\"Conv1D expects config.kernelSize to be number or number[] with length 1, but received \" + JSON.stringify(e.kernelSize) + \".\");\n  }, t.className = \"Conv1D\", t;\n}(Conv);\n\nserialization.registerClass(Conv1D);\n\nvar Cropping2D = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return \"number\" == typeof t.cropping ? n.cropping = [[t.cropping, t.cropping], [t.cropping, t.cropping]] : \"number\" == typeof t.cropping[0] ? n.cropping = [[t.cropping[0], t.cropping[0]], [t.cropping[1], t.cropping[1]]] : n.cropping = t.cropping, n.dataFormat = void 0 === t.dataFormat ? \"channelsLast\" : t.dataFormat, n.inputSpec = [{\n      ndim: 4\n    }], n;\n  }\n\n  return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    return \"channelsFirst\" === this.dataFormat ? [e[0], e[1], e[2] - this.cropping[0][0] - this.cropping[0][1], e[3] - this.cropping[1][0] - this.cropping[1][1]] : [e[0], e[1] - this.cropping[0][0] - this.cropping[0][1], e[2] - this.cropping[1][0] - this.cropping[1][1], e[3]];\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      return e = getExactlyOneTensor(e), \"channelsLast\" === n.dataFormat ? sliceAlongAxis(sliceAlongAxis(e, n.cropping[0][0], e.shape[1] - n.cropping[0][0] - n.cropping[0][1], 2), n.cropping[1][0], e.shape[2] - n.cropping[1][1] - n.cropping[1][0], 3) : sliceAlongAxis(sliceAlongAxis(e, n.cropping[0][0], e.shape[2] - n.cropping[0][0] - n.cropping[0][1], 3), n.cropping[1][0], e.shape[3] - n.cropping[1][1] - n.cropping[1][0], 4);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      cropping: this.cropping,\n      dataFormat: this.dataFormat\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"Cropping2D\", t;\n}(Layer);\n\nserialization.registerClass(Cropping2D);\n\nvar UpSampling2D = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.DEFAULT_SIZE = [2, 2], n.inputSpec = [{\n      ndim: 4\n    }], n.size = null == t.size ? n.DEFAULT_SIZE : t.size, n.dataFormat = null == t.dataFormat ? \"channelsLast\" : t.dataFormat, n;\n  }\n\n  return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    if (\"channelsFirst\" === this.dataFormat) {\n      var t = null == e[2] ? null : this.size[0] * e[2],\n          n = null == e[3] ? null : this.size[1] * e[3];\n      return [e[0], e[1], t, n];\n    }\n\n    t = null == e[1] ? null : this.size[0] * e[1], n = null == e[2] ? null : this.size[1] * e[2];\n    return [e[0], t, n, e[3]];\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      var t = getExactlyOneTensor(e),\n          r = t.shape;\n\n      if (\"channelsFirst\" === n.dataFormat) {\n        t = transpose(t, [0, 2, 3, 1]);\n        var i = n.size[0] * r[2],\n            a = n.size[1] * r[3],\n            o = t.resizeNearestNeighbor([i, a]);\n        return transpose(o, [0, 3, 1, 2]);\n      }\n\n      i = n.size[0] * r[1], a = n.size[1] * r[2];\n      return t.resizeNearestNeighbor([i, a]);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      size: this.size,\n      dataFormat: this.dataFormat\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"UpSampling2D\", t;\n}(Layer);\n\nfunction depthwiseConv2d$1(e, t, n, r, i, a) {\n  return void 0 === n && (n = [1, 1]), void 0 === r && (r = \"valid\"), tidy(function () {\n    null == i && (i = imageDataFormat()), checkDataFormat(i);\n    var o = preprocessConv2DInput(e, i);\n    if (4 !== e.rank) throw new ValueError(\"Input for depthwiseConv2d is required to be 4-D, but is instead \" + e.rank + \"-D\");\n    if (4 !== t.rank) throw new ValueError(\"depthwiseKernel is required to be 4-D, but is instead \" + t.rank + \"-D\");\n    return o = depthwiseConv2d(o, t, n, \"same\" === r ? \"same\" : \"valid\", \"NHWC\", a), \"channelsFirst\" === i && (o = transpose(o, [0, 3, 1, 2])), o;\n  });\n}\n\nserialization.registerClass(UpSampling2D);\n\nvar DepthwiseConv2D = function (e) {\n  function t(t) {\n    var n = e.call(this, 2, t) || this;\n    return n.depthwiseKernel = null, n.depthMultiplier = null == t.depthMultiplier ? 1 : t.depthMultiplier, n.depthwiseInitializer = getInitializer(t.depthwiseInitializer || n.DEFAULT_KERNEL_INITIALIZER), n.depthwiseConstraint = getConstraint(t.depthwiseConstraint), n.depthwiseRegularizer = getRegularizer(t.depthwiseRegularizer), n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    if ((e = getExactlyOneShape(e)).length < 4) throw new ValueError(\"Inputs to DepthwiseConv2D should have rank 4. Received input shape: \" + JSON.stringify(e) + \".\");\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : 3;\n    if (null == e[t] || e[t] < 0) throw new ValueError(\"The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (\" + e[t] + \").\");\n    var n = e[t],\n        r = [this.kernelSize[0], this.kernelSize[1], n, this.depthMultiplier];\n    this.depthwiseKernel = this.addWeight(\"depthwise_kernel\", r, null, this.depthwiseInitializer, this.depthwiseRegularizer, !0, this.depthwiseConstraint), this.useBias ? this.bias = this.addWeight(\"bias\", [n * this.depthMultiplier], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : this.bias = null, this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      var t = depthwiseConv2d$1(e = getExactlyOneTensor(e), n.depthwiseKernel.read(), n.strides, n.padding, n.dataFormat, null);\n      return n.useBias && (t = biasAdd(t, n.bias.read(), n.dataFormat)), null != n.activation && (t = n.activation.apply(t)), t;\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    e = getExactlyOneShape(e);\n    var t = \"channelsFirst\" === this.dataFormat ? e[2] : e[1],\n        n = \"channelsFirst\" === this.dataFormat ? e[3] : e[2],\n        r = \"channelsFirst\" === this.dataFormat ? e[1] * this.depthMultiplier : e[3] * this.depthMultiplier,\n        i = convOutputLength(t, this.kernelSize[0], this.padding, this.strides[0]),\n        a = convOutputLength(n, this.kernelSize[1], this.padding, this.strides[1]);\n    return \"channelsFirst\" === this.dataFormat ? [e[0], r, i, a] : [e[0], i, a, r];\n  }, t.prototype.getConfig = function () {\n    var t = e.prototype.getConfig.call(this);\n    return t.depthMultiplier = this.depthMultiplier, t.depthwiseInitializer = serializeInitializer(this.depthwiseInitializer), t.depthwiseRegularizer = serializeRegularizer(this.depthwiseRegularizer), t.depthwiseConstraint = serializeConstraint(this.depthwiseRegularizer), t;\n  }, t.className = \"DepthwiseConv2D\", t;\n}(BaseConv);\n\nserialization.registerClass(DepthwiseConv2D);\n\nvar Dropout = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    if (n.rate = Math.max(Math.min(t.rate, 1), 0), n.rateScalar = getScalar(n.rate), n.noiseShape = t.noiseShape, n.seed = t.seed, null != n.seed) throw new NotImplementedError(\"Non-default seed is not implemented in Dropout layer yet: \" + n.seed);\n    return n.supportsMasking = !0, n;\n  }\n\n  return __extends(t, e), t.prototype.getNoiseShape = function (e) {\n    if (null == this.noiseShape) return this.noiseShape;\n\n    for (var t = e.shape, n = [], r = 0; r < this.noiseShape.length; ++r) {\n      n.push(null == this.noiseShape[r] ? t[r] : this.noiseShape[r]);\n    }\n\n    return n;\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      n.invokeCallHook(e, t);\n      var r = getExactlyOneTensor(e);\n      if (null != n.noiseShape && !util.arraysEqual(r.shape, n.noiseShape)) throw new NotImplementedError(\"Non-default noise shape is not implemented in Dropout layer yet: \" + JSON.stringify(n.noiseShape));\n\n      if (0 < n.rate && n.rate < 1) {\n        var i = null != t.training && t.training,\n            a = n.getNoiseShape(r);\n        return inTrainPhase(function () {\n          return dropout(r, n.rateScalar, a, n.seed);\n        }, function () {\n          return r;\n        }, i);\n      }\n\n      return e;\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      rate: this.rate,\n      noiseShape: this.noiseShape,\n      seed: this.seed\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.prototype.dispose = function () {\n    var t = e.prototype.dispose.call(this);\n    return this.rateScalar.isDisposed || (this.rateScalar.dispose(), t.numDisposedVariables++), t;\n  }, t.className = \"Dropout\", t;\n}(Layer);\n\nserialization.registerClass(Dropout);\n\nvar Dense = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n\n    if (n.activation = null, n.useBias = !0, n.kernel = null, n.bias = null, n.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", n.DEFAULT_BIAS_INITIALIZER = \"zeros\", null == t.batchInputShape && null == t.inputShape && null != t.inputDim) {\n      var r = null;\n      null != t.batchSize && (r = t.batchSize), n.batchInputShape = [r, t.inputDim];\n    }\n\n    return n.units = t.units, n.activation = getActivation(t.activation), null != t.useBias && (n.useBias = t.useBias), n.kernelInitializer = getInitializer(t.kernelInitializer || n.DEFAULT_KERNEL_INITIALIZER), n.biasInitializer = getInitializer(t.biasInitializer || n.DEFAULT_BIAS_INITIALIZER), n.kernelConstraint = getConstraint(t.kernelConstraint), n.biasConstraint = getConstraint(t.biasConstraint), n.kernelRegularizer = getRegularizer(t.kernelRegularizer), n.biasRegularizer = getRegularizer(t.biasRegularizer), n.activityRegularizer = getRegularizer(t.activityRegularizer), n.supportsMasking = !0, n.inputSpec = [{\n      minNDim: 2\n    }], n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    var t,\n        n = (e = getExactlyOneShape(e))[e.length - 1];\n    null == this.kernel && (this.kernel = this.addWeight(\"kernel\", [n, this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.units], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint))), this.inputSpec = [{\n      minNDim: 2,\n      axes: (t = {}, t[-1] = n, t)\n    }], this.built = !0;\n  }, t.prototype.computeOutputShape = function (e) {\n    var t = (e = getExactlyOneShape(e)).slice();\n    return t[t.length - 1] = this.units, t;\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      n.invokeCallHook(e, t);\n      var r = dot(getExactlyOneTensor(e), n.kernel.read());\n      return null != n.bias && (r = biasAdd(r, n.bias.read())), null != n.activation && (r = n.activation.apply(r)), r;\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint)\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"Dense\", t;\n}(Layer);\n\nserialization.registerClass(Dense);\n\nvar Flatten = function (e) {\n  function t(t) {\n    var n = e.call(this, t || {}) || this;\n    return n.inputSpec = [{\n      minNDim: 3\n    }], n;\n  }\n\n  return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    for (var t = 0, n = (e = getExactlyOneShape(e)).slice(1); t < n.length; t++) {\n      if (null == n[t]) throw new ValueError('The shape of the input to \"Flatten\" is not fully defined (got ' + e.slice(1) + '). Make sure to pass a complete \"input_shape\" or \"batch_input_shape\" argument to the first layer in your model.');\n    }\n\n    return [e[0], arrayProd(e, 1)];\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      return n.invokeCallHook(e, t), batchFlatten(getExactlyOneTensor(e));\n    });\n  }, t.className = \"Flatten\", t;\n}(Layer);\n\nserialization.registerClass(Flatten);\n\nvar Activation$1 = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.supportsMasking = !0, n.activation = getActivation(t.activation), n;\n  }\n\n  return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      n.invokeCallHook(e, t);\n      var r = getExactlyOneTensor(e);\n      return n.activation.apply(r);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      activation: serializeActivation(this.activation)\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"Activation\", t;\n}(Layer);\n\nserialization.registerClass(Activation$1);\n\nvar RepeatVector = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.n = t.n, n.inputSpec = [{\n      ndim: 2\n    }], n;\n  }\n\n  return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    return [e[0], this.n, e[1]];\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      return repeat(e = getExactlyOneTensor(e), n.n);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      n: this.n\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"RepeatVector\", t;\n}(Layer);\n\nserialization.registerClass(RepeatVector);\n\nvar Reshape = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    n.targetShape = t.targetShape;\n\n    for (var r = 0; r < n.targetShape.length; ++r) {\n      n.isUnknown(n.targetShape[r]) && (n.targetShape[r] = null);\n    }\n\n    return n;\n  }\n\n  return __extends(t, e), t.prototype.isUnknown = function (e) {\n    return e < 0 || null == e;\n  }, t.prototype.fixUnknownDimension = function (e, t) {\n    for (var n = \"Total size of new array must be unchanged.\", r = t.slice(), i = 1, a = null, o = 0; o < r.length; ++o) {\n      var s = r[o];\n\n      if (this.isUnknown(s)) {\n        if (null !== a) throw new ValueError(\"Can only specifiy one unknown dimension.\");\n        a = o;\n      } else i *= s;\n    }\n\n    var l = arrayProd(e);\n\n    if (null !== a) {\n      if (0 === i || l % i != 0) throw new ValueError(n);\n      r[a] = l / i;\n    } else if (l !== i) throw new ValueError(n);\n\n    return r;\n  }, t.prototype.computeOutputShape = function (e) {\n    for (var t = !1, n = 0; n < e.length; ++n) {\n      if (this.isUnknown(e[n])) {\n        t = !0;\n        break;\n      }\n    }\n\n    return t ? e.slice(0, 1).concat(this.targetShape) : e.slice(0, 1).concat(this.fixUnknownDimension(e.slice(1), this.targetShape));\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      n.invokeCallHook(e, t);\n      var r = getExactlyOneTensor(e),\n          i = r.shape,\n          a = i.slice(0, 1).concat(n.fixUnknownDimension(i.slice(1), n.targetShape));\n      return r.reshape(a);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      targetShape: this.targetShape\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"Reshape\", t;\n}(Layer);\n\nserialization.registerClass(Reshape);\n\nvar Permute = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    if (null == t.dims) throw new Error(\"Required configuration field `dims` is missing during Permute constructor call.\");\n    if (!Array.isArray(t.dims)) throw new Error(\"Permute constructor requires `dims` to be an Array, but received \" + t.dims + \" instead.\");\n    var r = range(1, t.dims.length + 1);\n    if (!util.arraysEqual(t.dims.slice().sort(), r)) throw new Error(\"Invalid permutation `dims`: \" + JSON.stringify(t.dims) + \" `dims` must contain consecutive integers starting from 1.\");\n    return n.dims = t.dims, n.dimsIncludingBatch = [0].concat(n.dims), n.inputSpec = [new InputSpec({\n      ndim: n.dims.length + 1\n    })], n;\n  }\n\n  return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    var t = (e = getExactlyOneShape(e)).slice();\n    return this.dims.forEach(function (n, r) {\n      t[r + 1] = e[n];\n    }), t;\n  }, t.prototype.call = function (e, t) {\n    return transpose(getExactlyOneTensor(e), this.dimsIncludingBatch);\n  }, t.prototype.getConfig = function () {\n    var t = {\n      dims: this.dims\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"Permute\", t;\n}(Layer);\n\nserialization.registerClass(Permute);\n\nvar Embedding = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n\n    if (n.embeddings = null, n.DEFAULT_EMBEDDINGS_INITIALIZER = \"randomUniform\", null == t.batchInputShape && null == t.inputShape) {\n      var r = null;\n      null != t.batchSize && (r = t.batchSize), null == t.inputLength ? n.batchInputShape = [r, null] : n.batchInputShape = [r].concat(toList(t.inputLength));\n    }\n\n    return n.inputDim = t.inputDim, n.outputDim = t.outputDim, n.embeddingsInitializer = getInitializer(t.embeddingsInitializer || n.DEFAULT_EMBEDDINGS_INITIALIZER), n.embeddingsRegularizer = getRegularizer(t.embeddingsRegularizer), n.activityRegularizer = getRegularizer(t.activityRegularizer), n.embeddingsConstraint = getConstraint(t.embeddingsConstraint), n.maskZero = t.maskZero, n.supportsMasking = t.maskZero, n.inputLength = t.inputLength, n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    this.embeddings = this.addWeight(\"embeddings\", [this.inputDim, this.outputDim], this.dtype, this.embeddingsInitializer, this.embeddingsRegularizer, !0, this.embeddingsConstraint), this.built = !0;\n  }, t.prototype.warnOnIncompatibleInputShape = function (e) {}, t.prototype.computeMask = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      return n.maskZero ? (e = getExactlyOneTensor(e), notEqual(e, zerosLike(e))) : null;\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    if (e = getExactlyOneShape(e), null == this.inputLength) return e.concat([this.outputDim]);\n    var t = toList(this.inputLength);\n    if (t.length !== e.length - 1) throw new ValueError('\"inputLength\" is ' + this.inputLength + \", but received input shape has shape \" + e);\n\n    for (var n = 0, r = 0; r < t.length; ++r) {\n      var i = t[r],\n          a = e[r + 1];\n      if (null != i && null != a && i !== a) throw new ValueError('\"inputLength\" is ' + this.inputLength + \", but received input shape has shape \" + e);\n      null == i && (t[n] = a), n++;\n    }\n\n    return [e[0]].concat(t, [this.outputDim]);\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      n.invokeCallHook(e, t);\n      var r = getExactlyOneTensor(e);\n      return \"int32\" !== r.dtype && (r = cast$1(r, \"int32\")), gather$1(n.embeddings.read(), r.as1D()).reshape(getExactlyOneShape(n.computeOutputShape(r.shape)));\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      inputDim: this.inputDim,\n      outputDim: this.outputDim,\n      embeddingsInitializer: serializeInitializer(this.embeddingsInitializer),\n      embeddingsRegularizer: serializeRegularizer(this.embeddingsRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      embeddingsConstraint: serializeConstraint(this.embeddingsConstraint),\n      maskZero: this.maskZero,\n      inputLength: this.inputLength\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"Embedding\", t;\n}(Layer);\n\nserialization.registerClass(Embedding);\n\nvar Merge = function (e) {\n  function t(t) {\n    var n = e.call(this, t || {}) || this;\n    return n.supportsMasking = !0, n;\n  }\n\n  return __extends(t, e), t.prototype.mergeFunction = function (e) {\n    throw new NotImplementedError();\n  }, t.prototype.computeElementwiseOpOutputShape = function (e, t) {\n    if (null == e || null == t) return null;\n    if (e.length < t.length) return this.computeElementwiseOpOutputShape(t, e);\n    if (0 === t.length) return e;\n\n    for (var n = e.slice(0, e.length - t.length), r = 0; r < t.length; ++r) {\n      var i = e[e.length - t.length + r],\n          a = t[r];\n      if (null == i || null == a || i < 0 || a < 0) n.push(null);else if (1 === i) n.push(a);else if (1 === a) n.push(i);else {\n        if (i !== a) throw new ValueError(\"Operands could not be broadcast together with shapes \" + JSON.stringify(e) + \" \" + JSON.stringify(t));\n        n.push(i);\n      }\n    }\n\n    return n;\n  }, t.prototype.build = function (e) {\n    if (Array.isArray(e) && !Array.isArray(e[0]) && (e = [getExactlyOneShape(e)]), (e = e).length < 2) throw new ValueError(\"A merge layer should be called on an Array of at least 2 inputs. Got \" + e.length + \" input(s).\");\n\n    for (var t = [], n = 0, r = e; n < r.length; n++) {\n      null != (o = r[n]) && null !== o[0] && t.push(o[0]);\n    }\n\n    if ((t = unique(t)).length > 1) throw new ValueError(\"Can not merge tensors with different batch sizes. Got tensors with shapes: \" + JSON.stringify(e) + \".\");\n\n    for (var i = null == e[0] ? null : e[0].slice(1), a = 1; a < e.length; ++a) {\n      var o = null == e[a] ? null : e[a].slice(1);\n      i = this.computeElementwiseOpOutputShape(i, o);\n    }\n\n    var s = e.map(function (e) {\n      return e.length;\n    });\n    -1 === e.indexOf(null) && 1 === unique(s).length ? this.reshapeRequired = !1 : this.reshapeRequired = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      if (e = e, n.reshapeRequired) {\n        var t = [],\n            r = e.map(function (e) {\n          return e.rank;\n        });\n\n        if (-1 === r.indexOf(null)) {\n          for (var i = max$1(r), a = 0, o = e; a < o.length; a++) {\n            for (var s = (h = o[a]).rank, l = 0; l < i - s; ++l) {\n              h = expandDims$1(h, 1);\n            }\n\n            t.push(h);\n          }\n\n          return n.mergeFunction(t);\n        }\n\n        for (var u = !1, c = 0, p = e; c < p.length; c++) {\n          var h;\n\n          if (null == (s = (h = p[c]).rank)) {\n            var d = h.shape,\n                f = d[0],\n                g = d.slice(1).concat([f]),\n                m = h.reshape([f].concat(arrayProd(d.slice(1))));\n            m = (m = transpose(m, [1, 0])).reshape(g), t.push(m), u = !0;\n          } else if (s > 1) {\n            var y = range(1, s).concat([0]);\n            t.push(transpose(h, y)), u = !0;\n          } else t.push(h);\n        }\n\n        var v = n.mergeFunction(t),\n            b = v.rank;\n        if (u) if (null == b) {\n          var w = v.shape;\n          g = [f = w[w.length - 1]].concat(w.slice(0, w.length - 1));\n          v = transpose(v.reshape([-1, f]), [1, 0]).reshape(g);\n        } else if (b > 1) {\n          y = [b - 1].concat(range(0, b - 1));\n          v = transpose(v, y);\n        }\n        return v;\n      }\n\n      return n.mergeFunction(e);\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    var t;\n    t = null == (e = e)[0] ? null : e[0].slice(1);\n\n    for (var n = 1; n < e.length; ++n) {\n      var r = null == e[n] ? null : e[n].slice(1);\n      t = this.computeElementwiseOpOutputShape(t, r);\n    }\n\n    for (var i = [], a = 0, o = e; a < o.length; a++) {\n      null != (r = o[a]) && null !== r[0] && i.push(r[0]);\n    }\n\n    return t = 1 === (i = unique(i)).length ? i.concat(t) : [null].concat(t);\n  }, t.prototype.computeMask = function (e, t) {\n    return tidy(function () {\n      if (null == t) return null;\n      if (!Array.isArray(t)) throw new ValueError(\"`mask` should be an Array\");\n      if (!Array.isArray(e)) throw new ValueError(\"`inputs` should be an Array\");\n      if (t.length !== e.length) throw new ValueError(\"The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (\" + e.length + \" vs \" + t.length + \")\");\n      if (t.every(function (e) {\n        return null == e;\n      })) return null;\n\n      for (var n = (t = t.map(function (e) {\n        return null == e ? e : expandDims(e, 0);\n      }))[0], r = 1; r < t.length - 1; ++r) {\n        n = logicalAnd(n, t[r]);\n      }\n\n      return n;\n    });\n  }, t;\n}(Layer),\n    Add = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }\n\n  return __extends(t, e), t.prototype.mergeFunction = function (e) {\n    return tidy(function () {\n      for (var t = e[0].clone(), n = 1; n < e.length; ++n) {\n        t = add(t, e[n]);\n      }\n\n      return t;\n    });\n  }, t.className = \"Add\", t;\n}(Merge);\n\nserialization.registerClass(Add);\n\nvar Multiply = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }\n\n  return __extends(t, e), t.prototype.mergeFunction = function (e) {\n    return tidy(function () {\n      for (var t = e[0].clone(), n = 1; n < e.length; ++n) {\n        t = mul(t, e[n]);\n      }\n\n      return t;\n    });\n  }, t.className = \"Multiply\", t;\n}(Merge);\n\nserialization.registerClass(Multiply);\n\nvar Average = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }\n\n  return __extends(t, e), t.prototype.mergeFunction = function (e) {\n    return tidy(function () {\n      for (var t = e[0].clone(), n = 1; n < e.length; ++n) {\n        t = add(t, e[n]);\n      }\n\n      return mul(getScalar(1 / e.length), t);\n    });\n  }, t.className = \"Average\", t;\n}(Merge);\n\nserialization.registerClass(Average);\n\nvar Maximum = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }\n\n  return __extends(t, e), t.prototype.mergeFunction = function (e) {\n    return tidy(function () {\n      for (var t = e[0], n = 1; n < e.length; ++n) {\n        t = maximum(t, e[n]);\n      }\n\n      return t;\n    });\n  }, t.className = \"Maximum\", t;\n}(Merge);\n\nserialization.registerClass(Maximum);\n\nvar Minimum = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }\n\n  return __extends(t, e), t.prototype.mergeFunction = function (e) {\n    return tidy(function () {\n      for (var t = e[0], n = 1; n < e.length; ++n) {\n        t = minimum(t, e[n]);\n      }\n\n      return t;\n    });\n  }, t.className = \"Minimum\", t;\n}(Merge);\n\nserialization.registerClass(Minimum);\n\nvar Concatenate = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.DEFAULT_AXIS = -1, null == t && (t = {}), n.axis = null == t.axis ? n.DEFAULT_AXIS : t.axis, n.supportsMasking = !0, n.reshapeRequired = !1, n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    if (!Array.isArray(e) || !Array.isArray(e[0]) || 1 === e.length) throw new ValueError(\"A `Concatenate` layer should be called on a list of at least 2 inputs\");\n\n    for (var t = !0, n = 0, r = e = e; n < r.length; n++) {\n      if (null != (c = r[n])) {\n        t = !1;\n        break;\n      }\n    }\n\n    if (!t) {\n      for (var i = [], a = 0; a < e.length; ++a) {\n        var o = e[a].slice();\n        o.splice(this.axis, 1);\n\n        for (var s = !1, l = 0, u = i; l < u.length; l++) {\n          var c = u[l];\n\n          if (util.arraysEqual(c, o)) {\n            s = !0;\n            break;\n          }\n        }\n\n        s || i.push(o);\n      }\n\n      if (i.length > 1) throw new ValueError(\"A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: \" + JSON.stringify(e));\n    }\n  }, t.prototype.mergeFunction = function (e) {\n    var t = this;\n    return tidy(function () {\n      return concatenate(e, t.axis);\n    });\n  }, t.prototype.computeOutputShape = function (e) {\n    if (!Array.isArray(e) || !Array.isArray(e[0])) throw new ValueError(\"A `Concatenate` layer should be called on a list of inputs.\");\n\n    for (var t = e, n = t[0].slice(), r = this.axis < 0 ? n.length + this.axis : this.axis, i = 0, a = t.slice(1); i < a.length; i++) {\n      var o = a[i];\n\n      if (null == n[r] || null == o[r]) {\n        n[r] = null;\n        break;\n      }\n\n      n[r] += o[r];\n    }\n\n    return n;\n  }, t.prototype.computeMask = function (e, t) {\n    var n = this;\n    if (null == t) return null;\n    if (!Array.isArray(t)) throw new ValueError(\"`mask` should be an array for Concatenate\");\n    if (!Array.isArray(e)) throw new ValueError(\"`inputs` should be an array for Concatenate\");\n    if (t.length !== e.length) throw new ValueError(\"Mismatch in the length of mask (\" + t.length + \") and the legnth of inputs (\" + e.length + \")\");\n    return tidy(function () {\n      var r = !0;\n      if (t.forEach(function (e) {\n        null == e || (r = !1);\n      }), r) return null;\n\n      for (var i = [], a = 0; a < e.length; ++a) {\n        null == t[a] ? i.push(onesLike(e[a]).asType(\"bool\")) : t[a].rank < e[a].rank ? i.push(expandDims(t[a], -1)) : i.push(t[a]);\n      }\n\n      var o = concat(i, n.axis);\n      return all(o, -1, !1);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      axis: this.axis\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"Concatenate\", t;\n}(Merge);\n\nfunction interpretAxis(e, t) {\n  for (; e < 0;) {\n    e += t;\n  }\n\n  return e;\n}\n\nfunction batchDot(e, t, n) {\n  if (e.shape.length > 3 || t.shape.length > 3) throw new NotImplementedError(\"batchDot is not implemented for tensors of 4D or higher rank yet\");\n  if (util.assert(e.shape.length >= 2, \"batchDot requires the rank of x to be >= 2, but got \" + e.shape.length), util.assert(e.shape.length >= 2, \"batchDot requires the rank of y to be >= 2, but got \" + t.shape.length), \"number\" == typeof n && (n = [n, n]), \"complex64\" === e.dtype || \"complex64\" === t.dtype) throw new NotImplementedError(\"batchDot is not implemented for complex64-type Tensors yet.\");\n  var r = e.shape.length,\n      i = t.shape.length;\n  null == n && (n = [r - 1, i - 2]);\n  var a = n;\n  return tidy(function () {\n    var n, o;\n\n    if (r > i) {\n      n = r - i;\n\n      for (var s = [], l = 0; l < n; ++l) {\n        s.push(1);\n      }\n\n      t = t.reshape(t.shape.concat(s));\n    } else if (i > r) {\n      n = i - r;\n\n      for (s = [], l = 0; l < n; ++l) {\n        s.push(1);\n      }\n\n      e = e.reshape(e.shape.concat(s));\n    } else n = 0;\n\n    if (2 === e.shape.length && 2 === t.shape.length) o = a[0] === a[1] ? e.mulStrict(t).sum(a[0]) : e.transpose([1, 0]).mulStrict(t).sum(a[1]);else {\n      var u = a[0] !== e.shape.length - 1,\n          c = a[1] === t.shape.length - 1;\n      o = e.matMul(t, u, c);\n    }\n\n    if (n > 0) {\n      var p = void 0,\n          h = [];\n\n      for (l = p = r > i ? r + i - 3 : r - 1; l < p + n; ++l) {\n        h.push(l);\n      }\n\n      o = o.squeeze(h);\n    }\n\n    return 1 === o.shape.length && (o = o.expandDims(1)), o;\n  });\n}\n\nserialization.registerClass(Concatenate);\n\nvar Dot = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.axes = t.axes, n.normalize = null != t.normalize && t.normalize, n.supportsMasking = !0, n.reshapeRequired = !1, n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    util.assert(Array.isArray(e) && 2 === e.length && Array.isArray(e[0]) && Array.isArray(e[1]), \"A `Dot` layer should be called on a list of exactly 2 inputs.\");\n    var t = e[0],\n        n = e[1];\n    if (t.length > 3 || n.length > 3) throw new NotImplementedError(\"Dot layer does not support tensors of 4D or higher rank yet.\");\n    var r = this.interpretAxes(t, n);\n    if (t[r[0]] !== n[r[1]]) throw new ValueError(\"Dimension incompatibility: \" + t[r[0]] + \" !== \" + n[r[1]]);\n  }, t.prototype.mergeFunction = function (e) {\n    if (2 !== e.length) throw new ValueError(\"A `Dot` layer must be called on exactly 2 inputs, but received \" + e.length + \" input(s).\");\n    var t,\n        n = e[0],\n        r = e[1];\n    return t = Array.isArray(this.axes) ? this.axes.map(function (t, n) {\n      return interpretAxis(t, e[n].shape.length);\n    }) : [interpretAxis(this.axes, n.shape.length), interpretAxis(this.axes, r.shape.length)], this.normalize && (n = l2Normalize(n, t[0]), r = l2Normalize(r, t[1])), batchDot(n, r, t);\n  }, t.prototype.interpretAxes = function (e, t) {\n    return Array.isArray(this.axes) ? this.axes : [interpretAxis(this.axes, e.length), interpretAxis(this.axes, t.length)];\n  }, t.prototype.computeOutputShape = function (e) {\n    util.assert(Array.isArray(e) && 2 === e.length && Array.isArray(e[0]) && Array.isArray(e[1]), \"A `Dot` layer should be called on a list of exactly 2 inputs.\");\n    var t = e[0].slice(),\n        n = e[1].slice();\n    if (t.length > 3 || n.length > 3) throw new NotImplementedError(\"Dot layer does not support tensors of 4D or higher rank yet.\");\n    var r = this.interpretAxes(t, n);\n    t.splice(r[0], 1), n.splice(r[1], 1), n.splice(0, 1);\n    var i = t.concat(n);\n    return 1 === i.length && i.push(1), i;\n  }, t.prototype.computeMask = function (e, t) {\n    return null;\n  }, t.prototype.getConfig = function () {\n    var t = {\n      axes: this.axes,\n      normalize: this.normalize\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"Dot\", t;\n}(Merge);\n\nfunction batchNormalization(e, t, n, r, i, a) {\n  var o;\n  if (void 0 === a && (a = .001), 2 === e.rank) o = batchNorm2d(e, t, n, r, i, a);else if (3 === e.rank) o = batchNorm3d(e, t, n, r, i, a);else {\n    if (4 !== e.rank) throw new NotImplementedError(\"batchNormalization is not implemented for array of rank \" + e.rank + \" yet\");\n    o = batchNorm4d(e, t, n, r, i, a);\n  }\n  return o;\n}\n\nfunction regularNormalizeBatchInTraining(e, t, n, r, i) {\n  return void 0 === i && (i = .001), tidy(function () {\n    var a = moments(e, r),\n        o = a.mean,\n        s = a.variance;\n    return [batchNormalization(e, o, s, n, t, i), o, s];\n  });\n}\n\nfunction broadcastNormalizeBatchInTraining(e, t, n, r, i) {\n  return void 0 === i && (i = .001), tidy(function () {\n    for (var a = moments(e, r), o = a.mean, s = a.variance, l = [], u = 0, c = range(0, e.rank); u < c.length; u++) {\n      var p = c[u];\n      -1 !== r.indexOf(p) ? l.push(1) : l.push(e.shape[p]);\n    }\n\n    var h = o.reshape(l),\n        d = s.reshape(l),\n        f = null == t ? null : t.reshape(l),\n        g = null == n ? null : n.reshape(l);\n    return [batchNormalization(e, h, d, g, f, i), o, s];\n  });\n}\n\nfunction normalizeBatchInTraining(e, t, n, r, i) {\n  return void 0 === i && (i = .001), util.arraysEqual(r.slice().sort(), range(0, e.rank - 1)) ? regularNormalizeBatchInTraining(e, t, n, r, i) : broadcastNormalizeBatchInTraining(e, t, n, r, i);\n}\n\nserialization.registerClass(Dot);\n\nvar BatchNormalization = function (e) {\n  function t(t) {\n    var n = this;\n    return null == t && (t = {}), (n = e.call(this, t) || this).supportsMasking = !0, n.axis = null == t.axis ? -1 : t.axis, n.momentum = null == t.momentum ? .99 : t.momentum, n.epsilon = null == t.epsilon ? .001 : t.epsilon, n.center = null == t.center || t.center, n.scale = null == t.scale || t.scale, n.betaInitializer = getInitializer(t.betaInitializer || \"zeros\"), n.gammaInitializer = getInitializer(t.gammaInitializer || \"ones\"), n.movingMeanInitializer = getInitializer(t.movingMeanInitializer || \"zeros\"), n.movingVarianceInitializer = getInitializer(t.movingVarianceInitializer || \"ones\"), n.betaConstraint = getConstraint(t.betaConstraint), n.gammaConstraint = getConstraint(t.gammaConstraint), n.betaRegularizer = getRegularizer(t.betaRegularizer), n.gammaRegularizer = getRegularizer(t.gammaRegularizer), n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    e = getExactlyOneShape(e);\n    var t = this.axis >= 0 ? this.axis : this.axis + e.length,\n        n = e[t];\n    if (null == n) throw new ValueError(\"Axis \" + t + \" of input tensor should have a defined dimension but the layer received an input with shape \" + JSON.stringify(e) + \".\");\n    this.inputSpec = [new InputSpec({\n      ndim: e.length,\n      axes: (r = {}, r[t] = n, r)\n    })];\n    var r,\n        i = [n];\n    this.scale && (this.gamma = this.addWeight(\"gamma\", i, null, this.gammaInitializer, this.gammaRegularizer, !0, this.gammaConstraint)), this.center && (this.beta = this.addWeight(\"beta\", i, null, this.betaInitializer, this.betaRegularizer, !0, this.betaConstraint)), this.movingMean = this.addWeight(\"moving_mean\", i, null, this.movingMeanInitializer, null, !1), this.movingVariance = this.addWeight(\"moving_variance\", i, null, this.movingVarianceInitializer, null, !1), this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      var r = null != t.training && t.training,\n          i = getExactlyOneTensor(e),\n          a = i.shape,\n          o = a.length,\n          s = range(0, o),\n          l = n.axis >= 0 ? n.axis : n.axis + o;\n      s.splice(l, 1);\n      var u = pyListRepeat(1, o);\n      u[l] = a[l];\n      var c = s.slice();\n      c.sort();\n      var p = !util.arraysEqual(c, range(0, o).slice(0, o - 1));\n      if (!r) return function () {\n        if (p) {\n          var e = n.movingMean.read().reshape(u),\n              t = n.movingVariance.read().reshape(u),\n              r = n.center ? n.beta.read().reshape(u) : null,\n              a = n.scale ? n.gamma.read().reshape(u) : null;\n          return batchNormalization(i, e, t, r, a, n.epsilon);\n        }\n\n        return batchNormalization(i, n.movingMean.read(), n.movingVariance.read(), null == n.beta ? null : n.beta.read(), null == n.gamma ? null : n.gamma.read(), n.epsilon);\n      }();\n\n      var h = normalizeBatchInTraining(i, n.gamma.read(), n.beta.read(), s, n.epsilon),\n          d = h[0],\n          f = h[1],\n          g = h[2],\n          m = function m(e, t, n) {\n        tidy(function () {\n          var r = getScalar(1).sub(getScalar(n)),\n              i = e.read(),\n              a = i.sub(t).mul(r);\n          e.write(i.sub(a));\n        });\n      };\n\n      return m(n.movingMean, f, n.momentum), m(n.movingVariance, g, n.momentum), d;\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n      movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n      betaConstraint: serializeConstraint(this.betaConstraint),\n      gammaConstraint: serializeConstraint(this.gammaConstraint)\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"BatchNormalization\", t;\n}(Layer);\n\nfunction spatial2dPadding(e, t, n) {\n  return tidy(function () {\n    if (4 !== e.rank) throw new ValueError(\"temporalPadding expects input tensor to be 4-D, but received a \" + e.rank + \"-D tensor.\");\n    if (null == t && (t = [[1, 1], [1, 1]]), 2 !== t.length || 2 !== t[0].length || 2 !== t[1].length) throw new ValueError(\"spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.\");\n    if (null == n && (n = imageDataFormat()), \"channelsLast\" !== n && \"channelsFirst\" !== n) throw new ValueError(\"Unknown data format: \" + n + \". Supported data formats are 'channelsLast' and 'channelsFirst.\");\n    var r;\n    return r = \"channelsFirst\" === n ? [[0, 0], [0, 0], t[0], t[1]] : [[0, 0], t[0], t[1], [0, 0]], pad(e, r);\n  });\n}\n\nserialization.registerClass(BatchNormalization);\n\nvar ZeroPadding2D = function (e) {\n  function t(t) {\n    var n = this;\n    if (null == t && (t = {}), (n = e.call(this, t) || this).dataFormat = null == t.dataFormat ? imageDataFormat() : t.dataFormat, null == t.padding) n.padding = [[1, 1], [1, 1]];else if (\"number\" == typeof t.padding) n.padding = [[t.padding, t.padding], [t.padding, t.padding]];else {\n      if (t.padding = t.padding, 2 !== t.padding.length) throw new ValueError(\"ZeroPadding2D expects padding to be a length-2 array, but received a length-\" + t.padding.length + \" array.\");\n      var r = void 0,\n          i = void 0;\n      if (\"number\" == typeof t.padding[0]) r = [t.padding[0], t.padding[0]], i = [t.padding[1], t.padding[1]];else {\n        if (t.padding = t.padding, 2 !== t.padding[0].length) throw new ValueError(\"ZeroPadding2D expects height padding to be a length-2 array, but received a length-\" + t.padding[0].length + \" array.\");\n        if (r = t.padding[0], 2 !== t.padding[1].length) throw new ValueError(\"ZeroPadding2D expects width padding to be a length-2 array, but received a length-\" + t.padding[1].length + \" array.\");\n        i = t.padding[1];\n      }\n      n.padding = [r, i];\n    }\n    return n.inputSpec = [new InputSpec({\n      ndim: 4\n    })], n;\n  }\n\n  return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    var t, n;\n    return e = getExactlyOneShape(e), \"channelsFirst\" === this.dataFormat ? (t = null != e[2] && e[2] >= 0 ? e[2] + this.padding[0][0] + this.padding[0][1] : null, n = null != e[3] && e[3] >= 0 ? e[3] + this.padding[1][0] + this.padding[1][1] : null, [e[0], e[1], t, n]) : (t = null != e[1] && e[1] >= 0 ? e[1] + this.padding[0][0] + this.padding[0][1] : null, n = null != e[2] && e[2] >= 0 ? e[2] + this.padding[1][0] + this.padding[1][1] : null, [e[0], t, n, e[3]]);\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      return spatial2dPadding(getExactlyOneTensor(e), n.padding, n.dataFormat);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      padding: this.padding,\n      dataFormat: this.dataFormat\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"ZeroPadding2D\", t;\n}(Layer);\n\nfunction pool2d(e, t, n, r, i, a) {\n  return tidy(function () {\n    var o;\n    checkDataFormat(i), checkPoolMode(a), checkPaddingMode(r), null == n && (n = [1, 1]), null == r && (r = \"valid\"), null == i && (i = imageDataFormat()), null == a && (a = \"max\"), e = preprocessConv2DInput(e, i);\n    var s = \"same\" === r ? \"same\" : \"valid\";\n    return o = \"max\" === a ? maxPool(e, t, n, s) : avgPool(e, t, n, s), \"channelsFirst\" === i && (o = transpose(o, [0, 3, 1, 2])), o;\n  });\n}\n\nserialization.registerClass(ZeroPadding2D);\n\nvar Pooling1D = function (e) {\n  function t(t) {\n    var n = this;\n    if (null == t.poolSize && (t.poolSize = 2), n = e.call(this, t) || this, \"number\" == typeof t.poolSize) n.poolSize = [t.poolSize];else {\n      if (!Array.isArray(t.poolSize) || 1 !== t.poolSize.length || \"number\" != typeof t.poolSize[0]) throw new ValueError(\"poolSize for 1D convolutional layer must be a number or an Array of a single number, but received \" + JSON.stringify(t.poolSize));\n      n.poolSize = t.poolSize;\n    }\n    if (null == t.strides) n.strides = n.poolSize;else if (\"number\" == typeof t.strides) n.strides = [t.strides];else {\n      if (!Array.isArray(t.strides) || 1 !== t.strides.length || \"number\" != typeof t.strides[0]) throw new ValueError(\"strides for 1D convolutional layer must be a number or an Array of a single number, but received \" + JSON.stringify(t.strides));\n      n.strides = t.strides;\n    }\n    return n.padding = null == t.padding ? \"valid\" : t.padding, checkPaddingMode(n.padding), n.inputSpec = [new InputSpec({\n      ndim: 3\n    })], n;\n  }\n\n  return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    var t = convOutputLength((e = getExactlyOneShape(e))[1], this.poolSize[0], this.padding, this.strides[0]);\n    return [e[0], t, e[2]];\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      n.invokeCallHook(e, t), e = expandDims$1(getExactlyOneTensor(e), 2);\n      var r = n.poolingFunction(getExactlyOneTensor(e), [n.poolSize[0], 1], [n.strides[0], 1], n.padding, \"channelsLast\");\n      return squeeze(r, [2]);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      poolSize: this.poolSize,\n      padding: this.padding,\n      strides: this.strides\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t;\n}(Layer),\n    MaxPooling1D = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }\n\n  return __extends(t, e), t.prototype.poolingFunction = function (e, t, n, r, i) {\n    return checkDataFormat(i), checkPaddingMode(r), pool2d(e, t, n, r, i, \"max\");\n  }, t.className = \"MaxPooling1D\", t;\n}(Pooling1D);\n\nserialization.registerClass(MaxPooling1D);\n\nvar AveragePooling1D = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }\n\n  return __extends(t, e), t.prototype.poolingFunction = function (e, t, n, r, i) {\n    return checkDataFormat(i), checkPaddingMode(r), pool2d(e, t, n, r, i, \"avg\");\n  }, t.className = \"AveragePooling1D\", t;\n}(Pooling1D);\n\nserialization.registerClass(AveragePooling1D);\n\nvar Pooling2D = function (e) {\n  function t(t) {\n    var n = this;\n    if (null == t.poolSize && (t.poolSize = [2, 2]), (n = e.call(this, t) || this).poolSize = Array.isArray(t.poolSize) ? t.poolSize : [t.poolSize, t.poolSize], null == t.strides) n.strides = n.poolSize;else if (Array.isArray(t.strides)) {\n      if (2 !== t.strides.length) throw new ValueError(\"If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length \" + t.strides.length + \".\");\n      n.strides = t.strides;\n    } else n.strides = [t.strides, t.strides];\n    return n.padding = null == t.padding ? \"valid\" : t.padding, n.dataFormat = null == t.dataFormat ? \"channelsLast\" : t.dataFormat, checkDataFormat(n.dataFormat), checkPaddingMode(n.padding), n.inputSpec = [new InputSpec({\n      ndim: 4\n    })], n;\n  }\n\n  return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    e = getExactlyOneShape(e);\n    var t = \"channelsFirst\" === this.dataFormat ? e[2] : e[1],\n        n = \"channelsFirst\" === this.dataFormat ? e[3] : e[2];\n    return t = convOutputLength(t, this.poolSize[0], this.padding, this.strides[0]), n = convOutputLength(n, this.poolSize[1], this.padding, this.strides[1]), \"channelsFirst\" === this.dataFormat ? [e[0], e[1], t, n] : [e[0], t, n, e[3]];\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      return n.invokeCallHook(e, t), n.poolingFunction(getExactlyOneTensor(e), n.poolSize, n.strides, n.padding, n.dataFormat);\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      poolSize: this.poolSize,\n      padding: this.padding,\n      strides: this.strides,\n      dataFormat: this.dataFormat\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t;\n}(Layer),\n    MaxPooling2D = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }\n\n  return __extends(t, e), t.prototype.poolingFunction = function (e, t, n, r, i) {\n    return checkDataFormat(i), checkPaddingMode(r), pool2d(e, t, n, r, i, \"max\");\n  }, t.className = \"MaxPooling2D\", t;\n}(Pooling2D);\n\nserialization.registerClass(MaxPooling2D);\n\nvar AveragePooling2D = function (e) {\n  function t(t) {\n    return e.call(this, t) || this;\n  }\n\n  return __extends(t, e), t.prototype.poolingFunction = function (e, t, n, r, i) {\n    return checkDataFormat(i), checkPaddingMode(r), pool2d(e, t, n, r, i, \"avg\");\n  }, t.className = \"AveragePooling2D\", t;\n}(Pooling2D);\n\nserialization.registerClass(AveragePooling2D);\n\nvar GlobalPooling1D = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.inputSpec = [new InputSpec({\n      ndim: 3\n    })], n;\n  }\n\n  return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    return [e[0], e[2]];\n  }, t.prototype.call = function (e, t) {\n    throw new NotImplementedError();\n  }, t;\n}(Layer),\n    GlobalAveragePooling1D = function (e) {\n  function t(t) {\n    return e.call(this, t || {}) || this;\n  }\n\n  return __extends(t, e), t.prototype.call = function (e, t) {\n    return tidy(function () {\n      var t = getExactlyOneTensor(e);\n      return mean(t, 1);\n    });\n  }, t.className = \"GlobalAveragePooling1D\", t;\n}(GlobalPooling1D);\n\nserialization.registerClass(GlobalAveragePooling1D);\n\nvar GlobalMaxPooling1D = function (e) {\n  function t(t) {\n    return e.call(this, t || {}) || this;\n  }\n\n  return __extends(t, e), t.prototype.call = function (e, t) {\n    return tidy(function () {\n      var t = getExactlyOneTensor(e);\n      return max(t, 1);\n    });\n  }, t.className = \"GlobalMaxPooling1D\", t;\n}(GlobalPooling1D);\n\nserialization.registerClass(GlobalMaxPooling1D);\n\nvar GlobalPooling2D = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.dataFormat = null == t.dataFormat ? \"channelsLast\" : t.dataFormat, checkDataFormat(n.dataFormat), n.inputSpec = [new InputSpec({\n      ndim: 4\n    })], n;\n  }\n\n  return __extends(t, e), t.prototype.computeOutputShape = function (e) {\n    return e = e, \"channelsLast\" === this.dataFormat ? [e[0], e[3]] : [e[0], e[1]];\n  }, t.prototype.call = function (e, t) {\n    throw new NotImplementedError();\n  }, t.prototype.getConfig = function () {\n    var t = {\n      dataFormat: this.dataFormat\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t;\n}(Layer),\n    GlobalAveragePooling2D = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      var t = getExactlyOneTensor(e);\n      return \"channelsLast\" === n.dataFormat ? mean(t, [1, 2]) : mean(t, [2, 3]);\n    });\n  }, t.className = \"GlobalAveragePooling2D\", t;\n}(GlobalPooling2D);\n\nserialization.registerClass(GlobalAveragePooling2D);\n\nvar GlobalMaxPooling2D = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      var t = getExactlyOneTensor(e);\n      return \"channelsLast\" === n.dataFormat ? max(t, [1, 2]) : max(t, [2, 3]);\n    });\n  }, t.className = \"GlobalMaxPooling2D\", t;\n}(GlobalPooling2D);\n\nfunction standardizeArgs(e, t, n, r) {\n  if (Array.isArray(e)) {\n    if (null != t || null != n) throw new ValueError(\"When inputs is an array, neither initialState or constants should be provided\");\n    null != r && (n = e.slice(e.length - r, e.length), e = e.slice(0, e.length - r)), e.length > 1 && (t = e.slice(1, e.length)), e = e[0];\n  }\n\n  function i(e) {\n    return null == e || Array.isArray(e) ? e : [e];\n  }\n\n  return {\n    inputs: e,\n    initialState: t = i(t),\n    constants: n = i(n)\n  };\n}\n\nfunction rnn(e, t, n, r, i, a, o, s) {\n  return void 0 === r && (r = !1), void 0 === o && (o = !1), void 0 === s && (s = !1), tidy(function () {\n    var l = t.shape.length;\n    if (l < 3) throw new ValueError(\"Input should be at least 3D, but is \" + l + \"D.\");\n    var u = [1, 0].concat(range(2, l));\n    if (t = transpose(t, u), null != a) throw new NotImplementedError(\"The rnn() functoin of the deeplearn.js backend does not support constants yet.\");\n    o && console.warn(\"Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend.\"), null != i && ((i = i.asType(\"bool\").asType(\"float32\")).rank === l - 1 && (i = expandDims(i, -1)), i = transpose(i, u)), r && (t = reverse(t, 0), null != i && (i = reverse(i, 0)));\n    var c,\n        p,\n        h = [],\n        d = n,\n        f = t.shape[0],\n        g = unstack(t);\n    null != i && (p = unstack(i));\n\n    for (var m, y = function y(t) {\n      var n = g[t],\n          r = tidy(function () {\n        return e(n, d);\n      });\n      if (null == i) c = r[0], d = r[1];else {\n        var a = tidy(function () {\n          var e = p[t],\n              n = onesLike(e).sub(e);\n          return {\n            output: r[0].mul(e).addStrict(d[0].mul(n)),\n            newStates: d.map(function (t, i) {\n              return r[1][i].mul(e).addStrict(t.mul(n));\n            })\n          };\n        });\n        c = a.output, d = a.newStates;\n      }\n      s && h.push(c);\n    }, v = 0; v < f; ++v) {\n      y(v);\n    }\n\n    if (s) {\n      m = stack(h, 1);\n    }\n\n    return [c, m, d];\n  });\n}\n\nserialization.registerClass(GlobalMaxPooling2D);\n\nvar RNN = function (e) {\n  function t(t) {\n    var n,\n        r = e.call(this, t) || this;\n    if (null == t.cell) throw new ValueError(\"cell property is missing for the constructor of RNN.\");\n    if (null == (n = Array.isArray(t.cell) ? new StackedRNNCells({\n      cells: t.cell\n    }) : t.cell).stateSize) throw new ValueError(\"The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).\");\n    return r.cell = n, r.returnSequences = null != t.returnSequences && t.returnSequences, r.returnState = null != t.returnState && t.returnState, r.goBackwards = null != t.goBackwards && t.goBackwards, r._stateful = null != t.stateful && t.stateful, r.unroll = null != t.unroll && t.unroll, r.supportsMasking = !0, r.inputSpec = [new InputSpec({\n      ndim: 3\n    })], r.stateSpec = null, r.states_ = null, r.numConstants = null, r.keptStates = [], r;\n  }\n\n  return __extends(t, e), t.prototype.getStates = function () {\n    return null == this.states_ ? range(0, Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1).map(function (e) {\n      return null;\n    }) : this.states_;\n  }, t.prototype.setStates = function (e) {\n    this.states_ = e;\n  }, t.prototype.computeOutputShape = function (e) {\n    isArrayOfShapes(e) && (e = e[0]), e = e;\n    var t = this.cell.stateSize;\n    Array.isArray(t) || (t = [t]);\n    var n,\n        r = t[0];\n\n    if (n = this.returnSequences ? [e[0], e[1], r] : [e[0], r], this.returnState) {\n      for (var i = [], a = 0, o = t; a < o.length; a++) {\n        var s = o[a];\n        i.push([e[0], s]);\n      }\n\n      return [n].concat(i);\n    }\n\n    return n;\n  }, t.prototype.computeMask = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      Array.isArray(t) && (t = t[0]);\n      var e = n.returnSequences ? t : null;\n\n      if (n.returnState) {\n        var r = n.states.map(function (e) {\n          return null;\n        });\n        return [e].concat(r);\n      }\n\n      return e;\n    });\n  }, Object.defineProperty(t.prototype, \"states\", {\n    get: function get() {\n      if (null == this.states_) {\n        for (var e = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1, t = [], n = 0; n < e; ++n) {\n          t.push(null);\n        }\n\n        return t;\n      }\n\n      return this.states_;\n    },\n    set: function set(e) {\n      this.states_ = e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.build = function (e) {\n    if (null != this.numConstants) throw new NotImplementedError(\"Constants support is not implemented in RNN yet.\");\n    isArrayOfShapes(e) && (e = e[0]), e = e;\n    var t = this.stateful ? e[0] : null,\n        n = e[e.length - 1];\n    this.inputSpec[0] = new InputSpec({\n      shape: [t, null, n]\n    });\n    var r,\n        i = [e[0]].concat(e.slice(2));\n\n    if (this.cell.build(i), r = Array.isArray(this.cell.stateSize) ? this.cell.stateSize : [this.cell.stateSize], null != this.stateSpec) {\n      if (!util.arraysEqual(this.stateSpec.map(function (e) {\n        return e.shape[e.shape.length - 1];\n      }), r)) throw new ValueError(\"An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=\" + this.stateSpec + \"; However cell.stateSize is \" + this.cell.stateSize);\n    } else this.stateSpec = r.map(function (e) {\n      return new InputSpec({\n        shape: [null, e]\n      });\n    });\n\n    this.stateful && this.resetStates();\n  }, t.prototype.resetStates = function (e, t) {\n    var n = this;\n    void 0 === t && (t = !1), tidy(function () {\n      if (!n.stateful) throw new AttributeError(\"Cannot call resetStates() on an RNN Layer that is not stateful.\");\n      var r = n.inputSpec[0].shape[0];\n      if (null == r) throw new ValueError(\"If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \\n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.\");\n      if (null == n.states_) Array.isArray(n.cell.stateSize) ? n.states_ = n.cell.stateSize.map(function (e) {\n        return zeros([r, e]);\n      }) : n.states_ = [zeros([r, n.cell.stateSize])];else if (null == e) dispose(n.states_), null != n.keptStates && (dispose(n.keptStates), n.keptStates = []), Array.isArray(n.cell.stateSize) ? n.states_ = n.cell.stateSize.map(function (e) {\n        return zeros([r, e]);\n      }) : n.states_[0] = zeros([r, n.cell.stateSize]);else {\n        if (Array.isArray(e) || (e = [e]), e.length !== n.states_.length) throw new ValueError(\"Layer \" + n.name + \" expects \" + n.states_.length + \" state(s), but it received \" + e.length + \" state value(s). Input received: \" + e);\n        !0 === t ? n.keptStates.push(n.states_.slice()) : dispose(n.states_);\n\n        for (var i = 0; i < n.states_.length; ++i) {\n          var a = e[i],\n              o = Array.isArray(n.cell.stateSize) ? n.cell.stateSize[i] : n.cell.stateSize,\n              s = [r, o];\n          if (!util.arraysEqual(a.shape, s)) throw new ValueError(\"State \" + i + \" is incompatible with layer \" + n.name + \": expected shape=\" + s + \", received shape=\" + a.shape);\n          n.states_[i] = a;\n        }\n      }\n      n.states_.forEach(function (e) {\n        return keep(e);\n      });\n    });\n  }, t.prototype.apply = function (t, n) {\n    var r = null == n ? null : n.initialState,\n        i = null == n ? null : n.constants;\n    null == n && (n = {});\n    var a = standardizeArgs(t, r, i, this.numConstants);\n    t = a.inputs, r = a.initialState, i = a.constants;\n    var o = [],\n        s = [];\n\n    if (null != r) {\n      n.initialState = r, o = o.concat(r), this.stateSpec = [];\n\n      for (var l = 0, u = r; l < u.length; l++) {\n        var c = u[l];\n        this.stateSpec.push(new InputSpec({\n          shape: c.shape\n        }));\n      }\n\n      s = s.concat(this.stateSpec);\n    }\n\n    if (null != i && (n.constants = i, o = o.concat(i), this.numConstants = i.length), o[0] instanceof SymbolicTensor) {\n      var p = [t].concat(o),\n          h = this.inputSpec.concat(s),\n          d = this.inputSpec;\n      this.inputSpec = h;\n      var f = e.prototype.apply.call(this, p, n);\n      return this.inputSpec = d, f;\n    }\n\n    return e.prototype.apply.call(this, t, n);\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      var r = null == t ? null : t.mask,\n          i = null == t ? null : t.training,\n          a = null == t ? null : t.initialState;\n      e = getExactlyOneTensor(e), null == a && (a = n.stateful ? n.states_ : n.getInitialState(e));\n      var o = Array.isArray(n.cell.stateSize) ? n.cell.stateSize.length : 1;\n      if (a.length !== o) throw new ValueError(\"RNN Layer has \" + o + \" state(s) but was passed \" + a.length + \" initial state(s).\");\n      n.unroll && console.warn(\"Ignoring unroll = true for RNN layer, due to imperative backend.\");\n      var s = {\n        training: i\n      },\n          l = rnn(function (e, t) {\n        var r = n.cell.call([e].concat(t), s);\n        return [r[0], r.slice(1)];\n      }, e, a, n.goBackwards, r, null, n.unroll, n.returnSequences),\n          u = l[0],\n          c = l[1],\n          p = l[2];\n      n.stateful && n.resetStates(p, i);\n      var h = n.returnSequences ? c : u;\n      return n.returnState ? [h].concat(p) : h;\n    });\n  }, t.prototype.getInitialState = function (e) {\n    var t = this;\n    return tidy(function () {\n      var n = zeros(e.shape);\n      return n = expandDims$1(n = sum(n, [1, 2])), Array.isArray(t.cell.stateSize) ? t.cell.stateSize.map(function (e) {\n        return e > 1 ? tile$1(n, [1, e]) : n;\n      }) : t.cell.stateSize > 1 ? [tile$1(n, [1, t.cell.stateSize])] : [n];\n    });\n  }, Object.defineProperty(t.prototype, \"trainableWeights\", {\n    get: function get() {\n      return this.trainable ? this.cell.trainableWeights : [];\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"nonTrainableWeights\", {\n    get: function get() {\n      return this.trainable ? this.cell.nonTrainableWeights : this.cell.weights;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.setFastWeightInitDuringBuild = function (t) {\n    e.prototype.setFastWeightInitDuringBuild.call(this, t), null != this.cell && this.cell.setFastWeightInitDuringBuild(t);\n  }, t.prototype.getConfig = function () {\n    var t = {\n      returnSequences: this.returnSequences,\n      returnState: this.returnState,\n      goBackwards: this.goBackwards,\n      stateful: this.stateful,\n      unroll: this.unroll\n    };\n    null != this.numConstants && (t.numConstants = this.numConstants);\n    var n = this.cell.getConfig();\n    t.cell = {\n      className: this.cell.getClassName(),\n      config: n\n    };\n    var r = e.prototype.getConfig.call(this);\n    return Object.assign(t, r), t;\n  }, t.className = \"RNN\", t;\n}(Layer);\n\nserialization.registerClass(RNN);\n\nvar RNNCell = function (e) {\n  function t() {\n    return null !== e && e.apply(this, arguments) || this;\n  }\n\n  return __extends(t, e), t;\n}(Layer),\n    SimpleRNNCell = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.DEFAULT_ACTIVATION = \"tanh\", n.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", n.DEFAULT_RECURRENT_INITIALIZER = \"orthogonal\", n.DEFAULT_BIAS_INITIALIZER = \"zeros\", n.units = t.units, n.activation = getActivation(null == t.activation ? n.DEFAULT_ACTIVATION : t.activation), n.useBias = null == t.useBias || t.useBias, n.kernelInitializer = getInitializer(t.kernelInitializer || n.DEFAULT_KERNEL_INITIALIZER), n.recurrentInitializer = getInitializer(t.recurrentInitializer || n.DEFAULT_RECURRENT_INITIALIZER), n.biasInitializer = getInitializer(t.biasInitializer || n.DEFAULT_BIAS_INITIALIZER), n.kernelRegularizer = getRegularizer(t.kernelRegularizer), n.recurrentRegularizer = getRegularizer(t.recurrentRegularizer), n.biasRegularizer = getRegularizer(t.biasRegularizer), n.kernelConstraint = getConstraint(t.kernelConstraint), n.recurrentConstraint = getConstraint(t.recurrentConstraint), n.biasConstraint = getConstraint(t.biasConstraint), n.dropout = min$1([1, max$1([0, null == t.dropout ? 0 : t.dropout])]), n.recurrentDropout = min$1([1, max$1([0, null == t.recurrentDropout ? 0 : t.recurrentDropout])]), n.stateSize = n.units, n.dropoutMask = null, n.recurrentDropoutMask = null, n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    e = getExactlyOneShape(e), this.kernel = this.addWeight(\"kernel\", [e[e.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.recurrentKernel = this.addWeight(\"recurrent_kernel\", [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.useBias ? this.bias = this.addWeight(\"bias\", [this.units], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : this.bias = null, this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      if (2 !== (e = e).length) throw new ValueError(\"SimpleRNNCell expects 2 input Tensors, got \" + e.length + \".\");\n      var r = e[1];\n      e = e[0];\n      var i,\n          a = null != t.training && t.training;\n      0 < n.dropout && n.dropout < 1 && null == n.dropoutMask && (n.dropoutMask = generateDropoutMask(function () {\n        return onesLike(e);\n      }, n.dropout, a)), 0 < n.recurrentDropout && n.recurrentDropout < 1 && null == n.recurrentDropoutMask && (n.recurrentDropoutMask = generateDropoutMask(function () {\n        return onesLike(r);\n      }, n.recurrentDropout, a));\n      var o = n.dropoutMask,\n          s = n.recurrentDropoutMask;\n      i = dot(null != o ? mul(e, o) : e, n.kernel.read()), null != n.bias && (i = biasAdd(i, n.bias.read())), null != s && (r = mul(r, s));\n      var l = add(i, dot(r, n.recurrentKernel.read()));\n      return null != n.activation && (l = n.activation.apply(l)), [l, l];\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"SimpleRNNCell\", t;\n}(RNNCell);\n\nserialization.registerClass(SimpleRNNCell);\n\nvar SimpleRNN = function (e) {\n  function t(t) {\n    return t.cell = new SimpleRNNCell(t), e.call(this, t) || this;\n  }\n\n  return __extends(t, e), t.prototype.call = function (t, n) {\n    var r = this;\n    return tidy(function () {\n      null != r.cell.dropoutMask && (dispose(r.cell.dropoutMask), r.cell.dropoutMask = null), null != r.cell.recurrentDropoutMask && (dispose(r.cell.recurrentDropoutMask), r.cell.recurrentDropoutMask = null);\n      var i = null == n ? null : n.mask,\n          a = null == n ? null : n.training,\n          o = null == n ? null : n.initialState;\n      return e.prototype.call.call(r, t, {\n        mask: i,\n        training: a,\n        initialState: o\n      });\n    });\n  }, Object.defineProperty(t.prototype, \"units\", {\n    get: function get() {\n      return this.cell.units;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"activation\", {\n    get: function get() {\n      return this.cell.activation;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"useBias\", {\n    get: function get() {\n      return this.cell.useBias;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"kernelInitializer\", {\n    get: function get() {\n      return this.cell.kernelInitializer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentInitializer\", {\n    get: function get() {\n      return this.cell.recurrentInitializer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"biasInitializer\", {\n    get: function get() {\n      return this.cell.biasInitializer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"kernelRegularizer\", {\n    get: function get() {\n      return this.cell.kernelRegularizer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentRegularizer\", {\n    get: function get() {\n      return this.cell.recurrentRegularizer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"biasRegularizer\", {\n    get: function get() {\n      return this.cell.biasRegularizer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"kernelConstraint\", {\n    get: function get() {\n      return this.cell.kernelConstraint;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentConstraint\", {\n    get: function get() {\n      return this.cell.recurrentConstraint;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"biasConstraint\", {\n    get: function get() {\n      return this.cell.biasConstraint;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"dropout\", {\n    get: function get() {\n      return this.cell.dropout;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentDropout\", {\n    get: function get() {\n      return this.cell.recurrentDropout;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.getConfig = function () {\n    var t = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout\n    },\n        n = e.prototype.getConfig.call(this);\n    return delete n.cell, Object.assign(t, n), t;\n  }, t.className = \"SimpleRNN\", t;\n}(RNN);\n\nserialization.registerClass(SimpleRNN);\n\nvar GRUCell = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.DEFAULT_ACTIVATION = \"tanh\", n.DEFAULT_RECURRENT_ACTIVATION = \"hardSigmoid\", n.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", n.DEFAULT_RECURRENT_INITIALIZER = \"orthogonal\", n.DEFAULT_BIAS_INITIALIZER = \"zeros\", n.units = t.units, n.activation = getActivation(void 0 === t.activation ? n.DEFAULT_ACTIVATION : t.activation), n.recurrentActivation = getActivation(void 0 === t.recurrentActivation ? n.DEFAULT_RECURRENT_ACTIVATION : t.recurrentActivation), n.useBias = null == t.useBias || t.useBias, n.kernelInitializer = getInitializer(t.kernelInitializer || n.DEFAULT_KERNEL_INITIALIZER), n.recurrentInitializer = getInitializer(t.recurrentInitializer || n.DEFAULT_RECURRENT_INITIALIZER), n.biasInitializer = getInitializer(t.biasInitializer || n.DEFAULT_BIAS_INITIALIZER), n.kernelRegularizer = getRegularizer(t.kernelRegularizer), n.recurrentRegularizer = getRegularizer(t.recurrentRegularizer), n.biasRegularizer = getRegularizer(t.biasRegularizer), n.kernelConstraint = getConstraint(t.kernelConstraint), n.recurrentConstraint = getConstraint(t.recurrentConstraint), n.biasConstraint = getConstraint(t.biasConstraint), n.dropout = min$1([1, max$1([0, null == t.dropout ? 0 : t.dropout])]), n.recurrentDropout = min$1([1, max$1([0, null == t.recurrentDropout ? 0 : t.recurrentDropout])]), n.implementation = t.implementation, n.stateSize = n.units, n.dropoutMask = null, n.recurrentDropoutMask = null, n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    var t = (e = getExactlyOneShape(e))[e.length - 1];\n    this.kernel = this.addWeight(\"kernel\", [t, 3 * this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.recurrentKernel = this.addWeight(\"recurrent_kernel\", [this.units, 3 * this.units], null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.useBias ? this.bias = this.addWeight(\"bias\", [3 * this.units], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : this.bias = null, this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      if (2 !== (e = e).length) throw new ValueError(\"GRUCell expects 2 input Tensors (inputs, h, c), got \" + e.length + \".\");\n      var r = null != t.training && t.training,\n          i = e[1];\n      e = e[0], 0 < n.dropout && n.dropout < 1 && null == n.dropoutMask && (n.dropoutMask = generateDropoutMask(function () {\n        return onesLike(e);\n      }, n.dropout, r, 3)), 0 < n.recurrentDropout && n.recurrentDropout < 1 && null == n.recurrentDropoutMask && (n.recurrentDropoutMask = generateDropoutMask(function () {\n        return onesLike(i);\n      }, n.recurrentDropout, r, 3));\n      var a,\n          o,\n          s,\n          l = n.dropoutMask,\n          u = n.recurrentDropoutMask;\n      0 < n.dropout && n.dropout < 1 && (e = mul(e, l[0]));\n      var c = dot(e, n.kernel.read());\n      n.useBias && (c = biasAdd(c, n.bias.read())), 0 < n.recurrentDropout && n.recurrentDropout < 1 && (i = mul(i, u[0]));\n      var p = n.recurrentKernel.read(),\n          h = split(p, [2 * n.units, n.units], p.rank - 1),\n          d = h[0],\n          f = h[1],\n          g = dot(i, d),\n          m = split(c, 3, c.rank - 1),\n          y = m[0],\n          v = m[1],\n          b = m[2],\n          w = split(g, 2, g.rank - 1),\n          z = w[0],\n          S = w[1];\n      a = n.recurrentActivation.apply(add(y, z)), o = n.recurrentActivation.apply(add(v, S));\n      var I = dot(mul(o, i), f);\n      s = n.activation.apply(add(b, I));\n      var A = add(mul(a, i), mul(add(getScalar(1), neg(a)), s));\n      return [A, A];\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"GRUCell\", t;\n}(RNNCell);\n\nserialization.registerClass(GRUCell);\n\nvar GRU = function (e) {\n  function t(t) {\n    return 0 === t.implementation && console.warn(\"`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call.\"), t.cell = new GRUCell(t), e.call(this, t) || this;\n  }\n\n  return __extends(t, e), t.prototype.call = function (t, n) {\n    var r = this;\n    return tidy(function () {\n      null != r.cell.dropoutMask && (dispose(r.cell.dropoutMask), r.cell.dropoutMask = null), null != r.cell.recurrentDropoutMask && (dispose(r.cell.recurrentDropoutMask), r.cell.recurrentDropoutMask = null);\n      var i = null == n ? null : n.mask,\n          a = null == n ? null : n.training,\n          o = null == n ? null : n.initialState;\n      return e.prototype.call.call(r, t, {\n        mask: i,\n        training: a,\n        initialState: o\n      });\n    });\n  }, Object.defineProperty(t.prototype, \"units\", {\n    get: function get() {\n      return this.cell.units;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"activation\", {\n    get: function get() {\n      return this.cell.activation;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentActivation\", {\n    get: function get() {\n      return this.cell.recurrentActivation;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"useBias\", {\n    get: function get() {\n      return this.cell.useBias;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"kernelInitializer\", {\n    get: function get() {\n      return this.cell.kernelInitializer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentInitializer\", {\n    get: function get() {\n      return this.cell.recurrentInitializer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"biasInitializer\", {\n    get: function get() {\n      return this.cell.biasInitializer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"kernelRegularizer\", {\n    get: function get() {\n      return this.cell.kernelRegularizer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentRegularizer\", {\n    get: function get() {\n      return this.cell.recurrentRegularizer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"biasRegularizer\", {\n    get: function get() {\n      return this.cell.biasRegularizer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"kernelConstraint\", {\n    get: function get() {\n      return this.cell.kernelConstraint;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentConstraint\", {\n    get: function get() {\n      return this.cell.recurrentConstraint;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"biasConstraint\", {\n    get: function get() {\n      return this.cell.biasConstraint;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"dropout\", {\n    get: function get() {\n      return this.cell.dropout;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentDropout\", {\n    get: function get() {\n      return this.cell.recurrentDropout;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"implementation\", {\n    get: function get() {\n      return this.cell.implementation;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.getConfig = function () {\n    var t = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    },\n        n = e.prototype.getConfig.call(this);\n    return delete n.cell, Object.assign(t, n), t;\n  }, t.fromConfig = function (e, t) {\n    return 0 === t.implmentation && (t.implementation = 1), new e(t);\n  }, t.className = \"GRU\", t;\n}(RNN);\n\nserialization.registerClass(GRU);\n\nvar LSTMCell = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.DEFAULT_ACTIVATION = \"tanh\", n.DEFAULT_RECURRENT_ACTIVATION = \"hardSigmoid\", n.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", n.DEFAULT_RECURRENT_INITIALIZER = \"orthogonal\", n.DEFAULT_BIAS_INITIALIZER = \"zeros\", n.units = t.units, n.activation = getActivation(void 0 === t.activation ? n.DEFAULT_ACTIVATION : t.activation), n.recurrentActivation = getActivation(void 0 === t.recurrentActivation ? n.DEFAULT_RECURRENT_ACTIVATION : t.recurrentActivation), n.useBias = null == t.useBias || t.useBias, n.kernelInitializer = getInitializer(t.kernelInitializer || n.DEFAULT_KERNEL_INITIALIZER), n.recurrentInitializer = getInitializer(t.recurrentInitializer || n.DEFAULT_RECURRENT_INITIALIZER), n.biasInitializer = getInitializer(t.biasInitializer || n.DEFAULT_BIAS_INITIALIZER), n.unitForgetBias = t.unitForgetBias, n.kernelRegularizer = getRegularizer(t.kernelRegularizer), n.recurrentRegularizer = getRegularizer(t.recurrentRegularizer), n.biasRegularizer = getRegularizer(t.biasRegularizer), n.kernelConstraint = getConstraint(t.kernelConstraint), n.recurrentConstraint = getConstraint(t.recurrentConstraint), n.biasConstraint = getConstraint(t.biasConstraint), n.dropout = min$1([1, max$1([0, null == t.dropout ? 0 : t.dropout])]), n.recurrentDropout = min$1([1, max$1([0, null == t.recurrentDropout ? 0 : t.recurrentDropout])]), n.implementation = t.implementation, n.stateSize = [n.units, n.units], n.dropoutMask = null, n.recurrentDropoutMask = null, n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    var t,\n        n,\n        r = (e = getExactlyOneShape(e))[e.length - 1];\n\n    if (this.kernel = this.addWeight(\"kernel\", [r, 4 * this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.recurrentKernel = this.addWeight(\"recurrent_kernel\", [this.units, 4 * this.units], null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.useBias) {\n      if (this.unitForgetBias) {\n        var i = this.biasInitializer,\n            a = this.units;\n        t = new ((n = function (e) {\n          function t() {\n            return null !== e && e.apply(this, arguments) || this;\n          }\n\n          return __extends(t, e), t.prototype.apply = function (e, t) {\n            var n = i.apply([a]),\n                r = new Ones().apply([a]),\n                o = i.apply([2 * a]);\n            return concatAlongFirstAxis(concatAlongFirstAxis(n, r), o);\n          }, t;\n        }(Initializer)).className = \"CustomInit\", n)();\n      } else t = this.biasInitializer;\n\n      this.bias = this.addWeight(\"bias\", [4 * this.units], null, t, this.biasRegularizer, !0, this.biasConstraint);\n    } else this.bias = null;\n\n    this.built = !0;\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      var r = null != t.training && t.training;\n      if (3 !== (e = e).length) throw new ValueError(\"LSTMCell expects 3 input Tensors (inputs, h, c), got \" + e.length + \".\");\n      var i = e[1],\n          a = e[2];\n      e = e[0], 0 < n.dropout && n.dropout < 1 && null == n.dropoutMask && (n.dropoutMask = generateDropoutMask(function () {\n        return onesLike(e);\n      }, n.dropout, r, 4)), 0 < n.recurrentDropout && n.recurrentDropout < 1 && null == n.recurrentDropoutMask && (n.recurrentDropoutMask = generateDropoutMask(function () {\n        return onesLike(i);\n      }, n.recurrentDropout, r, 4));\n      var o,\n          s,\n          l,\n          u,\n          c = n.dropoutMask,\n          p = n.recurrentDropoutMask;\n      0 < n.dropout && n.dropout < 1 && (e = mul(e, c[0]));\n      var h = dot(e, n.kernel.read());\n      0 < n.recurrentDropout && n.recurrentDropout < 1 && (i = mul(i, p[0])), h = add(h, dot(i, n.recurrentKernel.read())), n.useBias && (h = biasAdd(h, n.bias.read()));\n      var d = split(h, 4, h.rank - 1),\n          f = d[0],\n          g = d[1],\n          m = d[2],\n          y = d[3];\n      o = n.recurrentActivation.apply(f), s = n.recurrentActivation.apply(g), l = add(mul(s, a), mul(o, n.activation.apply(m))), u = n.recurrentActivation.apply(y);\n      var v = mul(u, n.activation.apply(l));\n      return [v, v, l];\n    });\n  }, t.prototype.getConfig = function () {\n    var t = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.className = \"LSTMCell\", t;\n}(RNNCell);\n\nserialization.registerClass(LSTMCell);\n\nvar LSTM = function (e) {\n  function t(t) {\n    return 0 === t.implementation && console.warn(\"`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call.\"), t.cell = new LSTMCell(t), e.call(this, t) || this;\n  }\n\n  return __extends(t, e), t.prototype.call = function (t, n) {\n    var r = this;\n    return tidy(function () {\n      null != r.cell.dropoutMask && (dispose(r.cell.dropoutMask), r.cell.dropoutMask = null), null != r.cell.recurrentDropoutMask && (dispose(r.cell.recurrentDropoutMask), r.cell.recurrentDropoutMask = null);\n      var i = null == n ? null : n.mask,\n          a = null == n ? null : n.training,\n          o = null == n ? null : n.initialState;\n      return e.prototype.call.call(r, t, {\n        mask: i,\n        training: a,\n        initialState: o\n      });\n    });\n  }, Object.defineProperty(t.prototype, \"units\", {\n    get: function get() {\n      return this.cell.units;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"activation\", {\n    get: function get() {\n      return this.cell.activation;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentActivation\", {\n    get: function get() {\n      return this.cell.recurrentActivation;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"useBias\", {\n    get: function get() {\n      return this.cell.useBias;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"kernelInitializer\", {\n    get: function get() {\n      return this.cell.kernelInitializer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentInitializer\", {\n    get: function get() {\n      return this.cell.recurrentInitializer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"biasInitializer\", {\n    get: function get() {\n      return this.cell.biasInitializer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"unitForgetBias\", {\n    get: function get() {\n      return this.cell.unitForgetBias;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"kernelRegularizer\", {\n    get: function get() {\n      return this.cell.kernelRegularizer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentRegularizer\", {\n    get: function get() {\n      return this.cell.recurrentRegularizer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"biasRegularizer\", {\n    get: function get() {\n      return this.cell.biasRegularizer;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"kernelConstraint\", {\n    get: function get() {\n      return this.cell.kernelConstraint;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentConstraint\", {\n    get: function get() {\n      return this.cell.recurrentConstraint;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"biasConstraint\", {\n    get: function get() {\n      return this.cell.biasConstraint;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"dropout\", {\n    get: function get() {\n      return this.cell.dropout;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"recurrentDropout\", {\n    get: function get() {\n      return this.cell.recurrentDropout;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"implementation\", {\n    get: function get() {\n      return this.cell.implementation;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.getConfig = function () {\n    var t = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    },\n        n = e.prototype.getConfig.call(this);\n    return delete n.cell, Object.assign(t, n), t;\n  }, t.fromConfig = function (e, t) {\n    return 0 === t.implmentation && (t.implementation = 1), new e(t);\n  }, t.className = \"LSTM\", t;\n}(RNN);\n\nserialization.registerClass(LSTM);\n\nvar StackedRNNCells = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.cells = t.cells, n;\n  }\n\n  return __extends(t, e), Object.defineProperty(t.prototype, \"stateSize\", {\n    get: function get() {\n      for (var e = [], t = 0, n = this.cells.slice().reverse(); t < n.length; t++) {\n        var r = n[t];\n        Array.isArray(r.stateSize) ? e.push.apply(e, r.stateSize) : e.push(r.stateSize);\n      }\n\n      return e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      for (var r = (e = e).slice(1), i = [], a = 0, o = n.cells.slice().reverse(); a < o.length; a++) {\n        var s = o[a];\n        Array.isArray(s.stateSize) ? i.push(r.splice(0, s.stateSize.length)) : i.push(r.splice(0, 1));\n      }\n\n      i.reverse();\n\n      for (var l, u = [], c = 0; c < n.cells.length; ++c) {\n        s = n.cells[c];\n        r = i[c], l = 0 === c ? [e[0]].concat(r) : [l[0]].concat(r), l = s.call(l, t), u.push(l.slice(1));\n      }\n\n      r = [];\n\n      for (var p = 0, h = u.slice().reverse(); p < h.length; p++) {\n        var d = h[p];\n        r.push.apply(r, d);\n      }\n\n      return [l[0]].concat(r);\n    });\n  }, t.prototype.build = function (e) {\n    var t;\n    isArrayOfShapes(e) && (e = e[0]), e = e;\n\n    for (var n = 0, r = this.cells; n < r.length; n++) {\n      var i = r[n];\n      i.build(e), t = Array.isArray(i.stateSize) ? i.stateSize[0] : i.stateSize, e = [e[0], t];\n    }\n\n    this.built = !0;\n  }, t.prototype.getConfig = function () {\n    for (var t = [], n = 0, r = this.cells; n < r.length; n++) {\n      var i = r[n];\n      t.push({\n        className: this.getClassName(),\n        config: i.getConfig()\n      });\n    }\n\n    var a = {\n      cells: t\n    },\n        o = e.prototype.getConfig.call(this);\n    return Object.assign(a, o), a;\n  }, t.fromConfig = function (e, t, n) {\n    void 0 === n && (n = {});\n\n    for (var r = [], i = 0, a = t.cells; i < a.length; i++) {\n      var o = a[i];\n      r.push(deserialize(o, n));\n    }\n\n    return new e({\n      cells: r\n    });\n  }, Object.defineProperty(t.prototype, \"trainableWeights\", {\n    get: function get() {\n      if (!this.trainable) return [];\n\n      for (var e = [], t = 0, n = this.cells; t < n.length; t++) {\n        var r = n[t];\n        e.push.apply(e, r.trainableWeights);\n      }\n\n      return e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"nonTrainableWeights\", {\n    get: function get() {\n      for (var e = [], t = 0, n = this.cells; t < n.length; t++) {\n        var r = n[t];\n        e.push.apply(e, r.nonTrainableWeights);\n      }\n\n      if (!this.trainable) {\n        for (var i = [], a = 0, o = this.cells; a < o.length; a++) {\n          r = o[a];\n          i.push.apply(i, r.trainableWeights);\n        }\n\n        return i.concat(e);\n      }\n\n      return e;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.getWeights = function () {\n    for (var e = [], t = 0, n = this.cells; t < n.length; t++) {\n      var r = n[t];\n      e.push.apply(e, r.weights);\n    }\n\n    return batchGetValue(e);\n  }, t.prototype.setWeights = function (e) {\n    for (var t = [], n = 0, r = this.cells; n < r.length; n++) {\n      for (var i = r[n], a = i.weights.length, o = e.splice(a), s = 0; s < i.weights.length; ++s) {\n        t.push([i.weights[s], o[s]]);\n      }\n    }\n\n    batchSetValue(t);\n  }, t.className = \"StackedRNNCells\", t;\n}(RNNCell);\n\nfunction generateDropoutMask(e, t, n, r) {\n  function i() {\n    return dropout(e(), getScalar(t));\n  }\n\n  if (void 0 === n && (n = null), void 0 === r && (r = 1), r > 1) {\n    for (var a = [], o = 0; o < r; o++) {\n      a.push(inTrainPhase(i, e, n));\n    }\n\n    return a.forEach(function (e) {\n      return keep(e);\n    }), a;\n  }\n\n  return keep(inTrainPhase(i, e, n));\n}\n\nserialization.registerClass(StackedRNNCells);\n\nvar Wrapper = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.layer = t.layer, n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (e) {\n    this.built = !0;\n  }, Object.defineProperty(t.prototype, \"trainable\", {\n    get: function get() {\n      return null != this.layer && this.layer.trainable;\n    },\n    set: function set(e) {\n      null != this.layer && (this.layer.trainable = e);\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"trainableWeights\", {\n    get: function get() {\n      return this.layer.trainableWeights;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"nonTrainableWeights\", {\n    get: function get() {\n      return this.layer.nonTrainableWeights;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"updates\", {\n    get: function get() {\n      return this.layer._updates;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"losses\", {\n    get: function get() {\n      return this.layer.losses;\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.getWeights = function () {\n    return this.layer.getWeights();\n  }, t.prototype.setWeights = function (e) {\n    this.layer.setWeights(e);\n  }, t.prototype.getConfig = function () {\n    var t = {\n      layer: {\n        className: this.layer.getClassName(),\n        config: this.layer.getConfig()\n      }\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.prototype.setFastWeightInitDuringBuild = function (t) {\n    e.prototype.setFastWeightInitDuringBuild.call(this, t), null != this.layer && this.layer.setFastWeightInitDuringBuild(t);\n  }, t.fromConfig = function (e, t, n) {\n    void 0 === n && (n = {});\n    var r = deserialize(t.layer, n);\n    delete t.layer;\n    var i = {\n      layer: r\n    };\n    return Object.assign(i, t), new e(i);\n  }, t;\n}(Layer),\n    TimeDistributed = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this;\n    return n.supportsMasking = !0, n;\n  }\n\n  return __extends(t, e), t.prototype.build = function (t) {\n    if ((t = getExactlyOneShape(t)).length < 3) throw new ValueError(\"TimeDistributed layer expects an input shape >= 3D, but received input shape \" + JSON.stringify(t));\n    this.inputSpec = [{\n      shape: t\n    }];\n    var n = [t[0]].concat(t.slice(2));\n    this.layer.built || (this.layer.build(n), this.layer.built = !0), e.prototype.build.call(this, t);\n  }, t.prototype.computeOutputShape = function (e) {\n    var t = [(e = getExactlyOneShape(e))[0]].concat(e.slice(2)),\n        n = this.layer.computeOutputShape(t),\n        r = e[1];\n    return [n[0], r].concat(n.slice(1));\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      return rnn(function (e, r) {\n        return [getExactlyOneTensor(n.layer.call(e, t)), []];\n      }, e = getExactlyOneTensor(e), [], !1, null, null, !1, !0)[1];\n    });\n  }, t.className = \"TimeDistributed\", t;\n}(Wrapper);\n\nfunction checkBidirectionalMergeMode(e) {\n  checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, \"BidirectionalMergeMode\", e);\n}\n\nserialization.registerClass(TimeDistributed);\n\nvar Bidirectional = function (e) {\n  function t(t) {\n    var n = e.call(this, t) || this,\n        r = t.layer.getConfig();\n    if (n.forwardLayer = deserialize({\n      className: t.layer.getClassName(),\n      config: r\n    }), r.goBackwards = !0 !== r.goBackwards, n.backwardLayer = deserialize({\n      className: t.layer.getClassName(),\n      config: r\n    }), n.forwardLayer.name = \"forward_\" + n.forwardLayer.name, n.backwardLayer.name = \"backward_\" + n.backwardLayer.name, checkBidirectionalMergeMode(t.mergeMode), n.mergeMode = t.mergeMode, t.weights) throw new NotImplementedError(\"weights support is not implemented for Bidirectional layer yet.\");\n    return n._stateful = t.layer.stateful, n.returnSequences = t.layer.returnSequences, n.returnState = t.layer.returnState, n.supportsMasking = !0, n._trainable = !0, n.inputSpec = t.layer.inputSpec, n.numConstants = null, n;\n  }\n\n  return __extends(t, e), Object.defineProperty(t.prototype, \"trainable\", {\n    get: function get() {\n      return this._trainable;\n    },\n    set: function set(e) {\n      this._trainable = e, null != this.forwardLayer && (this.forwardLayer.trainable = e), null != this.backwardLayer && (this.backwardLayer.trainable = e);\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.getWeights = function () {\n    return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n  }, t.prototype.setWeights = function (e) {\n    var t = e.length,\n        n = Math.floor(t / 2);\n    this.forwardLayer.setWeights(e.slice(0, n)), this.backwardLayer.setWeights(e.slice(n));\n  }, t.prototype.computeOutputShape = function (e) {\n    var t,\n        n,\n        r,\n        i = this.forwardLayer.computeOutputShape(e);\n    return Array.isArray(i) && Array.isArray(i[0]) || (i = [i]), i = i, this.returnState ? (r = i.slice(1), t = i[0]) : t = i[0], t = t, \"concat\" === this.mergeMode ? (t[t.length - 1] *= 2, n = [t]) : n = null == this.mergeMode ? [t, t.slice()] : [t], this.returnState ? null == this.mergeMode ? n.concat(r).concat(r.slice()) : [t].concat(r).concat(r.slice()) : singletonOrArray(n);\n  }, t.prototype.apply = function (t, n) {\n    var r = null == n ? null : n.initialState,\n        i = null == n ? null : n.constants;\n    null == n && (n = {});\n    var a = standardizeArgs(t, r, i, this.numConstants);\n    if (t = a.inputs, r = a.initialState, i = a.constants, Array.isArray(t) && (r = t.slice(1), t = t[0]), (null == r || 0 === r.length) && null == i) return e.prototype.apply.call(this, t, n);\n    var o = [],\n        s = [];\n\n    if (null != r) {\n      var l = r.length;\n      if (l % 2 > 0) throw new ValueError(\"When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.\");\n      n.initialState = r, o.push.apply(o, r);\n      var u = r.map(function (e) {\n        return new InputSpec({\n          shape: e.shape\n        });\n      });\n      this.forwardLayer.stateSpec = u.slice(0, l / 2), this.backwardLayer.stateSpec = u.slice(l / 2), s.push.apply(s, u);\n    }\n\n    if (null != i) throw new NotImplementedError(\"Support for constants in Bidirectional layers is not implemented yet.\");\n\n    for (var c = o[0] instanceof SymbolicTensor, p = 0, h = o; p < h.length; p++) {\n      if (h[p] instanceof SymbolicTensor !== c) throw new ValueError(\"The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors\");\n    }\n\n    if (c) {\n      var d = [t].concat(o),\n          f = this.inputSpec.concat(s),\n          g = this.inputSpec;\n      this.inputSpec = f;\n      var m = e.prototype.apply.call(this, d, n);\n      return this.inputSpec = g, m;\n    }\n\n    return e.prototype.apply.call(this, t, n);\n  }, t.prototype.call = function (e, t) {\n    var n = this;\n    return tidy(function () {\n      if (null != t.mask) throw new NotImplementedError(\"The support for masking is not implemented for Bidirectional layers yet.\");\n      var r,\n          i,\n          a,\n          o,\n          s = t.initialState;\n      if (null == s) r = n.forwardLayer.call(e, t), i = n.backwardLayer.call(e, t);else {\n        var l = s.slice(0, s.length / 2),\n            u = s.slice(s.length / 2);\n        r = n.forwardLayer.call(e, Object.assign(t, {\n          initialState: l\n        })), i = n.backwardLayer.call(e, Object.assign(t, {\n          initialState: u\n        }));\n      }\n      return n.returnState && (Array.isArray(r) && (a = r.slice(1).concat(i.slice(1))), r = r[0], i = i[0]), n.returnSequences && (i = reverse(i, 1)), \"concat\" === n.mergeMode ? o = concatenate([r, i]) : \"sum\" === n.mergeMode ? o = add(r, i) : \"ave\" === n.mergeMode ? o = mul(getScalar(.5), add(r, i)) : \"mul\" === n.mergeMode ? o = mul(r, i) : null == n.mergeMode && (o = [r, i]), n.returnState ? null == n.mergeMode ? o.concat(a) : [o].concat(a) : o;\n    });\n  }, t.prototype.resetStates = function (e) {\n    this.forwardLayer.resetStates(), this.backwardLayer.resetStates();\n  }, t.prototype.build = function (e) {\n    var t = this;\n    nameScope(this.forwardLayer.name, function () {\n      t.forwardLayer.build(e);\n    }), nameScope(this.backwardLayer.name, function () {\n      t.backwardLayer.build(e);\n    }), this.built = !0;\n  }, Object.defineProperty(t.prototype, \"trainableWeights\", {\n    get: function get() {\n      return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n    },\n    enumerable: !0,\n    configurable: !0\n  }), Object.defineProperty(t.prototype, \"nonTrainableWeights\", {\n    get: function get() {\n      return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n    },\n    enumerable: !0,\n    configurable: !0\n  }), t.prototype.setFastWeightInitDuringBuild = function (t) {\n    e.prototype.setFastWeightInitDuringBuild.call(this, t), null != this.forwardLayer && this.forwardLayer.setFastWeightInitDuringBuild(t), null != this.backwardLayer && this.backwardLayer.setFastWeightInitDuringBuild(t);\n  }, t.prototype.getConfig = function () {\n    var t = {\n      mergeMode: this.mergeMode\n    },\n        n = e.prototype.getConfig.call(this);\n    return Object.assign(t, n), t;\n  }, t.fromConfig = function (e, t) {\n    var n = deserialize(t.layer);\n    if (delete t.layer, null != t.numConstants) throw new NotImplementedError(\"Deserialization of a Bidirectional layer with numConstants present is not supported yet.\");\n    var r = t;\n    return r.layer = n, new e(r);\n  }, t.className = \"Bidirectional\", t;\n}(Wrapper);\n\nfunction inputLayer(e) {\n  return new InputLayer(e);\n}\n\nfunction elu$2(e) {\n  return new ELU(e);\n}\n\nfunction reLU(e) {\n  return new ReLU(e);\n}\n\nfunction leakyReLU(e) {\n  return new LeakyReLU(e);\n}\n\nfunction prelu$1(e) {\n  return new PReLU(e);\n}\n\nfunction softmax$1(e) {\n  return new Softmax$1(e);\n}\n\nfunction thresholdedReLU(e) {\n  return new ThresholdedReLU(e);\n}\n\nfunction conv1d$2(e) {\n  return new Conv1D(e);\n}\n\nfunction conv2d$2(e) {\n  return new Conv2D(e);\n}\n\nfunction conv2dTranspose$1(e) {\n  return new Conv2DTranspose(e);\n}\n\nfunction separableConv2d$1(e) {\n  return new SeparableConv2D(e);\n}\n\nfunction cropping2D(e) {\n  return new Cropping2D(e);\n}\n\nfunction upSampling2d(e) {\n  return new UpSampling2D(e);\n}\n\nfunction depthwiseConv2d$2(e) {\n  return new DepthwiseConv2D(e);\n}\n\nfunction activation(e) {\n  return new Activation$1(e);\n}\n\nfunction dense(e) {\n  return new Dense(e);\n}\n\nfunction dropout$1(e) {\n  return new Dropout(e);\n}\n\nfunction flatten$1(e) {\n  return new Flatten(e);\n}\n\nfunction repeatVector(e) {\n  return new RepeatVector(e);\n}\n\nfunction reshape(e) {\n  return new Reshape(e);\n}\n\nfunction permute(e) {\n  return new Permute(e);\n}\n\nfunction embedding(e) {\n  return new Embedding(e);\n}\n\nfunction add$2(e) {\n  return new Add(e);\n}\n\nfunction average$1(e) {\n  return new Average(e);\n}\n\nfunction concatenate$2(e) {\n  return new Concatenate(e);\n}\n\nfunction maximum$2(e) {\n  return new Maximum(e);\n}\n\nfunction minimum$2(e) {\n  return new Minimum(e);\n}\n\nfunction multiply$1(e) {\n  return new Multiply(e);\n}\n\nfunction dot$1(e) {\n  return new Dot(e);\n}\n\nfunction batchNormalization$1(e) {\n  return new BatchNormalization(e);\n}\n\nfunction zeroPadding2d(e) {\n  return new ZeroPadding2D(e);\n}\n\nfunction averagePooling1d(e) {\n  return new AveragePooling1D(e);\n}\n\nfunction avgPool1d(e) {\n  return averagePooling1d(e);\n}\n\nfunction avgPooling1d(e) {\n  return averagePooling1d(e);\n}\n\nfunction averagePooling2d(e) {\n  return new AveragePooling2D(e);\n}\n\nfunction avgPool2d(e) {\n  return averagePooling2d(e);\n}\n\nfunction avgPooling2d(e) {\n  return averagePooling2d(e);\n}\n\nfunction globalAveragePooling1d(e) {\n  return new GlobalAveragePooling1D(e);\n}\n\nfunction globalAveragePooling2d(e) {\n  return new GlobalAveragePooling2D(e);\n}\n\nfunction globalMaxPooling1d(e) {\n  return new GlobalMaxPooling1D(e);\n}\n\nfunction globalMaxPooling2d(e) {\n  return new GlobalMaxPooling2D(e);\n}\n\nfunction maxPooling1d(e) {\n  return new MaxPooling1D(e);\n}\n\nfunction maxPooling2d(e) {\n  return new MaxPooling2D(e);\n}\n\nfunction gru(e) {\n  return new GRU(e);\n}\n\nfunction gruCell(e) {\n  return new GRUCell(e);\n}\n\nfunction lstm(e) {\n  return new LSTM(e);\n}\n\nfunction lstmCell(e) {\n  return new LSTMCell(e);\n}\n\nfunction simpleRNN(e) {\n  return new SimpleRNN(e);\n}\n\nfunction simpleRNNCell(e) {\n  return new SimpleRNNCell(e);\n}\n\nfunction rnn$1(e) {\n  return new RNN(e);\n}\n\nfunction stackedRNNCells(e) {\n  return new StackedRNNCells(e);\n}\n\nfunction bidirectional(e) {\n  return new Bidirectional(e);\n}\n\nfunction timeDistributed(e) {\n  return new TimeDistributed(e);\n}\n\nserialization.registerClass(Bidirectional);\nvar globalMaxPool1d = globalMaxPooling1d,\n    globalMaxPool2d = globalMaxPooling2d,\n    maxPool1d = maxPooling1d,\n    maxPool2d = maxPooling2d,\n    exports_layers = Object.freeze({\n  inputLayer: inputLayer,\n  elu: elu$2,\n  reLU: reLU,\n  leakyReLU: leakyReLU,\n  prelu: prelu$1,\n  softmax: softmax$1,\n  thresholdedReLU: thresholdedReLU,\n  conv1d: conv1d$2,\n  conv2d: conv2d$2,\n  conv2dTranspose: conv2dTranspose$1,\n  separableConv2d: separableConv2d$1,\n  cropping2D: cropping2D,\n  upSampling2d: upSampling2d,\n  depthwiseConv2d: depthwiseConv2d$2,\n  activation: activation,\n  dense: dense,\n  dropout: dropout$1,\n  flatten: flatten$1,\n  repeatVector: repeatVector,\n  reshape: reshape,\n  permute: permute,\n  embedding: embedding,\n  add: add$2,\n  average: average$1,\n  concatenate: concatenate$2,\n  maximum: maximum$2,\n  minimum: minimum$2,\n  multiply: multiply$1,\n  dot: dot$1,\n  batchNormalization: batchNormalization$1,\n  zeroPadding2d: zeroPadding2d,\n  averagePooling1d: averagePooling1d,\n  avgPool1d: avgPool1d,\n  avgPooling1d: avgPooling1d,\n  averagePooling2d: averagePooling2d,\n  avgPool2d: avgPool2d,\n  avgPooling2d: avgPooling2d,\n  globalAveragePooling1d: globalAveragePooling1d,\n  globalAveragePooling2d: globalAveragePooling2d,\n  globalMaxPooling1d: globalMaxPooling1d,\n  globalMaxPooling2d: globalMaxPooling2d,\n  maxPooling1d: maxPooling1d,\n  maxPooling2d: maxPooling2d,\n  gru: gru,\n  gruCell: gruCell,\n  lstm: lstm,\n  lstmCell: lstmCell,\n  simpleRNN: simpleRNN,\n  simpleRNNCell: simpleRNNCell,\n  rnn: rnn$1,\n  stackedRNNCells: stackedRNNCells,\n  bidirectional: bidirectional,\n  timeDistributed: timeDistributed,\n  globalMaxPool1d: globalMaxPool1d,\n  globalMaxPool2d: globalMaxPool2d,\n  maxPool1d: maxPool1d,\n  maxPool2d: maxPool2d,\n  Layer: Layer,\n  RNN: RNN,\n  RNNCell: RNNCell,\n  input: input\n});\n\nfunction binaryAccuracy$1(e, t) {\n  return binaryAccuracy(e, t);\n}\n\nfunction binaryCrossentropy$2(e, t) {\n  return binaryCrossentropy$1(e, t);\n}\n\nfunction sparseCategoricalAccuracy$1(e, t) {\n  return sparseCategoricalAccuracy(e, t);\n}\n\nfunction categoricalAccuracy$1(e, t) {\n  return categoricalAccuracy(e, t);\n}\n\nfunction categoricalCrossentropy$2(e, t) {\n  return categoricalCrossentropy$1(e, t);\n}\n\nfunction precision$1(e, t) {\n  return precision(e, t);\n}\n\nfunction recall$1(e, t) {\n  return recall(e, t);\n}\n\nfunction cosineProximity$1(e, t) {\n  return cosineProximity(e, t);\n}\n\nfunction meanAbsoluteError$1(e, t) {\n  return meanAbsoluteError(e, t);\n}\n\nfunction meanAbsolutePercentageError$1(e, t) {\n  return meanAbsolutePercentageError(e, t);\n}\n\nfunction MAPE$2(e, t) {\n  return meanAbsolutePercentageError(e, t);\n}\n\nfunction mape$2(e, t) {\n  return meanAbsolutePercentageError(e, t);\n}\n\nfunction meanSquaredError$1(e, t) {\n  return meanSquaredError(e, t);\n}\n\nfunction MSE$2(e, t) {\n  return meanSquaredError(e, t);\n}\n\nfunction mse$2(e, t) {\n  return meanSquaredError(e, t);\n}\n\nvar exports_metrics = Object.freeze({\n  binaryAccuracy: binaryAccuracy$1,\n  binaryCrossentropy: binaryCrossentropy$2,\n  sparseCategoricalAccuracy: sparseCategoricalAccuracy$1,\n  categoricalAccuracy: categoricalAccuracy$1,\n  categoricalCrossentropy: categoricalCrossentropy$2,\n  precision: precision$1,\n  recall: recall$1,\n  cosineProximity: cosineProximity$1,\n  meanAbsoluteError: meanAbsoluteError$1,\n  meanAbsolutePercentageError: meanAbsolutePercentageError$1,\n  MAPE: MAPE$2,\n  mape: mape$2,\n  meanSquaredError: meanSquaredError$1,\n  MSE: MSE$2,\n  mse: mse$2\n}),\n    exports_models = Object.freeze({\n  modelFromJSON: modelFromJSON\n});\n\nfunction l1l2(e) {\n  return new L1L2(e);\n}\n\nfunction l1$1(e) {\n  return l1(e);\n}\n\nfunction l2$1(e) {\n  return l2(e);\n}\n\nvar exports_regularizers = Object.freeze({\n  l1l2: l1l2,\n  l1: l1$1,\n  l2: l2$1\n}),\n    Callback = function (e) {\n  function t() {\n    var t = null !== e && e.apply(this, arguments) || this;\n    return t.model = null, t;\n  }\n\n  return __extends(t, e), t.prototype.setModel = function (e) {\n    if (!(e instanceof Model)) throw new Error(\"model must be a Model, not some other Container\");\n    this.model = e;\n  }, t;\n}(BaseCallback);\n\nexport { exports_constraints as constraints, exports_initializers as initializers, exports_layers as layers, exports_metrics as metrics, exports_models as models, exports_regularizers as regularizers, CallbackList, CustomCallback, History, Callback, InputSpec, SymbolicTensor, Model, input, loadLayersModel, loadModel, model, registerCallbackConstructor, sequential, RNN, Sequential, LayerVariable, version as version_layers };","map":null,"metadata":{},"sourceType":"module"}