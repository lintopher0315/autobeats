{"ast":null,"code":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar tf = require(\"@tensorflow/tfjs\");\n\nexports.ATTENTION_PREFIX = 'attention_cell_wrapper/';\n\nvar AttentionWrapper = function () {\n  function AttentionWrapper(cells, attnLength, attnSize) {\n    this.cells = cells;\n    this.attnLength = attnLength;\n    this.attnSize = attnSize;\n  }\n\n  AttentionWrapper.isWrapped = function (vars) {\n    return \"rnn/\" + exports.ATTENTION_PREFIX + \"kernel\" in vars;\n  };\n\n  AttentionWrapper.prototype.initialize = function (vars) {\n    var prefix = \"rnn/\" + exports.ATTENTION_PREFIX;\n    this.attnInputMatrix = vars[prefix + \"kernel\"];\n    this.attnInputBias = vars[prefix + \"bias\"];\n    this.attnW = vars[prefix + \"attention/attn_w\"];\n    this.attnV = vars[prefix + \"attention/attn_v\"];\n    this.attnMatrix = vars[prefix + \"attention/kernel\"];\n    this.attnBias = vars[prefix + \"attention/bias\"];\n    this.attnOutputMatrix = vars[prefix + \"attention_output_projection/kernel\"];\n    this.attnOutputBias = vars[prefix + \"attention_output_projection/bias\"];\n  };\n\n  AttentionWrapper.prototype.initState = function () {\n    var attention = tf.zeros([this.attnSize]);\n    var attentionState = tf.zeros([1, this.attnSize * this.attnLength]);\n    return {\n      attention: attention,\n      attentionState: attentionState\n    };\n  };\n\n  AttentionWrapper.prototype.call = function (input, c, h, state) {\n    var _a;\n\n    var nextAttnInput = tf.concat([input, state.attention.as2D(1, -1)], 1);\n    var nextRnnInput = tf.add(tf.matMul(nextAttnInput, this.attnInputMatrix), this.attnInputBias.as2D(1, -1));\n    _a = tf.multiRNNCell(this.cells, nextRnnInput, c, h), c = _a[0], h = _a[1];\n    var attnHidden = tf.reshape(state.attentionState, [-1, this.attnLength, 1, this.attnSize]);\n    var attnHiddenFeatures = tf.conv2d(attnHidden, this.attnW, [1, 1], 'same');\n    var attnQueryParts = [];\n\n    for (var q = 0; q < c.length; q++) {\n      attnQueryParts.push(c[q]);\n      attnQueryParts.push(h[q]);\n    }\n\n    var attnQuery = tf.concat(attnQueryParts, 1);\n    var attnY = tf.matMul(attnQuery, this.attnMatrix).reshape([-1, 1, 1, this.attnSize]);\n    var attnS = tf.sum(tf.mul(this.attnV, tf.tanh(tf.add(attnHiddenFeatures, attnY))), [2, 3]);\n    var attnA = tf.softmax(attnS);\n    var attnD = tf.sum(tf.mul(tf.reshape(attnA, [-1, this.attnLength, 1, 1]), attnHidden), [1, 2]);\n    var newAttns = attnD.reshape([-1, this.attnSize]);\n    var attnStates = state.attentionState.reshape([-1, this.attnLength, this.attnSize]);\n    var newAttnStates = tf.slice(attnStates, [0, 1, 0], [attnStates.shape[0], attnStates.shape[1] - 1, attnStates.shape[2]]);\n    var output = tf.add(tf.matMul(tf.concat([h[2], newAttns], 1), this.attnOutputMatrix), this.attnOutputBias);\n    var attention = newAttns.flatten();\n    var attentionState = tf.concat([newAttnStates, output.as3D(output.shape[0], 1, output.shape[1])], 1).reshape([-1, this.attnLength * this.attnSize]);\n    return {\n      output: output,\n      c: c,\n      h: h,\n      attentionState: {\n        attention: attention,\n        attentionState: attentionState\n      }\n    };\n  };\n\n  return AttentionWrapper;\n}();\n\nexports.AttentionWrapper = AttentionWrapper;","map":null,"metadata":{},"sourceType":"script"}